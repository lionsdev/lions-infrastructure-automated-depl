---
# =============================================================================
# LIONS Infrastructure - Playbook d'Installation K3s Complet avec Monitoring
# =============================================================================
# Titre: Playbook d'installation K3s optimis√© avec Prometheus/Grafana int√©gr√©
# Description: Installe et configure K3s sur VPS avec monitoring complet
# Auteur: √âquipe LIONS Infrastructure
# Date: 2025-05-23
# Version: 2.1.0
#
# Corrections appliqu√©es:
# - Suppression du flag d√©pr√©ci√© RemoveSelfLink=false
# - Correction de la syntaxe des flags --disable
# - Ajout du d√©ploiement Prometheus/Grafana optimis√©
# - Configuration monitoring haute performance
# - Am√©lioration de la documentation et du logging
# - Application des meilleures pratiques DevOps
# =============================================================================

- name: "LIONS K3s Installation - Production Ready Deployment with Monitoring"
  hosts: vps
  become: yes
  gather_facts: yes

  # =============================================================================
  # VARIABLES DE CONFIGURATION
  # =============================================================================
  vars:
    # Version K3s - Version LTS stable recommand√©e
    k3s_version: "v1.28.6+k3s2"

    # Configuration K3s optimis√©e (FLAGS CORRIG√âS)
    k3s_server_args: >-
      server
      --disable=traefik
      --disable=servicelb
      --disable=local-storage
      --write-kubeconfig-mode=644
      --kubelet-arg=cgroup-driver=systemd
      --kubelet-arg=feature-gates=GracefulNodeShutdown=false

    # Chemins et configurations
    kubeconfig_local_path: "~/.kube/config"
    k3s_service_file: "/etc/systemd/system/k3s.service"
    k3s_kubeconfig: "/etc/rancher/k3s/k3s.yaml"

    # Versions des composants
    traefik_chart_version: "25.0.0"
    metallb_chart_version: "0.13.12"
    cert_manager_version: "v1.13.3"
    kube_prometheus_stack_version: "56.21.4"

    # Flags d√©pr√©ci√©s √† supprimer automatiquement
    deprecated_flags:
      - pattern: "--kube-controller-manager-arg.*feature-gates=.*RemoveSelfLink=false.*"
        replacement: ""
        description: "Flag RemoveSelfLink d√©pr√©ci√©"
      - pattern: "--no-deploy ([a-zA-Z0-9-]+)"
        replacement: "--disable=\\1"
        description: "Syntaxe --no-deploy d√©pr√©ci√©e"

    # Configuration monitoring optimis√©e
    monitoring_config:
      namespace: "monitoring"
      grafana_admin_password: "{{ lions_grafana_password | default('admin123!') }}"
      prometheus_retention: "15d"
      prometheus_storage: "10Gi"
      grafana_storage: "5Gi"
      resource_limits:
        prometheus_cpu: "1000m"
        prometheus_memory: "2Gi"
        grafana_cpu: "500m"
        grafana_memory: "512Mi"

  # =============================================================================
  # T√ÇCHES PRINCIPALES
  # =============================================================================
  tasks:

    # =========================================================================
    # PHASE 1: V√âRIFICATIONS PR√âLIMINAIRES ET NETTOYAGE
    # =========================================================================

    - name: "üìã PHASE 1 - V√©rifications pr√©liminaires"
      debug:
        msg: |
          ==========================================================
          üöÄ D√âMARRAGE INSTALLATION K3S - LIONS INFRASTRUCTURE
          ==========================================================
          Version K3s: {{ k3s_version }}
          Host cible: {{ ansible_host }}
          Environment: {{ lions_env | default('development') }}
          Monitoring: ‚úÖ Prometheus + Grafana inclus
          ==========================================================

    - name: "üîç V√©rification de l'environnement syst√®me"
      block:
        - name: "V√©rification des ressources syst√®me minimales"
          assert:
            that:
              - ansible_memtotal_mb >= 2048  # Augment√© pour le monitoring
              - ansible_processor_vcpus >= 2
            fail_msg: "Ressources syst√®me insuffisantes pour le monitoring (RAM: {{ ansible_memtotal_mb }}MB, CPU: {{ ansible_processor_vcpus }})"
            success_msg: "‚úÖ Ressources syst√®me suffisantes (RAM: {{ ansible_memtotal_mb }}MB, CPU: {{ ansible_processor_vcpus }})"

        - name: "V√©rification de l'espace disque disponible"
          shell: df / | awk 'NR==2 {print $4}'
          register: disk_space_check
          changed_when: false

        - name: "Validation de l'espace disque"
          assert:
            that:
              - disk_space_check.stdout | int > 10000000  # 10GB minimum pour le monitoring
            fail_msg: "Espace disque insuffisant: {{ (disk_space_check.stdout | int / 1000 / 1000) | round(1) }}GB disponible"
            success_msg: "‚úÖ Espace disque suffisant: {{ (disk_space_check.stdout | int / 1000 / 1000) | round(1) }}GB disponible"

      rescue:
        - name: "‚ùå √âchec des v√©rifications pr√©liminaires"
          fail:
            msg: "Les v√©rifications syst√®me ont √©chou√©. Installation annul√©e."

    - name: "üßπ D√©tection et nettoyage de l'installation K3s existante"
      block:
        - name: "V√©rification de l'existence du binaire K3s"
          stat:
            path: /usr/local/bin/k3s
          register: k3s_binary_exists

        - name: "V√©rification du service K3s existant"
          systemd:
            name: k3s
          register: k3s_service_check
          ignore_errors: true

        - name: "Analyse de l'√©tat du service K3s"
          debug:
            msg: |
              K3s Binary: {{ 'Pr√©sent' if k3s_binary_exists.stat.exists else 'Absent' }}
              Service K3s: {{ k3s_service_check.status.ActiveState | default('Inexistant') }}

        - name: "Correction proactive des flags d√©pr√©ci√©s"
          block:
            - name: "Lecture du fichier de service K3s"
              slurp:
                src: "{{ k3s_service_file }}"
              register: k3s_service_content_raw
              when: k3s_binary_exists.stat.exists

            - name: "D√©codage du contenu du service"
              set_fact:
                k3s_service_content: "{{ k3s_service_content_raw.content | b64decode }}"
              when: k3s_service_content_raw is defined

            - name: "üîç Diagnostic du contenu du service K3s"
              debug:
                msg: |
                  Contenu du service K3s:
                  {{ k3s_service_content }}
              when: k3s_service_content is defined

            - name: "D√©tection des flags d√©pr√©ci√©s"
              set_fact:
                deprecated_flags_found: "{{ deprecated_flags_found | default([]) + [item] }}"
              loop: "{{ deprecated_flags }}"
              when:
                - k3s_service_content is defined
                - k3s_service_content is search(item.pattern)

            - name: "Notification des flags d√©pr√©ci√©s d√©tect√©s"
              debug:
                msg: |
                  ‚ö†Ô∏è  FLAGS D√âPR√âCI√âS D√âTECT√âS:
                  {% for flag in deprecated_flags_found | default([]) %}
                  - {{ flag.description }}
                  {% endfor %}
              when: deprecated_flags_found is defined and deprecated_flags_found | length > 0

            - name: "Suppression des flags d√©pr√©ci√©s"
              replace:
                path: "{{ k3s_service_file }}"
                regexp: "{{ item.pattern }}"
                replace: "{{ item.replacement | default('') }}"
                backup: yes
              loop: "{{ deprecated_flags }}"
              when:
                - k3s_service_content is defined
                - k3s_service_content is search(item.pattern)
              register: flags_corrected
              notify: "reload systemd and restart k3s"

            - name: "üîß Correction forc√©e du flag RemoveSelfLink (solution de secours)"
              replace:
                path: "{{ k3s_service_file }}"
                regexp: "RemoveSelfLink=false"
                replace: ""
                backup: yes
              when:
                - k3s_service_content is defined
                - "'RemoveSelfLink=false' in k3s_service_content"
              register: forced_correction
              notify: "reload systemd and restart k3s"

            - name: "üìã Rapport de correction des flags"
              debug:
                msg: |
                  Flags corrig√©s automatiquement: {{ 'Oui' if flags_corrected.changed else 'Non' }}
                  Correction forc√©e effectu√©e: {{ 'Oui' if forced_correction.changed else 'Non' }}

          when: k3s_binary_exists.stat.exists

    # =========================================================================
    # PHASE 2: INSTALLATION K3S
    # =========================================================================

    - name: "üöÄ PHASE 2 - Installation K3s"
      debug:
        msg: "D√©marrage de l'installation K3s avec la configuration corrig√©e"

    - name: "üì• T√©l√©chargement du script d'installation K3s"
      get_url:
        url: "https://get.k3s.io"
        dest: "/tmp/k3s-install.sh"
        mode: '0755'
        timeout: 30
      register: k3s_script_download
      retries: 3
      delay: 5

    - name: "‚öôÔ∏è  Installation K3s avec configuration optimis√©e"
      command: "/tmp/k3s-install.sh"
      environment:
        INSTALL_K3S_VERSION: "{{ k3s_version }}"
        INSTALL_K3S_EXEC: "{{ k3s_server_args }}"
        K3S_KUBECONFIG_MODE: "644"
      register: k3s_installation
      when: not k3s_binary_exists.stat.exists or (deprecated_flags_found is defined and deprecated_flags_found | length > 0)

    - name: "üîÑ D√©marrage et activation du service K3s"
      systemd:
        name: k3s
        state: started
        enabled: yes
        daemon_reload: yes
      register: k3s_service_start
      retries: 3
      delay: 10

    # =========================================================================
    # PHASE 3: V√âRIFICATIONS POST-INSTALLATION
    # =========================================================================

    - name: "‚úÖ PHASE 3 - V√©rifications post-installation"
      debug:
        msg: "V√©rification de l'installation K3s"

    - name: "‚è≥ Attente de la disponibilit√© de l'API K3s"
      wait_for:
        port: 6443
        host: localhost
        delay: 10
        timeout: 300
      register: k3s_api_ready

    - name: "üîç V√©rification de l'√©tat du service K3s"
      systemd:
        name: k3s
      register: k3s_final_status

    - name: "üìä Validation de l'installation K3s"
      block:
        - name: "Test de connectivit√© kubectl"
          command: "/usr/local/bin/k3s kubectl get nodes"
          register: k3s_nodes_check
          changed_when: false
          environment:
            KUBECONFIG: "{{ k3s_kubeconfig }}"

        - name: "V√©rification des pods syst√®me"
          command: "/usr/local/bin/k3s kubectl get pods -n kube-system"
          register: k3s_system_pods
          changed_when: false
          environment:
            KUBECONFIG: "{{ k3s_kubeconfig }}"

        - name: "üìà Rapport d'√©tat K3s"
          debug:
            msg: |
              ==========================================================
              ‚úÖ K3S INSTALLATION R√âUSSIE
              ==========================================================
              Service Status: {{ k3s_final_status.status.ActiveState }}
              API Port: ‚úÖ Accessible sur :6443
              Nodes: {{ k3s_nodes_check.stdout_lines | length }} n≈ìud(s) d√©tect√©(s)
              System Pods: {{ k3s_system_pods.stdout_lines | length - 1 }} pod(s) syst√®me
              ==========================================================

      rescue:
        - name: "üîß Diagnostic en cas d'√©chec"
          block:
            - name: "Collecte des logs K3s"
              command: "journalctl -u k3s -n 20 --no-pager"
              register: k3s_logs

            - name: "üîç V√©rification du contenu actuel du service K3s"
              slurp:
                src: "{{ k3s_service_file }}"
              register: current_service_content

            - name: "üìã Diagnostic d√©taill√©"
              debug:
                msg: |
                  ‚ùå √âCHEC DE L'INSTALLATION K3S - DIAGNOSTIC D√âTAILL√â
                  ==========================================================
                  Service Status: {{ k3s_final_status.status.ActiveState | default('Unknown') }}
                  
                  Contenu actuel du service K3s:
                  {{ current_service_content.content | b64decode }}
                  
                  Derniers logs:
                  {{ k3s_logs.stdout }}
                  ==========================================================

            - name: "üîß Nettoyage complet et r√©installation si flag RemoveSelfLink d√©tect√©"
              block:
                - name: "Arr√™t du service K3s"
                  systemd:
                    name: k3s
                    state: stopped
                  ignore_errors: true

                - name: "D√©sinstallation compl√®te de K3s"
                  shell: |
                    /usr/local/bin/k3s-uninstall.sh || true
                    rm -rf /etc/rancher/k3s/*
                    rm -rf /var/lib/rancher/k3s/*
                    rm -f /etc/systemd/system/k3s.service*
                  ignore_errors: true

                - name: "Rechargement systemd"
                  systemd:
                    daemon_reload: yes

                - name: "R√©installation propre de K3s"
                  command: "/tmp/k3s-install.sh"
                  environment:
                    INSTALL_K3S_VERSION: "{{ k3s_version }}"
                    INSTALL_K3S_EXEC: "{{ k3s_server_args }}"
                    K3S_KUBECONFIG_MODE: "644"

                - name: "D√©marrage du service K3s apr√®s r√©installation"
                  systemd:
                    name: k3s
                    state: started
                    enabled: yes
                    daemon_reload: yes
                  register: k3s_fresh_start

                - name: "Attente de l'API apr√®s r√©installation"
                  wait_for:
                    port: 6443
                    host: localhost
                    timeout: 120
                  when: k3s_fresh_start is succeeded

              when: "'RemoveSelfLink=false' in (current_service_content.content | b64decode)"

            - name: "‚ùå √âchec d√©finitif si probl√®me persiste"
              fail:
                msg: |
                  ‚ùå INSTALLATION K3S √âCHOU√âE D√âFINITIVEMENT
                  Le flag RemoveSelfLink=false cause toujours des probl√®mes.
                  V√©rifiez manuellement le fichier {{ k3s_service_file }}
                  et supprimez toute r√©f√©rence √† RemoveSelfLink=false
              when:
                - "'RemoveSelfLink=false' in (current_service_content.content | b64decode)"
                - k3s_fresh_start is failed

    # =========================================================================
    # PHASE 4: CONFIGURATION KUBECTL ET KUBECONFIG
    # =========================================================================

    - name: "üîß PHASE 4 - Configuration kubectl et kubeconfig"
      debug:
        msg: "Configuration de l'acc√®s kubectl"

    - name: "üîç D√©tection de l'utilisateur courant"
      set_fact:
        current_user: "{{ ansible_user | default(ansible_user_id) | default('root') }}"
        current_user_home: "{{ ansible_user_dir | default('/root') }}"

    - name: "üìÅ Cr√©ation des r√©pertoires de configuration"
      file:
        path: "{{ item }}"
        state: directory
        mode: '0755'
        owner: "{{ current_user }}"
        group: "{{ current_user }}"
      loop:
        - "{{ current_user_home }}/.kube"
        - "/root/.kube"

    - name: "üìÑ Configuration du kubeconfig utilisateur"
      copy:
        src: "{{ k3s_kubeconfig }}"
        dest: "{{ current_user_home }}/.kube/config"
        remote_src: yes
        owner: "{{ current_user }}"
        group: "{{ current_user }}"
        mode: '0600'
      when:
        - k3s_kubeconfig is exists
        - current_user_home != '/root'

    - name: "üìÑ Configuration du kubeconfig root"
      copy:
        src: "{{ k3s_kubeconfig }}"
        dest: "/root/.kube/config"
        remote_src: yes
        mode: '0600'
      when: k3s_kubeconfig is exists

    - name: "üåê Mise √† jour du kubeconfig avec l'IP externe (utilisateur)"
      replace:
        path: "{{ current_user_home }}/.kube/config"
        regexp: 'https://127.0.0.1:6443'
        replace: 'https://{{ ansible_host }}:6443'
      when: current_user_home != '/root'

    - name: "üåê Mise √† jour du kubeconfig avec l'IP externe (root)"
      replace:
        path: "/root/.kube/config"
        regexp: 'https://127.0.0.1:6443'
        replace: 'https://{{ ansible_host }}:6443'

    - name: "üìÅ Cr√©ation du fichier .bashrc si n√©cessaire"
      file:
        path: "{{ current_user_home }}/.bashrc"
        state: touch
        owner: "{{ current_user }}"
        group: "{{ current_user }}"
        mode: '0644'
      when: current_user_home != '/root'

    - name: "‚öôÔ∏è  Configuration de l'environnement bash"
      lineinfile:
        path: "{{ current_user_home }}/.bashrc"
        line: "export KUBECONFIG={{ current_user_home }}/.kube/config"
        state: present
      when: current_user_home != '/root'

    # =========================================================================
    # PHASE 5: INSTALLATION DES OUTILS COMPL√âMENTAIRES
    # =========================================================================

    - name: "üõ†Ô∏è  PHASE 5 - Installation des outils compl√©mentaires"
      debug:
        msg: "Installation de kubectl et Helm"

    - name: "üì¶ Installation de kubectl"
      block:
        - name: "Cr√©ation du r√©pertoire pour les cl√©s GPG"
          file:
            path: /etc/apt/keyrings
            state: directory
            mode: '0755'

        - name: "T√©l√©chargement de la cl√© GPG Kubernetes"
          get_url:
            url: "https://pkgs.k8s.io/core:/stable:/v1.28/deb/Release.key"
            dest: "/tmp/kubernetes-release.key"
            mode: '0644'

        - name: "Installation de la cl√© GPG Kubernetes"
          shell: |
            cat /tmp/kubernetes-release.key | gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
          args:
            creates: /etc/apt/keyrings/kubernetes-apt-keyring.gpg

        - name: "Ajout du d√©p√¥t Kubernetes"
          apt_repository:
            repo: "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.28/deb/ /"
            state: present
            filename: kubernetes

        - name: "Installation de kubectl"
          apt:
            name: kubectl
            state: present
            update_cache: yes

    - name: "üéõÔ∏è  Installation de Helm"
      block:
        - name: "T√©l√©chargement du script d'installation Helm"
          get_url:
            url: "https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3"
            dest: "/tmp/get-helm-3.sh"
            mode: '0755'

        - name: "Ex√©cution de l'installation Helm"
          command: "/tmp/get-helm-3.sh"
          args:
            creates: /usr/local/bin/helm

    - name: "üêç Installation des d√©pendances Python Kubernetes"
      pip:
        name:
          - kubernetes>=28.0.0
          - openshift>=0.13.0
          - PyYAML>=6.0
        state: present
        extra_args: "--upgrade"

    # =========================================================================
    # PHASE 6: D√âPLOIEMENT DE L'INFRASTRUCTURE DE BASE
    # =========================================================================

    - name: "üèóÔ∏è  PHASE 6 - D√©ploiement de l'infrastructure de base"
      debug:
        msg: "D√©ploiement des composants d'infrastructure essentiels"

    - name: "üåê D√©ploiement de Traefik (Ingress Controller)"
      block:
        - name: "Ajout du d√©p√¥t Helm Traefik"
          kubernetes.core.helm_repository:
            name: traefik
            repo_url: "https://helm.traefik.io/traefik"
          environment:
            KUBECONFIG: "{{ k3s_kubeconfig }}"

        - name: "Cr√©ation du namespace Traefik"
          kubernetes.core.k8s:
            name: traefik
            api_version: v1
            kind: Namespace
            state: present
          environment:
            KUBECONFIG: "{{ k3s_kubeconfig }}"

        - name: "D√©ploiement de Traefik via Helm"
          kubernetes.core.helm:
            name: traefik
            chart_ref: "traefik/traefik"
            chart_version: "{{ traefik_chart_version }}"
            release_namespace: traefik
            create_namespace: true
            wait: true
            wait_timeout: "600s"
            values:
              deployment:
                replicas: 1
              ports:
                web:
                  port: 80
                  expose: true
                  exposedPort: 80
                websecure:
                  port: 443
                  expose: true
                  exposedPort: 443
              service:
                type: LoadBalancer
                spec:
                  externalIPs:
                    - "{{ ansible_host }}"
              ingressClass:
                enabled: true
                isDefaultClass: true
              logs:
                general:
                  level: INFO
              metrics:
                prometheus:
                  enabled: true
          environment:
            KUBECONFIG: "{{ k3s_kubeconfig }}"

    - name: "üîí D√©ploiement de cert-manager (Gestion TLS)"
      block:
        - name: "Ajout du d√©p√¥t Helm cert-manager"
          kubernetes.core.helm_repository:
            name: jetstack
            repo_url: "https://charts.jetstack.io"
          environment:
            KUBECONFIG: "{{ k3s_kubeconfig }}"

        - name: "Nettoyage des CRDs cert-manager orphelins"
          kubernetes.core.k8s:
            name: "{{ item }}"
            api_version: apiextensions.k8s.io/v1
            kind: CustomResourceDefinition
            state: absent
          loop:
            - certificaterequests.cert-manager.io
            - certificates.cert-manager.io
            - challenges.acme.cert-manager.io
            - clusterissuers.cert-manager.io
            - issuers.cert-manager.io
            - orders.acme.cert-manager.io
          ignore_errors: true
          environment:
            KUBECONFIG: "{{ k3s_kubeconfig }}"

        - name: "D√©ploiement de cert-manager"
          kubernetes.core.helm:
            name: cert-manager
            chart_ref: "jetstack/cert-manager"
            chart_version: "{{ cert_manager_version }}"
            release_namespace: cert-manager
            create_namespace: true
            wait: true
            wait_timeout: "600s"
            values:
              installCRDs: true
              global:
                leaderElection:
                  namespace: cert-manager
          environment:
            KUBECONFIG: "{{ k3s_kubeconfig }}"

    - name: "‚öñÔ∏è  D√©ploiement de MetalLB (Load Balancer)"
      block:
        - name: "Ajout du d√©p√¥t Helm MetalLB"
          kubernetes.core.helm_repository:
            name: metallb
            repo_url: "https://metallb.github.io/metallb"
          environment:
            KUBECONFIG: "{{ k3s_kubeconfig }}"

        - name: "D√©ploiement de MetalLB"
          kubernetes.core.helm:
            name: metallb
            chart_ref: "metallb/metallb"
            chart_version: "{{ metallb_chart_version }}"
            release_namespace: metallb-system
            create_namespace: true
            wait: true
            wait_timeout: "600s"
          environment:
            KUBECONFIG: "{{ k3s_kubeconfig }}"

        - name: "Configuration IPAddressPool MetalLB"
          kubernetes.core.k8s:
            state: present
            definition:
              apiVersion: metallb.io/v1beta1
              kind: IPAddressPool
              metadata:
                name: lions-ip-pool
                namespace: metallb-system
              spec:
                addresses:
                  - "{{ ansible_host }}/32"
          environment:
            KUBECONFIG: "{{ k3s_kubeconfig }}"

        - name: "Configuration L2Advertisement MetalLB"
          kubernetes.core.k8s:
            state: present
            definition:
              apiVersion: metallb.io/v1beta1
              kind: L2Advertisement
              metadata:
                name: lions-l2-advert
                namespace: metallb-system
              spec:
                ipAddressPools:
                  - lions-ip-pool
          environment:
            KUBECONFIG: "{{ k3s_kubeconfig }}"

    # =========================================================================
    # PHASE 7: D√âPLOIEMENT DU MONITORING COMPLET (NOUVELLE PHASE)
    # =========================================================================

    - name: "üìä PHASE 7 - D√©ploiement du syst√®me de monitoring complet"
      debug:
        msg: |
          D√©ploiement de Prometheus et Grafana avec configuration optimis√©e
          Namespace: {{ monitoring_config.namespace }}
          R√©tention Prometheus: {{ monitoring_config.prometheus_retention }}

    - name: "üìä Pr√©paration du monitoring"
      block:
        - name: "Cr√©ation du namespace monitoring"
          kubernetes.core.k8s:
            name: "{{ monitoring_config.namespace }}"
            api_version: v1
            kind: Namespace
            state: present
          environment:
            KUBECONFIG: "{{ k3s_kubeconfig }}"

        - name: "Ajout du d√©p√¥t Helm Prometheus Community"
          kubernetes.core.helm_repository:
            name: prometheus-community
            repo_url: "https://prometheus-community.github.io/helm-charts"
          environment:
            KUBECONFIG: "{{ k3s_kubeconfig }}"

        - name: "Mise √† jour des d√©p√¥ts Helm"
          command: "helm repo update"
          environment:
            KUBECONFIG: "{{ k3s_kubeconfig }}"

    - name: "üîç Nettoyage des installations monitoring pr√©c√©dentes"
      block:
        - name: "V√©rification de l'existence d'une installation Prometheus existante"
          command: "helm list -n {{ monitoring_config.namespace }} --filter prometheus"
          register: existing_prometheus
          environment:
            KUBECONFIG: "{{ k3s_kubeconfig }}"
          ignore_errors: yes

        - name: "Suppression de l'installation Prometheus d√©faillante si n√©cessaire"
          command: "helm uninstall prometheus -n {{ monitoring_config.namespace }}"
          environment:
            KUBECONFIG: "{{ k3s_kubeconfig }}"
          when:
            - existing_prometheus.stdout is defined
            - "'failed' in existing_prometheus.stdout or 'pending' in existing_prometheus.stdout"
          ignore_errors: yes

        - name: "Attente de la suppression compl√®te"
          pause:
            seconds: 30
          when:
            - existing_prometheus.stdout is defined
            - "'failed' in existing_prometheus.stdout or 'pending' in existing_prometheus.stdout"

    - name: "üöÄ D√©ploiement de kube-prometheus-stack optimis√©"
      kubernetes.core.helm:
        name: prometheus
        chart_ref: "prometheus-community/kube-prometheus-stack"
        chart_version: "{{ kube_prometheus_stack_version }}"
        release_namespace: "{{ monitoring_config.namespace }}"
        create_namespace: true
        wait: true
        wait_timeout: "900s"
        force: true
        values:
          # Configuration Prometheus optimis√©e
          prometheus:
            prometheusSpec:
              retention: "{{ monitoring_config.prometheus_retention }}"
              retentionSize: "8GB"
              storageSpec:
                volumeClaimTemplate:
                  spec:
                    accessModes: ["ReadWriteOnce"]
                    resources:
                      requests:
                        storage: "{{ monitoring_config.prometheus_storage }}"
              resources:
                requests:
                  cpu: "200m"
                  memory: "512Mi"
                limits:
                  cpu: "{{ monitoring_config.resource_limits.prometheus_cpu }}"
                  memory: "{{ monitoring_config.resource_limits.prometheus_memory }}"
              # Configuration pour √©viter les probl√®mes de performance
              evaluationInterval: "30s"
              scrapeInterval: "30s"
              scrapeTimeout: "10s"
              # R√®gles d'alerte essentielles uniquement
              ruleSelector:
                matchLabels:
                  app: lions-infrastructure
                  tier: essential
            service:
              type: ClusterIP
              port: 9090
              targetPort: 9090

          # Configuration Grafana optimis√©e
          grafana:
            enabled: true
            adminPassword: "{{ monitoring_config.grafana_admin_password }}"
            persistence:
              enabled: true
              size: "{{ monitoring_config.grafana_storage }}"
              accessModes:
                - ReadWriteOnce
            service:
              type: NodePort
              nodePort: 30000
              port: 80
              targetPort: 3000
            resources:
              requests:
                cpu: "100m"
                memory: "128Mi"
              limits:
                cpu: "{{ monitoring_config.resource_limits.grafana_cpu }}"
                memory: "{{ monitoring_config.resource_limits.grafana_memory }}"
            # Configuration pour de meilleures performances
            grafana.ini:
              server:
                root_url: "http://{{ ansible_host }}:30000"
                serve_from_sub_path: false
              database:
                type: sqlite3
                cache_mode: shared
              users:
                allow_sign_up: false
                auto_assign_org: true
                auto_assign_org_role: Viewer
                default_theme: dark
              dashboards:
                default_home_dashboard_path: /tmp/dashboards/lions-overview.json
            # Dashboards par d√©faut optimis√©s
            defaultDashboardsEnabled: true
            defaultDashboardsTimezone: Europe/Paris
            sidecar:
              dashboards:
                enabled: true
                label: grafana_dashboard
                folder: /tmp/dashboards
                searchNamespace: ALL
              datasources:
                enabled: true
                defaultDatasourceEnabled: true

          # Configuration AlertManager simplifi√©e
          alertmanager:
            enabled: true
            alertmanagerSpec:
              storage:
                volumeClaimTemplate:
                  spec:
                    accessModes: ["ReadWriteOnce"]
                    resources:
                      requests:
                        storage: "2Gi"
              resources:
                requests:
                  cpu: "50m"
                  memory: "64Mi"
                limits:
                  cpu: "100m"
                  memory: "256Mi"
            service:
              type: ClusterIP
              port: 9093
            config:
              global:
                resolve_timeout: 5m
              route:
                group_by: ['alertname', 'cluster', 'service']
                group_wait: 30s
                group_interval: 5m
                repeat_interval: 12h
                receiver: 'lions-default'
              receivers:
                - name: 'lions-default'
                  webhook_configs:
                    - url: 'http://localhost:9093/api/v1/alerts'
                      send_resolved: true

          # Node Exporter optimis√©
          nodeExporter:
            enabled: true
            serviceMonitor:
              enabled: true
              interval: "30s"
              scrapeTimeout: "10s"

          # Kube State Metrics optimis√©
          kubeStateMetrics:
            enabled: true
            resources:
              requests:
                cpu: "10m"
                memory: "32Mi"
              limits:
                cpu: "100m"
                memory: "128Mi"

          # Configuration Service Monitor
          serviceMonitor:
            enabled: true
            interval: "30s"
            scrapeTimeout: "10s"

          # D√©sactivation des composants non essentiels pour optimiser les performances
          kubeApiServer:
            enabled: false
          kubelet:
            enabled: true
            serviceMonitor:
              interval: "30s"
          kubeControllerManager:
            enabled: false
          coreDns:
            enabled: true
            serviceMonitor:
              interval: "30s"
          kubeEtcd:
            enabled: false
          kubeScheduler:
            enabled: false
          kubeProxy:
            enabled: false

      environment:
        KUBECONFIG: "{{ k3s_kubeconfig }}"

    - name: "‚è≥ V√©rification du d√©ploiement du monitoring"
      block:
        - name: "Attente de la disponibilit√© des pods Prometheus"
          kubernetes.core.k8s_info:
            api_version: v1
            kind: Pod
            namespace: "{{ monitoring_config.namespace }}"
            label_selectors:
              - "app.kubernetes.io/name=prometheus"
            wait: true
            wait_condition:
              type: Ready
              status: "True"
            wait_timeout: 300
          environment:
            KUBECONFIG: "{{ k3s_kubeconfig }}"

        - name: "Attente de la disponibilit√© des pods Grafana"
          kubernetes.core.k8s_info:
            api_version: v1
            kind: Pod
            namespace: "{{ monitoring_config.namespace }}"
            label_selectors:
              - "app.kubernetes.io/name=grafana"
            wait: true
            wait_condition:
              type: Ready
              status: "True"
            wait_timeout: 300
          environment:
            KUBECONFIG: "{{ k3s_kubeconfig }}"

        - name: "V√©rification de l'√©tat des services monitoring"
          command: "/usr/local/bin/k3s kubectl get pods -n {{ monitoring_config.namespace }}"
          register: monitoring_pods_status
          environment:
            KUBECONFIG: "{{ k3s_kubeconfig }}"

        - name: "Affichage de l'√©tat du monitoring"
          debug:
            msg: |
              ==========================================================
              üìä √âTAT DU MONITORING
              ==========================================================
              {{ monitoring_pods_status.stdout }}
              ==========================================================

    - name: "üîó Configuration des acc√®s monitoring"
      block:
        - name: "Cr√©ation du service NodePort pour Prometheus"
          kubernetes.core.k8s:
            state: present
            definition:
              apiVersion: v1
              kind: Service
              metadata:
                name: prometheus-external
                namespace: "{{ monitoring_config.namespace }}"
                labels:
                  app: prometheus-external
              spec:
                type: NodePort
                ports:
                  - port: 9090
                    targetPort: 9090
                    nodePort: 30090
                    protocol: TCP
                    name: prometheus
                selector:
                  app.kubernetes.io/name: prometheus
          environment:
            KUBECONFIG: "{{ k3s_kubeconfig }}"

        - name: "R√©cup√©ration du mot de passe Grafana"
          kubernetes.core.k8s_info:
            api_version: v1
            kind: Secret
            name: prometheus-grafana
            namespace: "{{ monitoring_config.namespace }}"
          register: grafana_secret
          environment:
            KUBECONFIG: "{{ k3s_kubeconfig }}"

        - name: "Affichage des informations d'acc√®s"
          debug:
            msg: |
              ==========================================================
              üîê INFORMATIONS D'ACC√àS MONITORING
              ==========================================================
              üéØ Grafana:
                URL: http://{{ ansible_host }}:30000
                Utilisateur: admin
                Mot de passe: {{ monitoring_config.grafana_admin_password }}
              
              üìä Prometheus:
                URL: http://{{ ansible_host }}:30090
                Interface Web: http://{{ ansible_host }}:30090/graph
              
              üö® AlertManager:
                URL interne: http://prometheus-kube-prometheus-alertmanager:9093
              ==========================================================

    # =========================================================================
    # PHASE 8: V√âRIFICATIONS FINALES ET RAPPORT
    # =========================================================================

    - name: "‚úÖ PHASE 8 - V√©rifications finales et tests d'int√©gration"
      debug:
        msg: "Validation finale de l'installation compl√®te avec monitoring"

    - name: "üè• Tests de sant√© des composants"
      block:
        - name: "V√©rification des n≈ìuds K3s"
          command: "/usr/local/bin/k3s kubectl get nodes -o wide"
          register: final_nodes_check
          environment:
            KUBECONFIG: "{{ k3s_kubeconfig }}"

        - name: "V√©rification des pods syst√®me"
          command: "/usr/local/bin/k3s kubectl get pods --all-namespaces"
          register: final_pods_check
          environment:
            KUBECONFIG: "{{ k3s_kubeconfig }}"

        - name: "V√©rification des services"
          command: "/usr/local/bin/k3s kubectl get services --all-namespaces"
          register: final_services_check
          environment:
            KUBECONFIG: "{{ k3s_kubeconfig }}"

        - name: "V√©rification sp√©cifique du monitoring"
          command: "/usr/local/bin/k3s kubectl get pods -n {{ monitoring_config.namespace }} -o wide"
          register: monitoring_final_check
          environment:
            KUBECONFIG: "{{ k3s_kubeconfig }}"

    - name: "üéØ Tests de connectivit√© des services"
      block:
        - name: "Test de connectivit√© Traefik"
          uri:
            url: "http://{{ ansible_host }}"
            method: GET
            status_code: [200, 404, 503]
            timeout: 10
          register: traefik_connectivity
          ignore_errors: true

        - name: "Test de connectivit√© Grafana"
          uri:
            url: "http://{{ ansible_host }}:30000"
            method: GET
            status_code: [200, 302]
            timeout: 15
          register: grafana_connectivity
          ignore_errors: true

        - name: "Test de connectivit√© Prometheus"
          uri:
            url: "http://{{ ansible_host }}:30090"
            method: GET
            status_code: [200]
            timeout: 15
          register: prometheus_connectivity
          ignore_errors: true

    - name: "üìä RAPPORT FINAL D'INSTALLATION COMPL√àTE"
      debug:
        msg: |
          ==========================================================
          üéâ INSTALLATION K3S LIONS + MONITORING TERMIN√âE
          ==========================================================
          
          üìç INFORMATIONS G√âN√âRALES:
          ‚Ä¢ Host: {{ ansible_host }}
          ‚Ä¢ Version K3s: {{ k3s_version }}
          ‚Ä¢ Environment: {{ lions_env | default('development') }}
          ‚Ä¢ Date: {{ ansible_date_time.iso8601 }}
          
          üîß COMPOSANTS INSTALL√âS:
          ‚Ä¢ ‚úÖ K3s Server (API: :6443)
          ‚Ä¢ ‚úÖ kubectl
          ‚Ä¢ ‚úÖ Helm v3
          ‚Ä¢ ‚úÖ Traefik Ingress Controller
          ‚Ä¢ ‚úÖ cert-manager (TLS)
          ‚Ä¢ ‚úÖ MetalLB Load Balancer
          ‚Ä¢ ‚úÖ Prometheus Stack {{ kube_prometheus_stack_version }}
          ‚Ä¢ ‚úÖ Grafana avec dashboards
          ‚Ä¢ ‚úÖ AlertManager
          ‚Ä¢ ‚úÖ Node Exporter
          
          üåê ACC√àS SERVICES:
          ‚Ä¢ API Kubernetes: https://{{ ansible_host }}:6443
          ‚Ä¢ Traefik: http://{{ ansible_host }} {{ '‚úÖ' if traefik_connectivity.status == 200 else '‚ö†Ô∏è' }}
          ‚Ä¢ Grafana: http://{{ ansible_host }}:30000 {{ '‚úÖ' if grafana_connectivity.status in [200, 302] else '‚ö†Ô∏è' }}
          ‚Ä¢ Prometheus: http://{{ ansible_host }}:30090 {{ '‚úÖ' if prometheus_connectivity.status == 200 else '‚ö†Ô∏è' }}
          
          üîê CREDENTIALS:
          ‚Ä¢ Grafana User: admin
          ‚Ä¢ Grafana Password: {{ monitoring_config.grafana_admin_password }}
          
          üìà STATISTIQUES:
          ‚Ä¢ N≈ìuds: {{ final_nodes_check.stdout_lines | length - 1 }}
          ‚Ä¢ Namespaces: {{ (final_pods_check.stdout_lines | select('match', '^[^\\s]+\\s+[^\\s]+\\s+') | list | map('regex_replace', '^([^\\s]+)\\s+.*', '\\1') | unique | list) | length }}
          ‚Ä¢ Pods actifs: {{ (final_pods_check.stdout_lines | select('match', '.*Running.*') | list) | length }}
          ‚Ä¢ Pods monitoring: {{ (monitoring_final_check.stdout_lines | select('match', '.*Running.*') | list) | length }} / {{ monitoring_final_check.stdout_lines | length - 1 }}
          
          üöÄ PROCHAINES √âTAPES:
          1. ‚úÖ Monitoring op√©rationnel (Prometheus + Grafana)
          2. Configurer les alertes personnalis√©es
          3. Configurer les certificats SSL (Let's Encrypt)
          4. D√©ployer les applications LIONS
          5. Configurer les sauvegardes automatiques
          
          üí° CONSEILS D'UTILISATION:
          ‚Ä¢ Acc√©dez √† Grafana pour visualiser vos m√©triques
          ‚Ä¢ Consultez Prometheus pour les requ√™tes avanc√©es
          ‚Ä¢ Les dashboards par d√©faut sont pr√©-install√©s
          ‚Ä¢ Le monitoring est configur√© pour une r√©tention de {{ monitoring_config.prometheus_retention }}
          
          ==========================================================

  # =============================================================================
  # HANDLERS
  # =============================================================================
  handlers:
    - name: "reload systemd and restart k3s"
      listen: "reload systemd and restart k3s"
      systemd:
        name: k3s
        state: restarted
        daemon_reload: yes
      register: k3s_handler_restart

    - name: "verify k3s after handler restart"
      listen: "reload systemd and restart k3s"
      wait_for:
        port: 6443
        host: localhost
        timeout: 120
      when: k3s_handler_restart is succeeded