---
# =========================================================================
# LIONS INFRASTRUCTURE 5.0 - DÃ‰PLOIEMENT OLLAMA AI SERVICE
# =========================================================================
# Description: TÃ¢ches avancÃ©es de dÃ©ploiement du service IA Ollama avec intÃ©gration complÃ¨te
# Composant: Ollama LLM Service - Orchestration Kubernetes Enterprise
# Version: 5.0.0
# Auteur: Ã‰quipe DevOps LIONS
# Date: 2025-05-29
# DÃ©pendances: Kubernetes 1.24+, Persistent Storage, GPU (optionnel), Vault, Prometheus
# Documentation: https://docs.lions.dev/services/ollama/deployment
# =========================================================================

# =========================================================================
# CHARGEMENT DE LA CONFIGURATION ENVIRONNEMENT
# =========================================================================
- name: "[OLLAMA-DEPLOY] Charger la configuration d'environnement"
  block:
    - name: "DÃ©finir les variables d'environnement Ollama depuis LIONS_ENV"
      set_fact:
        ollama_config:
          # Configuration principale
          service_name: "{{ lookup('env', 'LIONS_OLLAMA_SERVICE_NAME') | default('ollama') }}"
          namespace: "{{ lookup('env', 'LIONS_OLLAMA_NAMESPACE') | default('ai') }}"
          version: "{{ lookup('env', 'LIONS_OLLAMA_VERSION') | default('0.1.26') }}"
          enabled: "{{ lookup('env', 'LIONS_OLLAMA_ENABLED') | default('true') | bool }}"
          port: "{{ lookup('env', 'LIONS_OLLAMA_PORT') | default('11434') | int }}"

          # Configuration rÃ©seau et DNS
          domain: "{{ lookup('env', 'LIONS_DNS_FULL_DOMAIN') | default('lions.local') }}"
          subdomain: "ollama"
          full_domain: "ollama.{{ lookup('env', 'LIONS_DNS_FULL_DOMAIN') | default('lions.local') }}"

          # Configuration stockage
          storage_enabled: true
          storage_size: "{{ lookup('env', 'LIONS_OLLAMA_STORAGE_SIZE') | default('100Gi') }}"
          storage_class: "{{ lookup('env', 'LIONS_STORAGE_CLASS_DEFAULT') | default('local-path') }}"

          # Configuration GPU
          gpu_enabled: "{{ lookup('env', 'LIONS_OLLAMA_GPU_ENABLED') | default('false') | bool }}"

          # Configuration sÃ©curitÃ©
          tls_enabled: "{{ lookup('env', 'LIONS_SECURITY_TLS_ENABLED') | default('true') | bool }}"
          network_policies: "{{ lookup('env', 'LIONS_SECURITY_NETWORK_POLICIES') | default('true') | bool }}"

          # Configuration monitoring
          monitoring_enabled: "{{ lookup('env', 'LIONS_MONITORING_ENABLED') | default('true') | bool }}"
          monitoring_namespace: "{{ lookup('env', 'LIONS_MONITORING_NAMESPACE') | default('monitoring') }}"

          # Configuration Vault
          vault_enabled: "{{ lookup('env', 'LIONS_VAULT_ENABLED') | default('true') | bool }}"
          vault_addr: "{{ lookup('env', 'LIONS_VAULT_ADDR') | default('') }}"
          vault_namespace: "{{ lookup('env', 'LIONS_VAULT_NAMESPACE') | default('vault-system') }}"

          # Configuration environnement
          environment: "{{ lookup('env', 'LIONS_ENVIRONMENT') | default('development') }}"
          debug_mode: "{{ lookup('env', 'LIONS_DEBUG_MODE') | default('false') | bool }}"
          dry_run: "{{ lookup('env', 'LIONS_DRY_RUN') | default('false') | bool }}"

    - name: "DÃ©finir les ressources par environnement"
      set_fact:
        ollama_resources: >-
          {%- if ollama_config.environment == 'production' -%}
          {
            "requests": {
              "cpu": "{{ lookup('env', 'LIONS_RESOURCES_LARGE_CPU_REQUEST') | default('500m') }}",
              "memory": "{{ lookup('env', 'LIONS_RESOURCES_LARGE_MEMORY_REQUEST') | default('1Gi') }}"
            },
            "limits": {
              "cpu": "{{ lookup('env', 'LIONS_RESOURCES_LARGE_CPU_LIMIT') | default('2000m') }}",
              "memory": "{{ lookup('env', 'LIONS_RESOURCES_LARGE_MEMORY_LIMIT') | default('4Gi') }}"
            }
          }
          {%- elif ollama_config.environment == 'staging' -%}
          {
            "requests": {
              "cpu": "{{ lookup('env', 'LIONS_RESOURCES_MEDIUM_CPU_REQUEST') | default('200m') }}",
              "memory": "{{ lookup('env', 'LIONS_RESOURCES_MEDIUM_MEMORY_REQUEST') | default('512Mi') }}"
            },
            "limits": {
              "cpu": "{{ lookup('env', 'LIONS_RESOURCES_MEDIUM_CPU_LIMIT') | default('1000m') }}",
              "memory": "{{ lookup('env', 'LIONS_RESOURCES_MEDIUM_MEMORY_LIMIT') | default('2Gi') }}"
            }
          }
          {%- else -%}
          {
            "requests": {
              "cpu": "{{ lookup('env', 'LIONS_RESOURCES_SMALL_CPU_REQUEST') | default('100m') }}",
              "memory": "{{ lookup('env', 'LIONS_RESOURCES_SMALL_MEMORY_REQUEST') | default('128Mi') }}"
            },
            "limits": {
              "cpu": "{{ lookup('env', 'LIONS_RESOURCES_SMALL_CPU_LIMIT') | default('500m') }}",
              "memory": "{{ lookup('env', 'LIONS_RESOURCES_SMALL_MEMORY_LIMIT') | default('512Mi') }}"
            }
          }
          {%- endif -%}

    - name: "GÃ©nÃ©rer l'identifiant de dÃ©ploiement unique"
      set_fact:
        deployment_metadata:
          id: "{{ ansible_date_time.iso8601_basic_short }}-{{ ansible_hostname | hash('md5') | truncate(8, True, '') }}"
          timestamp: "{{ ansible_date_time.epoch }}"
          version: "{{ ollama_config.version }}"
          environment: "{{ ollama_config.environment }}"
          operator: "{{ ansible_user_id | default('system') }}"

    - name: "Afficher la configuration de dÃ©ploiement"
      debug:
        msg:
          - "ğŸš€ Configuration Ollama AI Service LIONS 5.0:"
          - "  ğŸ“ ID DÃ©ploiement: {{ deployment_metadata.id }}"
          - "  ğŸ·ï¸  Service: {{ ollama_config.service_name }}"
          - "  ğŸ“¦ Namespace: {{ ollama_config.namespace }}"
          - "  ğŸ”– Version: {{ ollama_config.version }}"
          - "  ğŸŒ Environnement: {{ ollama_config.environment }}"
          - "  ğŸŒ Domaine: {{ ollama_config.full_domain }}"
          - "  ğŸ–¥ï¸  GPU: {{ 'ActivÃ©' if ollama_config.gpu_enabled else 'DÃ©sactivÃ©' }}"
          - "  ğŸ’¾ Stockage: {{ ollama_config.storage_size if ollama_config.storage_enabled else 'DÃ©sactivÃ©' }}"
          - "  ğŸ”’ TLS: {{ 'ActivÃ©' if ollama_config.tls_enabled else 'DÃ©sactivÃ©' }}"
          - "  ğŸ“Š Monitoring: {{ 'ActivÃ©' if ollama_config.monitoring_enabled else 'DÃ©sactivÃ©' }}"
      when: ollama_config.debug_mode

  tags: [ollama, ai-ml, deploy, config]

# =========================================================================
# VALIDATION PRÃ‰-DÃ‰PLOIEMENT AVANCÃ‰E
# =========================================================================
- name: "[OLLAMA-DEPLOY] Validation des prÃ©requis systÃ¨me"
  block:
    - name: "VÃ©rifier si Ollama est activÃ©"
      meta: end_play
      when: not ollama_config.enabled

    - name: "Valider la configuration Kubernetes"
      kubernetes.core.k8s_info:
        api_version: v1
        kind: Node
      register: k8s_nodes
      failed_when: k8s_nodes.resources | length == 0

    - name: "VÃ©rifier l'existence du namespace cible"
      kubernetes.core.k8s:
        state: present
        definition:
          apiVersion: v1
          kind: Namespace
          metadata:
            name: "{{ ollama_config.namespace }}"
            labels:
              lions.dev/environment: "{{ ollama_config.environment }}"
              lions.dev/service-type: "ai-ml"
              lions.dev/managed-by: "ansible"
        wait: true
        wait_timeout: 60

    - name: "Valider la disponibilitÃ© des ressources GPU"
      kubernetes.core.k8s_info:
        api_version: v1
        kind: Node
        label_selectors:
          - "nvidia.com/gpu"
      register: gpu_nodes
      when: ollama_config.gpu_enabled
      failed_when:
        - ollama_config.gpu_enabled
        - gpu_nodes.resources | length == 0

    - name: "VÃ©rifier la connectivitÃ© Vault"
      uri:
        url: "{{ ollama_config.vault_addr }}/v1/sys/health"
        method: GET
        status_code: [200, 429, 472, 473, 501, 503]
        timeout: 10
      register: vault_health
      when: ollama_config.vault_enabled
      failed_when: false

    - name: "Valider les classes de stockage disponibles"
      kubernetes.core.k8s_info:
        api_version: storage.k8s.io/v1
        kind: StorageClass
        name: "{{ ollama_config.storage_class }}"
      register: storage_class_check
      when: ollama_config.storage_enabled
      failed_when:
        - ollama_config.storage_enabled
        - storage_class_check.resources | length == 0

    - name: "VÃ©rifier les quotas de ressources du namespace"
      kubernetes.core.k8s_info:
        api_version: v1
        kind: ResourceQuota
        namespace: "{{ ollama_config.namespace }}"
      register: resource_quotas

    - name: "Validation de la configuration rÃ©seau"
      kubernetes.core.k8s_info:
        api_version: networking.k8s.io/v1
        kind: NetworkPolicy
        namespace: "{{ ollama_config.namespace }}"
      register: network_policies
      when: ollama_config.network_policies

  rescue:
    - name: "GÃ©rer l'Ã©chec de validation des prÃ©requis"
      debug:
        msg: |
          âŒ Ã‰chec de validation des prÃ©requis Ollama AI:
          - Kubernetes: {{ 'OK' if k8s_nodes.resources | default([]) | length > 0 else 'ERREUR' }}
          - GPU: {{ 'OK' if not ollama_config.gpu_enabled or (gpu_nodes.resources | default([]) | length > 0) else 'ERREUR' }}
          - Vault: {{ 'OK' if not ollama_config.vault_enabled or vault_health.status == 200 else 'AVERTISSEMENT' }}
          - Stockage: {{ 'OK' if not ollama_config.storage_enabled or (storage_class_check.resources | default([]) | length > 0) else 'ERREUR' }}

    - fail:
        msg: "Les prÃ©requis systÃ¨me pour Ollama AI ne sont pas satisfaits"

  tags: [ollama, ai-ml, deploy, validation]

# =========================================================================
# GESTION AVANCÃ‰E DES SECRETS ET CONFIGURATION
# =========================================================================
- name: "[OLLAMA-DEPLOY] Configuration des secrets et donnÃ©es sensibles"
  block:
    - name: "GÃ©nÃ©rer les identifiants d'API Ollama"
      set_fact:
        ollama_credentials:
          api_key: "{{ lookup('password', '/dev/null length=64 chars=ascii_letters,digits') }}"
          admin_token: "{{ lookup('password', '/dev/null length=32 chars=ascii_letters,digits') }}"
          deployment_secret: "{{ lookup('password', '/dev/null length=16 chars=ascii_letters,digits') }}"
      no_log: true

    - name: "Stocker les secrets dans Vault"
      uri:
        url: "{{ ollama_config.vault_addr }}/v1/secret/data/lions/{{ ollama_config.environment }}/ollama/credentials"
        method: POST
        headers:
          X-Vault-Token: "{{ vault_token | default('') }}"
          Content-Type: "application/json"
        body_format: json
        body:
          data:
            api_key: "{{ ollama_credentials.api_key }}"
            admin_token: "{{ ollama_credentials.admin_token }}"
            deployment_id: "{{ deployment_metadata.id }}"
            created_at: "{{ ansible_date_time.iso8601 }}"
            environment: "{{ ollama_config.environment }}"
        status_code: [200, 204]
        timeout: 30
      register: vault_secret_result
      when:
        - ollama_config.vault_enabled
        - vault_health.status | default(0) == 200
      no_log: true

    - name: "CrÃ©er le secret Kubernetes pour Ollama"
      kubernetes.core.k8s:
        state: present
        definition:
          apiVersion: v1
          kind: Secret
          metadata:
            name: "{{ ollama_config.service_name }}-secrets"
            namespace: "{{ ollama_config.namespace }}"
            labels:
              app.kubernetes.io/name: "{{ ollama_config.service_name }}"
              app.kubernetes.io/instance: "{{ ollama_config.service_name }}-{{ ollama_config.environment }}"
              app.kubernetes.io/component: "ai-ml"
              app.kubernetes.io/version: "{{ ollama_config.version }}"
              lions.dev/environment: "{{ ollama_config.environment }}"
              lions.dev/deployment-id: "{{ deployment_metadata.id }}"
            annotations:
              lions.dev/created-by: "ansible"
              lions.dev/last-updated: "{{ ansible_date_time.iso8601 }}"
          type: Opaque
          data:
            api-key: "{{ ollama_credentials.api_key | b64encode }}"
            admin-token: "{{ ollama_credentials.admin_token | b64encode }}"
            deployment-secret: "{{ ollama_credentials.deployment_secret | b64encode }}"
        wait: true
        wait_timeout: 60
      no_log: true

  rescue:
    - name: "Utiliser la configuration de secours pour les secrets"
      debug:
        msg: "âš ï¸  Utilisation de la configuration de secours pour les secrets Ollama"

    - set_fact:
        ollama_credentials:
          api_key: "fallback-api-key-{{ deployment_metadata.id | truncate(8, True, '') }}"
          admin_token: "fallback-admin-{{ deployment_metadata.id | truncate(8, True, '') }}"
          deployment_secret: "fallback-secret"

  tags: [ollama, ai-ml, deploy, secrets]

# =========================================================================
# DÃ‰PLOIEMENT DES RESSOURCES KUBERNETES CORE
# =========================================================================
- name: "[OLLAMA-DEPLOY] DÃ©ployer les ressources Kubernetes fondamentales"
  block:
    - name: "DÃ©ployer le ServiceAccount avec RBAC"
      kubernetes.core.k8s:
        state: present
        definition: "{{ lookup('template', 'serviceaccount.yml.j2') | from_yaml }}"
        wait: true
        wait_timeout: 60

    - name: "CrÃ©er la ConfigMap de configuration Ollama"
      kubernetes.core.k8s:
        state: present
        definition: "{{ lookup('template', 'configmap.yml.j2') | from_yaml }}"
        wait: true
        wait_timeout: 60

    - name: "Provisionner le stockage persistant"
      kubernetes.core.k8s:
        state: present
        definition: "{{ lookup('template', 'persistentvolumeclaim.yml.j2') | from_yaml }}"
        wait: true
        wait_condition:
          type: Bound
          status: "True"
        wait_timeout: 300
      when: ollama_config.storage_enabled

    - name: "DÃ©ployer le Service rÃ©seau"
      kubernetes.core.k8s:
        state: present
        definition: "{{ lookup('template', 'service.yml.j2') | from_yaml }}"
        wait: true
        wait_timeout: 60

    - name: "DÃ©ployer les politiques rÃ©seau"
      kubernetes.core.k8s:
        state: present
        definition:
          apiVersion: networking.k8s.io/v1
          kind: NetworkPolicy
          metadata:
            name: "{{ ollama_config.service_name }}-network-policy"
            namespace: "{{ ollama_config.namespace }}"
            labels:
              app.kubernetes.io/name: "{{ ollama_config.service_name }}"
              lions.dev/environment: "{{ ollama_config.environment }}"
          spec:
            podSelector:
              matchLabels:
                app.kubernetes.io/name: "{{ ollama_config.service_name }}"
            policyTypes:
              - Ingress
              - Egress
            ingress:
              - from:
                  - namespaceSelector:
                      matchLabels:
                        name: "{{ ollama_config.monitoring_namespace }}"
                  - podSelector:
                      matchLabels:
                        app.kubernetes.io/name: traefik
                ports:
                  - protocol: TCP
                    port: "{{ ollama_config.port }}"
            egress:
              - to: []
                ports:
                  - protocol: TCP
                    port: 443
                  - protocol: TCP
                    port: 80
                  - protocol: UDP
                    port: 53
        wait: true
        wait_timeout: 60
      when: ollama_config.network_policies

  rescue:
    - name: "GÃ©rer l'Ã©chec de dÃ©ploiement des ressources fondamentales"
      debug:
        msg: "âŒ Ã‰chec du dÃ©ploiement des ressources Kubernetes fondamentales"
    - fail:
        msg: "Le dÃ©ploiement des ressources fondamentales Ollama a Ã©chouÃ©"

  tags: [ollama, ai-ml, deploy, kubernetes, core]

# =========================================================================
# DÃ‰PLOIEMENT DE L'APPLICATION OLLAMA
# =========================================================================
- name: "[OLLAMA-DEPLOY] DÃ©ployer l'application Ollama AI"
  block:
    - name: "DÃ©ployer l'application Ollama avec stratÃ©gie de dÃ©ploiement intelligente"
      kubernetes.core.k8s:
        state: present
        definition: "{{ lookup('template', 'deployment.yml.j2') | from_yaml }}"
        wait: true
        wait_condition:
          type: Available
          status: "True"
        wait_timeout: 1200  # Timeout Ã©tendu pour les modÃ¨les IA
      register: deployment_result

    - name: "VÃ©rifier le statut de dÃ©ploiement dÃ©taillÃ©"
      kubernetes.core.k8s_info:
        api_version: apps/v1
        kind: Deployment
        namespace: "{{ ollama_config.namespace }}"
        name: "{{ ollama_config.service_name }}"
      register: deployment_status
      until:
        - deployment_status.resources | length > 0
        - deployment_status.resources[0].status.readyReplicas | default(0) > 0
        - deployment_status.resources[0].status.readyReplicas == deployment_status.resources[0].spec.replicas
      retries: 80
      delay: 15

    - name: "Obtenir les informations des pods dÃ©ployÃ©s"
      kubernetes.core.k8s_info:
        api_version: v1
        kind: Pod
        namespace: "{{ ollama_config.namespace }}"
        label_selectors:
          - "app.kubernetes.io/name={{ ollama_config.service_name }}"
          - "app.kubernetes.io/instance={{ ollama_config.service_name }}-{{ ollama_config.environment }}"
      register: ollama_pods

    - name: "Valider la santÃ© des pods"
      assert:
        that:
          - ollama_pods.resources | length > 0
          - ollama_pods.resources | selectattr('status.phase', 'equalto', 'Running') | list | length == ollama_pods.resources | length
          - ollama_pods.resources | selectattr('status.containerStatuses.0.ready', 'equalto', true) | list | length == ollama_pods.resources | length
        fail_msg: "âŒ Les pods Ollama ne sont pas tous en Ã©tat de fonctionnement optimal"
        success_msg: "âœ… Tous les pods Ollama sont dÃ©ployÃ©s et opÃ©rationnels"

  rescue:
    - name: "Diagnostiquer l'Ã©chec de dÃ©ploiement de l'application"
      kubernetes.core.k8s_info:
        api_version: v1
        kind: Event
        namespace: "{{ ollama_config.namespace }}"
        field_selectors:
          - "involvedObject.name={{ ollama_config.service_name }}"
      register: deployment_events

    - name: "Collecter les logs des pods en Ã©chec"
      kubernetes.core.k8s_log:
        namespace: "{{ ollama_config.namespace }}"
        name: "{{ item.metadata.name }}"
        tail_lines: 200
      register: failed_pod_logs
      loop: "{{ ollama_pods.resources | default([]) }}"
      when:
        - ollama_pods.resources is defined
        - item.status.phase != "Running"

    - name: "Afficher les informations de diagnostic"
      debug:
        msg: |
          ğŸ” Diagnostic d'Ã©chec de dÃ©ploiement Ollama:
          {% if deployment_events.resources %}
          ğŸ“‹ Ã‰vÃ©nements rÃ©cents:
          {% for event in deployment_events.resources[:5] %}
          - {{ event.firstTimestamp }}: {{ event.reason }} - {{ event.message }}
          {% endfor %}
          {% endif %}
          
          {% if failed_pod_logs.results %}
          ğŸ“œ Logs des pods en Ã©chec:
          {% for log in failed_pod_logs.results %}
          Pod {{ log.item.metadata.name }}:
          {{ log.log | default('Aucun log disponible') | truncate(500) }}
          {% endfor %}
          {% endif %}

    - fail:
        msg: "Le dÃ©ploiement de l'application Ollama AI a Ã©chouÃ©"

  tags: [ollama, ai-ml, deploy, application]

# =========================================================================
# CONFIGURATION RÃ‰SEAU ET INGRESS
# =========================================================================
- name: "[OLLAMA-DEPLOY] Configurer l'accÃ¨s rÃ©seau et l'exposition"
  block:
    - name: "DÃ©ployer la ressource Ingress avec TLS"
      kubernetes.core.k8s:
        state: present
        definition: "{{ lookup('template', 'ingress.yml.j2') | from_yaml }}"
        wait: true
        wait_timeout: 120
      when: ollama_config.tls_enabled

    - name: "Attendre le provisionnement du certificat TLS"
      kubernetes.core.k8s_info:
        api_version: cert-manager.io/v1
        kind: Certificate
        namespace: "{{ ollama_config.namespace }}"
        name: "{{ ollama_config.service_name }}-tls"
      register: tls_certificate
      until:
        - tls_certificate.resources | length > 0
        - tls_certificate.resources[0].status.conditions | selectattr('type', 'equalto', 'Ready') | selectattr('status', 'equalto', 'True') | list | length > 0
      retries: 30
      delay: 10
      when: ollama_config.tls_enabled

    - name: "VÃ©rifier la rÃ©solution DNS"
      uri:
        url: "{{ 'https' if ollama_config.tls_enabled else 'http' }}://{{ ollama_config.full_domain }}/api/tags"
        method: GET
        status_code: [200, 404, 405]
        timeout: 30
        validate_certs: "{{ not (ollama_config.environment == 'development') }}"
      register: dns_check
      retries: 10
      delay: 30
      until: dns_check is not failed

  rescue:
    - name: "GÃ©rer l'Ã©chec de configuration rÃ©seau"
      debug:
        msg: "âš ï¸  Configuration rÃ©seau partiellement Ã©chouÃ©e. Ollama sera accessible uniquement en interne"

  tags: [ollama, ai-ml, deploy, network, ingress]

# =========================================================================
# VÃ‰RIFICATIONS DE SANTÃ‰ AVANCÃ‰ES
# =========================================================================
- name: "[OLLAMA-DEPLOY] Effectuer les vÃ©rifications de santÃ© complÃ¨tes"
  block:
    - name: "Test de connectivitÃ© API interne"
      uri:
        url: "http://{{ ollama_config.service_name }}.{{ ollama_config.namespace }}.svc.cluster.local:{{ ollama_config.port }}/api/tags"
        method: GET
        status_code: 200
        timeout: 30
      register: internal_health_check
      retries: 20
      delay: 15
      until: internal_health_check is succeeded

    - name: "Test de connectivitÃ© API externe"
      uri:
        url: "{{ 'https' if ollama_config.tls_enabled else 'http' }}://{{ ollama_config.full_domain }}/api/tags"
        method: GET
        status_code: 200
        timeout: 30
        validate_certs: "{{ not (ollama_config.environment == 'development') }}"
      register: external_health_check
      retries: 15
      delay: 20
      until: external_health_check is succeeded
      when: ollama_config.tls_enabled

    - name: "VÃ©rifier la disponibilitÃ© du GPU"
      uri:
        url: "http://{{ ollama_config.service_name }}.{{ ollama_config.namespace }}.svc.cluster.local:{{ ollama_config.port }}/api/ps"
        method: GET
        status_code: 200
        timeout: 30
      register: gpu_check
      when: ollama_config.gpu_enabled

    - name: "Test de fonctionnalitÃ© basique de gÃ©nÃ©ration"
      uri:
        url: "http://{{ ollama_config.service_name }}.{{ ollama_config.namespace }}.svc.cluster.local:{{ ollama_config.port }}/api/generate"
        method: POST
        body_format: json
        body:
          model: "phi3"
          prompt: "Test"
          stream: false
        status_code: [200, 404]  # 404 acceptable si le modÃ¨le n'est pas encore chargÃ©
        timeout: 60
      register: generation_test
      failed_when: false

  rescue:
    - name: "GÃ©rer l'Ã©chec des vÃ©rifications de santÃ©"
      debug:
        msg: |
          âš ï¸  Certaines vÃ©rifications de santÃ© ont Ã©chouÃ©:
          - API interne: {{ 'OK' if internal_health_check is succeeded else 'ERREUR' }}
          - API externe: {{ 'OK' if external_health_check is succeeded else 'ERREUR' }}
          - GPU: {{ 'OK' if not ollama_config.gpu_enabled or gpu_check is succeeded else 'ERREUR' }}
          - GÃ©nÃ©ration: {{ 'OK' if generation_test.status == 200 else 'NON TESTÃ‰' }}

  tags: [ollama, ai-ml, deploy, health-check]

# =========================================================================
# CONFIGURATION DU MONITORING AVANCÃ‰
# =========================================================================
- name: "[OLLAMA-DEPLOY] Configurer le monitoring et l'observabilitÃ©"
  block:
    - name: "DÃ©ployer le ServiceMonitor pour Prometheus"
      kubernetes.core.k8s:
        state: present
        definition: "{{ lookup('template', 'servicemonitor.yml.j2') | from_yaml }}"
        wait: true
        wait_timeout: 60
      when: ollama_config.monitoring_enabled

    - name: "CrÃ©er les alertes Prometheus personnalisÃ©es"
      kubernetes.core.k8s:
        state: present
        definition:
          apiVersion: v1
          kind: ConfigMap
          metadata:
            name: "{{ ollama_config.service_name }}-alerts"
            namespace: "{{ ollama_config.monitoring_namespace }}"
            labels:
              prometheus-rule: "true"
              app.kubernetes.io/name: "{{ ollama_config.service_name }}"
              lions.dev/environment: "{{ ollama_config.environment }}"
          data:
            ollama-alerts.yml: |
              groups:
                - name: ollama.alerts
                  rules:
                    - alert: OllamaDown
                      expr: up{job="{{ ollama_config.service_name }}"} == 0
                      for: 5m
                      labels:
                        severity: critical
                        service: ollama
                        environment: "{{ ollama_config.environment }}"
                      annotations:
                        summary: "Ollama AI service is down"
                        description: "Ollama AI service has been down for more than 5 minutes"
              
                    - alert: OllamaHighMemoryUsage
                      expr: container_memory_usage_bytes{pod=~"{{ ollama_config.service_name }}-.*"} / container_spec_memory_limit_bytes > 0.9
                      for: 10m
                      labels:
                        severity: warning
                        service: ollama
                        environment: "{{ ollama_config.environment }}"
                      annotations:
                        summary: "Ollama high memory usage"
                        description: "Ollama pod {{ $labels.pod }} memory usage is above 90%"
              
                    - alert: OllamaModelLoadingFailed
                      expr: increase(ollama_model_load_failures_total[5m]) > 0
                      for: 1m
                      labels:
                        severity: warning
                        service: ollama
                        environment: "{{ ollama_config.environment }}"
                      annotations:
                        summary: "Ollama model loading failures"
                        description: "Ollama has failed to load models in the last 5 minutes"
      when: ollama_config.monitoring_enabled

    - name: "DÃ©ployer le tableau de bord Grafana"
      kubernetes.core.k8s:
        state: present
        definition:
          apiVersion: v1
          kind: ConfigMap
          metadata:
            name: "{{ ollama_config.service_name }}-dashboard"
            namespace: "{{ ollama_config.monitoring_namespace }}"
            labels:
              grafana_dashboard: "1"
              app.kubernetes.io/name: "{{ ollama_config.service_name }}"
              lions.dev/environment: "{{ ollama_config.environment }}"
              lions.dev/service-type: "ai-ml"
          data:
            ollama-dashboard.json: "{{ lookup('file', '../../../monitoring/dashboards/ollama/application-dashboard.json') | default('{}') }}"
      when: ollama_config.monitoring_enabled

  rescue:
    - name: "GÃ©rer l'Ã©chec de configuration du monitoring"
      debug:
        msg: "âš ï¸  Configuration du monitoring partiellement Ã©chouÃ©e"

  tags: [ollama, ai-ml, deploy, monitoring]

# =========================================================================
# GESTION INTELLIGENTE DES MODÃˆLES IA
# =========================================================================
- name: "[OLLAMA-DEPLOY] Gestion avancÃ©e des modÃ¨les IA"
  block:
    - name: "Attendre la disponibilitÃ© complÃ¨te d'Ollama"
      kubernetes.core.k8s_info:
        api_version: v1
        kind: Pod
        namespace: "{{ ollama_config.namespace }}"
        label_selectors:
          - "app.kubernetes.io/name={{ ollama_config.service_name }}"
        field_selectors:
          - "status.phase=Running"
      register: ready_pods
      until: ready_pods.resources | length > 0
      retries: 30
      delay: 20

    - name: "DÃ©finir les modÃ¨les Ã  prÃ©charger selon l'environnement"
      set_fact:
        models_to_preload: >-
          {%- if ollama_config.environment == 'production' -%}
          ["phi3", "llava", "mistral"]
          {%- elif ollama_config.environment == 'staging' -%}
          ["phi3", "llava"]
          {%- else -%}
          ["phi3"]
          {%- endif -%}

    - name: "PrÃ©charger les modÃ¨les IA de base"
      kubernetes.core.k8s_exec:
        namespace: "{{ ollama_config.namespace }}"
        pod: "{{ ready_pods.resources[0].metadata.name }}"
        command: ["/bin/sh", "-c", "timeout 1800 ollama pull {{ item }} || echo 'Failed to pull {{ item }}'"]
      loop: "{{ models_to_preload }}"
      register: model_preload_results
      when: models_to_preload | length > 0
      failed_when: false
      async: 1800
      poll: 60

    - name: "VÃ©rifier les modÃ¨les disponibles"
      kubernetes.core.k8s_exec:
        namespace: "{{ ollama_config.namespace }}"
        pod: "{{ ready_pods.resources[0].metadata.name }}"
        command: ["/bin/sh", "-c", "ollama list"]
      register: available_models_list
      failed_when: false

    - name: "CrÃ©er la ConfigMap des modÃ¨les disponibles"
      kubernetes.core.k8s:
        state: present
        definition:
          apiVersion: v1
          kind: ConfigMap
          metadata:
            name: "{{ ollama_config.service_name }}-models"
            namespace: "{{ ollama_config.namespace }}"
            labels:
              app.kubernetes.io/name: "{{ ollama_config.service_name }}"
              lions.dev/environment: "{{ ollama_config.environment }}"
          data:
            models.list: "{{ available_models_list.stdout | default('No models available') }}"
            preload.status: "{{ model_preload_results.results | map(attribute='rc') | list | join(',') }}"
            last.updated: "{{ ansible_date_time.iso8601 }}"

  rescue:
    - name: "GÃ©rer l'Ã©chec de gestion des modÃ¨les"
      debug:
        msg: "âš ï¸  Gestion des modÃ¨les partiellement Ã©chouÃ©e. Ollama est fonctionnel mais certains modÃ¨les peuvent manquer"

  when: ollama_config.storage_enabled
  tags: [ollama, ai-ml, deploy, models]

# =========================================================================
# FINALISATION ET REPORTING
# =========================================================================
- name: "[OLLAMA-DEPLOY] Finaliser le dÃ©ploiement et gÃ©nÃ©rer le rapport"
  block:
    - name: "Enregistrer le statut de dÃ©ploiement dans Vault"
      uri:
        url: "{{ ollama_config.vault_addr }}/v1/secret/data/lions/{{ ollama_config.environment }}/ollama/deployment"
        method: POST
        headers:
          X-Vault-Token: "{{ vault_token | default('') }}"
          Content-Type: "application/json"
        body_format: json
        body:
          data:
            status: "deployed"
            deployment_id: "{{ deployment_metadata.id }}"
            version: "{{ ollama_config.version }}"
            namespace: "{{ ollama_config.namespace }}"
            environment: "{{ ollama_config.environment }}"
            deployed_at: "{{ ansible_date_time.iso8601 }}"
            deployed_by: "{{ deployment_metadata.operator }}"
            internal_url: "http://{{ ollama_config.service_name }}.{{ ollama_config.namespace }}.svc.cluster.local:{{ ollama_config.port }}"
            external_url: "{{ 'https' if ollama_config.tls_enabled else 'http' }}://{{ ollama_config.full_domain }}"
            gpu_enabled: "{{ ollama_config.gpu_enabled }}"
            storage_enabled: "{{ ollama_config.storage_enabled }}"
            monitoring_enabled: "{{ ollama_config.monitoring_enabled }}"
            models_available: "{{ available_models_list.stdout | default('') }}"
        status_code: [200, 204]
        timeout: 30
      when:
        - ollama_config.vault_enabled
        - vault_health.status | default(0) == 200

    - name: "GÃ©nÃ©rer le rapport de dÃ©ploiement complet"
      debug:
        msg: |
          
          âœ… DÃ‰PLOIEMENT OLLAMA AI SERVICE LIONS 5.0 TERMINÃ‰ AVEC SUCCÃˆS !
          
          â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
          ğŸ“Š RÃ‰SUMÃ‰ DU DÃ‰PLOIEMENT
          â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
          
          ğŸ·ï¸  Identification:
              â€¢ ID DÃ©ploiement: {{ deployment_metadata.id }}
              â€¢ Service: {{ ollama_config.service_name }}
              â€¢ Version: {{ ollama_config.version }}
              â€¢ Namespace: {{ ollama_config.namespace }}
              â€¢ Environnement: {{ ollama_config.environment }}
              â€¢ OpÃ©rateur: {{ deployment_metadata.operator }}
              â€¢ DÃ©ployÃ© le: {{ ansible_date_time.iso8601 }}
          
          ğŸŒ Configuration RÃ©seau:
              â€¢ Domaine: {{ ollama_config.full_domain }}
              â€¢ Port: {{ ollama_config.port }}
              â€¢ TLS: {{ 'ActivÃ©' if ollama_config.tls_enabled else 'DÃ©sactivÃ©' }}
              â€¢ URL Interne: http://{{ ollama_config.service_name }}.{{ ollama_config.namespace }}.svc.cluster.local:{{ ollama_config.port }}
              â€¢ URL Externe: {{ 'https' if ollama_config.tls_enabled else 'http' }}://{{ ollama_config.full_domain }}
          
          ğŸ”§ Configuration Technique:
              â€¢ GPU: {{ 'ActivÃ©' if ollama_config.gpu_enabled else 'DÃ©sactivÃ©' }}
              â€¢ Stockage: {{ ollama_config.storage_size if ollama_config.storage_enabled else 'DÃ©sactivÃ©' }}
              â€¢ Monitoring: {{ 'ActivÃ©' if ollama_config.monitoring_enabled else 'DÃ©sactivÃ©' }}
              â€¢ Vault: {{ 'ActivÃ©' if ollama_config.vault_enabled else 'DÃ©sactivÃ©' }}
              â€¢ Politiques RÃ©seau: {{ 'ActivÃ©es' if ollama_config.network_policies else 'DÃ©sactivÃ©es' }}
          
          ğŸ§  Ressources AllouÃ©es:
              â€¢ CPU Request: {{ ollama_resources.requests.cpu }}
              â€¢ CPU Limit: {{ ollama_resources.limits.cpu }}
              â€¢ Memory Request: {{ ollama_resources.requests.memory }}
              â€¢ Memory Limit: {{ ollama_resources.limits.memory }}
          
          ğŸ¤– ModÃ¨les IA Disponibles:
          {{ available_models_list.stdout | default('Aucun modÃ¨le prÃ©chargÃ©') | indent(6, True) }}
          
          â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
          ğŸ”— POINTS D'ACCÃˆS API
          â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
          
          ğŸ“ API Documentation: {{ 'https' if ollama_config.tls_enabled else 'http' }}://{{ ollama_config.full_domain }}/api
          ğŸ·ï¸  Liste des modÃ¨les: {{ 'https' if ollama_config.tls_enabled else 'http' }}://{{ ollama_config.full_domain }}/api/tags
          ğŸ’¬ GÃ©nÃ©ration de texte: {{ 'https' if ollama_config.tls_enabled else 'http' }}://{{ ollama_config.full_domain }}/api/generate
          ğŸ—¨ï¸  Chat IA: {{ 'https' if ollama_config.tls_enabled else 'http' }}://{{ ollama_config.full_domain }}/api/chat
          ğŸ“Š Statut des processus: {{ 'https' if ollama_config.tls_enabled else 'http' }}://{{ ollama_config.full_domain }}/api/ps
          
          â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
          ğŸ”§ COMMANDES D'ADMINISTRATION
          â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
          
          ğŸ“‹ VÃ©rifier les pods:
              kubectl get pods -n {{ ollama_config.namespace }} -l app.kubernetes.io/name={{ ollama_config.service_name }}
          
          ğŸ“œ Consulter les logs:
              kubectl logs -n {{ ollama_config.namespace }} -l app.kubernetes.io/name={{ ollama_config.service_name }} -f
          
          ğŸ” Port-forward pour tests locaux:
              kubectl port-forward -n {{ ollama_config.namespace }} svc/{{ ollama_config.service_name }} {{ ollama_config.port }}:{{ ollama_config.port }}
          
          ğŸ“Š MÃ©triques Prometheus:
              kubectl get servicemonitor -n {{ ollama_config.namespace }} {{ ollama_config.service_name }}
          
          â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
          ğŸ§ª TESTS DE VALIDATION
          â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
          
          ğŸŒ Test de connectivitÃ©:
              curl {{ 'https' if ollama_config.tls_enabled else 'http' }}://{{ ollama_config.full_domain }}/api/tags
          
          ğŸ’¬ Test de gÃ©nÃ©ration simple:
              curl -X POST {{ 'https' if ollama_config.tls_enabled else 'http' }}://{{ ollama_config.full_domain }}/api/generate \
                   -H "Content-Type: application/json" \
                   -d '{"model":"phi3","prompt":"Hello, comment allez-vous?","stream":false}'
          
          ğŸ” Test de santÃ©:
              curl {{ 'https' if ollama_config.tls_enabled else 'http' }}://{{ ollama_config.full_domain }}/api/ps
          
          â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
          
          ğŸ‰ Ollama AI Service est maintenant opÃ©rationnel dans l'environnement {{ ollama_config.environment | upper }} !
          ğŸ“š Documentation complÃ¨te: https://docs.lions.dev/services/ollama
          ğŸ”§ Support technique: devops@lions.dev

    - name: "DÃ©finir les variables de rÃ©sultat final"
      set_fact:
        ollama_deployment_result:
          status: "success"
          deployment_id: "{{ deployment_metadata.id }}"
          service_name: "{{ ollama_config.service_name }}"
          namespace: "{{ ollama_config.namespace }}"
          version: "{{ ollama_config.version }}"
          environment: "{{ ollama_config.environment }}"
          internal_url: "http://{{ ollama_config.service_name }}.{{ ollama_config.namespace }}.svc.cluster.local:{{ ollama_config.port }}"
          external_url: "{{ 'https' if ollama_config.tls_enabled else 'http' }}://{{ ollama_config.full_domain }}"
          gpu_enabled: "{{ ollama_config.gpu_enabled }}"
          storage_enabled: "{{ ollama_config.storage_enabled }}"
          monitoring_enabled: "{{ ollama_config.monitoring_enabled }}"
          models_available: "{{ available_models_list.stdout | default('') }}"
          deployed_at: "{{ ansible_date_time.iso8601 }}"
          deployed_by: "{{ deployment_metadata.operator }}"

  rescue:
    - name: "GÃ©rer l'Ã©chec de finalisation"
      debug:
        msg: "âš ï¸  Finalisation partiellement Ã©chouÃ©e mais dÃ©ploiement rÃ©ussi"

  tags: [ollama, ai-ml, deploy, completion]

# =========================================================================
# GESTION D'ERREURS GLOBALE ET ROLLBACK AUTOMATIQUE
# =========================================================================
- name: "[OLLAMA-DEPLOY] ProcÃ©dure d'urgence en cas d'Ã©chec critique"
  block:
    - name: "DÃ©clencher la procÃ©dure de rollback automatique"
      include_tasks: "rollback.yml"
      vars:
        rollback_reason: "critical_deployment_failure"
        deployment_id: "{{ deployment_metadata.id }}"
        preserve_data: true
      when:
        - ollama_config.environment == "production"
        - ansible_failed_task is defined

    - name: "Enregistrer l'Ã©chec dans Vault"
      uri:
        url: "{{ ollama_config.vault_addr }}/v1/secret/data/lions/{{ ollama_config.environment }}/ollama/deployment"
        method: POST
        headers:
          X-Vault-Token: "{{ vault_token | default('') }}"
          Content-Type: "application/json"
        body_format: json
        body:
          data:
            status: "failed"
            deployment_id: "{{ deployment_metadata.id }}"
            failed_at: "{{ ansible_date_time.iso8601 }}"
            error_details: "{{ ansible_failed_result.msg | default('Erreur critique inconnue') }}"
            rollback_initiated: true
        status_code: [200, 204]
        timeout: 30
      when:
        - ollama_config.vault_enabled
        - ansible_failed_task is defined

    - name: "DÃ©finir le rÃ©sultat d'Ã©chec"
      set_fact:
        ollama_deployment_result:
          status: "failed"
          deployment_id: "{{ deployment_metadata.id }}"
          failed_at: "{{ ansible_date_time.iso8601 }}"
          error_message: "{{ ansible_failed_result.msg | default('Erreur critique de dÃ©ploiement') }}"
          rollback_initiated: true

  when: ansible_failed_task is defined
  tags: [ollama, ai-ml, deploy, error-handling, rollback]