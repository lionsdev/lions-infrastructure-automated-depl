---
# =========================================================================
# LIONS INFRASTRUCTURE 5.0 - D√âPLOIEMENT OLLAMA AI SERVICE
# =========================================================================
# Description: T√¢ches avanc√©es de d√©ploiement du service IA Ollama avec int√©gration compl√®te
# Composant: Ollama LLM Service - Orchestration Kubernetes Enterprise
# Version: 5.0.0
# Auteur: √âquipe DevOps LIONS
# Date: 2025-05-29
# D√©pendances: Kubernetes 1.24+, Persistent Storage, GPU (optionnel), Vault, Prometheus
# Documentation: https://docs.lions.dev/services/ollama/deployment
# =========================================================================

# =========================================================================
# CHARGEMENT DE LA CONFIGURATION ENVIRONNEMENT
# =========================================================================
- name: "[OLLAMA-DEPLOY] Charger la configuration d'environnement"
  block:
    - name: "D√©finir les variables d'environnement Ollama depuis LIONS_ENV"
      set_fact:
        ollama_config:
          # Configuration principale
          service_name: "{{ lookup('env', 'LIONS_OLLAMA_SERVICE_NAME') | default('ollama') }}"
          namespace: "{{ lookup('env', 'LIONS_OLLAMA_NAMESPACE') | default('ai') }}"
          version: "{{ lookup('env', 'LIONS_OLLAMA_VERSION') | default('0.1.26') }}"
          enabled: "{{ lookup('env', 'LIONS_OLLAMA_ENABLED') | default('true') | bool }}"
          port: "{{ lookup('env', 'LIONS_OLLAMA_PORT') | default('11434') | int }}"

          # Configuration r√©seau et DNS
          domain: "{{ lookup('env', 'LIONS_DNS_FULL_DOMAIN') | default('lions.local') }}"
          subdomain: "ollama"
          full_domain: "ollama.{{ lookup('env', 'LIONS_DNS_FULL_DOMAIN') | default('lions.local') }}"

          # Configuration stockage
          storage_enabled: true
          storage_size: "{{ lookup('env', 'LIONS_OLLAMA_STORAGE_SIZE') | default('100Gi') }}"
          storage_class: "{{ lookup('env', 'LIONS_STORAGE_CLASS_DEFAULT') | default('local-path') }}"

          # Configuration GPU
          gpu_enabled: "{{ lookup('env', 'LIONS_OLLAMA_GPU_ENABLED') | default('false') | bool }}"

          # Configuration s√©curit√©
          tls_enabled: "{{ lookup('env', 'LIONS_SECURITY_TLS_ENABLED') | default('true') | bool }}"
          network_policies: "{{ lookup('env', 'LIONS_SECURITY_NETWORK_POLICIES') | default('true') | bool }}"

          # Configuration monitoring
          monitoring_enabled: "{{ lookup('env', 'LIONS_MONITORING_ENABLED') | default('true') | bool }}"
          monitoring_namespace: "{{ lookup('env', 'LIONS_MONITORING_NAMESPACE') | default('monitoring') }}"

          # Configuration Vault
          vault_enabled: "{{ lookup('env', 'LIONS_VAULT_ENABLED') | default('true') | bool }}"
          vault_addr: "{{ lookup('env', 'LIONS_VAULT_ADDR') | default('') }}"
          vault_namespace: "{{ lookup('env', 'LIONS_VAULT_NAMESPACE') | default('vault-system') }}"

          # Configuration environnement
          environment: "{{ lookup('env', 'LIONS_ENVIRONMENT') | default('development') }}"
          debug_mode: "{{ lookup('env', 'LIONS_DEBUG_MODE') | default('false') | bool }}"
          dry_run: "{{ lookup('env', 'LIONS_DRY_RUN') | default('false') | bool }}"

    - name: "D√©finir les ressources par environnement"
      set_fact:
        ollama_resources: >-
          {%- if ollama_config.environment == 'production' -%}
          {
            "requests": {
              "cpu": "{{ lookup('env', 'LIONS_RESOURCES_LARGE_CPU_REQUEST') | default('500m') }}",
              "memory": "{{ lookup('env', 'LIONS_RESOURCES_LARGE_MEMORY_REQUEST') | default('1Gi') }}"
            },
            "limits": {
              "cpu": "{{ lookup('env', 'LIONS_RESOURCES_LARGE_CPU_LIMIT') | default('2000m') }}",
              "memory": "{{ lookup('env', 'LIONS_RESOURCES_LARGE_MEMORY_LIMIT') | default('4Gi') }}"
            }
          }
          {%- elif ollama_config.environment == 'staging' -%}
          {
            "requests": {
              "cpu": "{{ lookup('env', 'LIONS_RESOURCES_MEDIUM_CPU_REQUEST') | default('200m') }}",
              "memory": "{{ lookup('env', 'LIONS_RESOURCES_MEDIUM_MEMORY_REQUEST') | default('512Mi') }}"
            },
            "limits": {
              "cpu": "{{ lookup('env', 'LIONS_RESOURCES_MEDIUM_CPU_LIMIT') | default('1000m') }}",
              "memory": "{{ lookup('env', 'LIONS_RESOURCES_MEDIUM_MEMORY_LIMIT') | default('2Gi') }}"
            }
          }
          {%- else -%}
          {
            "requests": {
              "cpu": "{{ lookup('env', 'LIONS_RESOURCES_SMALL_CPU_REQUEST') | default('100m') }}",
              "memory": "{{ lookup('env', 'LIONS_RESOURCES_SMALL_MEMORY_REQUEST') | default('128Mi') }}"
            },
            "limits": {
              "cpu": "{{ lookup('env', 'LIONS_RESOURCES_SMALL_CPU_LIMIT') | default('500m') }}",
              "memory": "{{ lookup('env', 'LIONS_RESOURCES_SMALL_MEMORY_LIMIT') | default('512Mi') }}"
            }
          }
          {%- endif -%}

    - name: "G√©n√©rer l'identifiant de d√©ploiement unique"
      set_fact:
        deployment_metadata:
          id: "{{ ansible_date_time.iso8601_basic_short }}-{{ ansible_hostname | hash('md5') | truncate(8, True, '') }}"
          timestamp: "{{ ansible_date_time.epoch }}"
          version: "{{ ollama_config.version }}"
          environment: "{{ ollama_config.environment }}"
          operator: "{{ ansible_user_id | default('system') }}"

    - name: "Afficher la configuration de d√©ploiement"
      debug:
        msg:
          - "üöÄ Configuration Ollama AI Service LIONS 5.0:"
          - "  üìù ID D√©ploiement: {{ deployment_metadata.id }}"
          - "  üè∑Ô∏è  Service: {{ ollama_config.service_name }}"
          - "  üì¶ Namespace: {{ ollama_config.namespace }}"
          - "  üîñ Version: {{ ollama_config.version }}"
          - "  üåç Environnement: {{ ollama_config.environment }}"
          - "  üåê Domaine: {{ ollama_config.full_domain }}"
          - "  üñ•Ô∏è  GPU: {{ 'Activ√©' if ollama_config.gpu_enabled else 'D√©sactiv√©' }}"
          - "  üíæ Stockage: {{ ollama_config.storage_size if ollama_config.storage_enabled else 'D√©sactiv√©' }}"
          - "  üîí TLS: {{ 'Activ√©' if ollama_config.tls_enabled else 'D√©sactiv√©' }}"
          - "  üìä Monitoring: {{ 'Activ√©' if ollama_config.monitoring_enabled else 'D√©sactiv√©' }}"
      when: ollama_config.debug_mode

  tags: [ollama, ai-ml, deploy, config]

# =========================================================================
# VALIDATION PR√â-D√âPLOIEMENT AVANC√âE
# =========================================================================
- name: "[OLLAMA-DEPLOY] Validation des pr√©requis syst√®me"
  block:
    - name: "V√©rifier si Ollama est activ√©"
      meta: end_play
      when: not ollama_config.enabled

    - name: "Valider la configuration Kubernetes"
      kubernetes.core.k8s_info:
        api_version: v1
        kind: Node
      register: k8s_nodes
      failed_when: k8s_nodes.resources | length == 0

    - name: "V√©rifier l'existence du namespace cible"
      kubernetes.core.k8s:
        state: present
        definition:
          apiVersion: v1
          kind: Namespace
          metadata:
            name: "{{ ollama_config.namespace }}"
            labels:
              lions.dev/environment: "{{ ollama_config.environment }}"
              lions.dev/service-type: "ai-ml"
              lions.dev/managed-by: "ansible"
        wait: true
        wait_timeout: 60

    - name: "Valider la disponibilit√© des ressources GPU"
      kubernetes.core.k8s_info:
        api_version: v1
        kind: Node
        label_selectors:
          - "nvidia.com/gpu"
      register: gpu_nodes
      when: ollama_config.gpu_enabled
      failed_when:
        - ollama_config.gpu_enabled
        - gpu_nodes.resources | length == 0

    - name: "V√©rifier la connectivit√© Vault"
      uri:
        url: "{{ ollama_config.vault_addr }}/v1/sys/health"
        method: GET
        status_code: [200, 429, 472, 473, 501, 503]
        timeout: 10
      register: vault_health
      when: ollama_config.vault_enabled
      failed_when: false

    - name: "Valider les classes de stockage disponibles"
      kubernetes.core.k8s_info:
        api_version: storage.k8s.io/v1
        kind: StorageClass
        name: "{{ ollama_config.storage_class }}"
      register: storage_class_check
      when: ollama_config.storage_enabled
      failed_when:
        - ollama_config.storage_enabled
        - storage_class_check.resources | length == 0

    - name: "V√©rifier les quotas de ressources du namespace"
      kubernetes.core.k8s_info:
        api_version: v1
        kind: ResourceQuota
        namespace: "{{ ollama_config.namespace }}"
      register: resource_quotas

    - name: "Validation de la configuration r√©seau"
      kubernetes.core.k8s_info:
        api_version: networking.k8s.io/v1
        kind: NetworkPolicy
        namespace: "{{ ollama_config.namespace }}"
      register: network_policies
      when: ollama_config.network_policies

  rescue:
    - name: "G√©rer l'√©chec de validation des pr√©requis"
      debug:
        msg: |
          ‚ùå √âchec de validation des pr√©requis Ollama AI:
          - Kubernetes: {{ 'OK' if k8s_nodes.resources | default([]) | length > 0 else 'ERREUR' }}
          - GPU: {{ 'OK' if not ollama_config.gpu_enabled or (gpu_nodes.resources | default([]) | length > 0) else 'ERREUR' }}
          - Vault: {{ 'OK' if not ollama_config.vault_enabled or vault_health.status == 200 else 'AVERTISSEMENT' }}
          - Stockage: {{ 'OK' if not ollama_config.storage_enabled or (storage_class_check.resources | default([]) | length > 0) else 'ERREUR' }}

    - fail:
        msg: "Les pr√©requis syst√®me pour Ollama AI ne sont pas satisfaits"

  tags: [ollama, ai-ml, deploy, validation]

# =========================================================================
# GESTION AVANC√âE DES SECRETS ET CONFIGURATION
# =========================================================================
- name: "[OLLAMA-DEPLOY] Configuration des secrets et donn√©es sensibles"
  block:
    - name: "G√©n√©rer les identifiants d'API Ollama"
      set_fact:
        ollama_credentials:
          api_key: "{{ lookup('password', '/dev/null length=64 chars=ascii_letters,digits') }}"
          admin_token: "{{ lookup('password', '/dev/null length=32 chars=ascii_letters,digits') }}"
          deployment_secret: "{{ lookup('password', '/dev/null length=16 chars=ascii_letters,digits') }}"
      no_log: true

    - name: "Stocker les secrets dans Vault"
      uri:
        url: "{{ ollama_config.vault_addr }}/v1/secret/data/lions/{{ ollama_config.environment }}/ollama/credentials"
        method: POST
        headers:
          X-Vault-Token: "{{ vault_token | default('') }}"
          Content-Type: "application/json"
        body_format: json
        body:
          data:
            api_key: "{{ ollama_credentials.api_key }}"
            admin_token: "{{ ollama_credentials.admin_token }}"
            deployment_id: "{{ deployment_metadata.id }}"
            created_at: "{{ ansible_date_time.iso8601 }}"
            environment: "{{ ollama_config.environment }}"
        status_code: [200, 204]
        timeout: 30
      register: vault_secret_result
      when:
        - ollama_config.vault_enabled
        - vault_health.status | default(0) == 200
      no_log: true

    - name: "Cr√©er le secret Kubernetes pour Ollama"
      kubernetes.core.k8s:
        state: present
        definition:
          apiVersion: v1
          kind: Secret
          metadata:
            name: "{{ ollama_config.service_name }}-secrets"
            namespace: "{{ ollama_config.namespace }}"
            labels:
              app.kubernetes.io/name: "{{ ollama_config.service_name }}"
              app.kubernetes.io/instance: "{{ ollama_config.service_name }}-{{ ollama_config.environment }}"
              app.kubernetes.io/component: "ai-ml"
              app.kubernetes.io/version: "{{ ollama_config.version }}"
              lions.dev/environment: "{{ ollama_config.environment }}"
              lions.dev/deployment-id: "{{ deployment_metadata.id }}"
            annotations:
              lions.dev/created-by: "ansible"
              lions.dev/last-updated: "{{ ansible_date_time.iso8601 }}"
          type: Opaque
          data:
            api-key: "{{ ollama_credentials.api_key | b64encode }}"
            admin-token: "{{ ollama_credentials.admin_token | b64encode }}"
            deployment-secret: "{{ ollama_credentials.deployment_secret | b64encode }}"
        wait: true
        wait_timeout: 60
      no_log: true

  rescue:
    - name: "Utiliser la configuration de secours pour les secrets"
      debug:
        msg: "‚ö†Ô∏è  Utilisation de la configuration de secours pour les secrets Ollama"

    - set_fact:
        ollama_credentials:
          api_key: "fallback-api-key-{{ deployment_metadata.id | truncate(8, True, '') }}"
          admin_token: "fallback-admin-{{ deployment_metadata.id | truncate(8, True, '') }}"
          deployment_secret: "fallback-secret"

  tags: [ollama, ai-ml, deploy, secrets]

# =========================================================================
# D√âPLOIEMENT DES RESSOURCES KUBERNETES CORE
# =========================================================================
- name: "[OLLAMA-DEPLOY] D√©ployer les ressources Kubernetes fondamentales"
  block:
    - name: "D√©ployer le ServiceAccount avec RBAC"
      kubernetes.core.k8s:
        state: present
        definition: "{{ lookup('template', 'serviceaccount.yml.j2') | from_yaml }}"
        wait: true
        wait_timeout: 60

    - name: "Cr√©er la ConfigMap de configuration Ollama"
      kubernetes.core.k8s:
        state: present
        definition: "{{ lookup('template', 'configmap.yml.j2') | from_yaml }}"
        wait: true
        wait_timeout: 60

    - name: "Provisionner le stockage persistant"
      kubernetes.core.k8s:
        state: present
        definition: "{{ lookup('template', 'persistentvolumeclaim.yml.j2') | from_yaml }}"
        wait: true
        wait_condition:
          type: Bound
          status: "True"
        wait_timeout: 300
      when: ollama_config.storage_enabled

    - name: "D√©ployer le Service r√©seau"
      kubernetes.core.k8s:
        state: present
        definition: "{{ lookup('template', 'service.yml.j2') | from_yaml }}"
        wait: true
        wait_timeout: 60

    - name: "D√©ployer les politiques r√©seau"
      kubernetes.core.k8s:
        state: present
        definition:
          apiVersion: networking.k8s.io/v1
          kind: NetworkPolicy
          metadata:
            name: "{{ ollama_config.service_name }}-network-policy"
            namespace: "{{ ollama_config.namespace }}"
            labels:
              app.kubernetes.io/name: "{{ ollama_config.service_name }}"
              lions.dev/environment: "{{ ollama_config.environment }}"
          spec:
            podSelector:
              matchLabels:
                app.kubernetes.io/name: "{{ ollama_config.service_name }}"
            policyTypes:
              - Ingress
              - Egress
            ingress:
              - from:
                  - namespaceSelector:
                      matchLabels:
                        name: "{{ ollama_config.monitoring_namespace }}"
                  - podSelector:
                      matchLabels:
                        app.kubernetes.io/name: traefik
                ports:
                  - protocol: TCP
                    port: "{{ ollama_config.port }}"
            egress:
              - to: []
                ports:
                  - protocol: TCP
                    port: 443
                  - protocol: TCP
                    port: 80
                  - protocol: UDP
                    port: 53
        wait: true
        wait_timeout: 60
      when: ollama_config.network_policies

  rescue:
    - name: "G√©rer l'√©chec de d√©ploiement des ressources fondamentales"
      debug:
        msg: "‚ùå √âchec du d√©ploiement des ressources Kubernetes fondamentales"
    - fail:
        msg: "Le d√©ploiement des ressources fondamentales Ollama a √©chou√©"

  tags: [ollama, ai-ml, deploy, kubernetes, core]

# =========================================================================
# D√âPLOIEMENT DE L'APPLICATION OLLAMA
# =========================================================================
- name: "[OLLAMA-DEPLOY] D√©ployer l'application Ollama AI"
  block:
    - name: "D√©ployer l'application Ollama avec strat√©gie de d√©ploiement intelligente"
      kubernetes.core.k8s:
        state: present
        definition: "{{ lookup('template', 'deployment.yml.j2') | from_yaml }}"
        wait: true
        wait_condition:
          type: Available
          status: "True"
        wait_timeout: 1200  # Timeout √©tendu pour les mod√®les IA
      register: deployment_result

    - name: "V√©rifier le statut de d√©ploiement d√©taill√©"
      kubernetes.core.k8s_info:
        api_version: apps/v1
        kind: Deployment
        namespace: "{{ ollama_config.namespace }}"
        name: "{{ ollama_config.service_name }}"
      register: deployment_status
      until:
        - deployment_status.resources | length > 0
        - deployment_status.resources[0].status.readyReplicas | default(0) > 0
        - deployment_status.resources[0].status.readyReplicas == deployment_status.resources[0].spec.replicas
      retries: 80
      delay: 15

    - name: "Obtenir les informations des pods d√©ploy√©s"
      kubernetes.core.k8s_info:
        api_version: v1
        kind: Pod
        namespace: "{{ ollama_config.namespace }}"
        label_selectors:
          - "app.kubernetes.io/name={{ ollama_config.service_name }}"
          - "app.kubernetes.io/instance={{ ollama_config.service_name }}-{{ ollama_config.environment }}"
      register: ollama_pods

    - name: "Valider la sant√© des pods"
      assert:
        that:
          - ollama_pods.resources | length > 0
          - ollama_pods.resources | selectattr('status.phase', 'equalto', 'Running') | list | length == ollama_pods.resources | length
          - ollama_pods.resources | selectattr('status.containerStatuses.0.ready', 'equalto', true) | list | length == ollama_pods.resources | length
        fail_msg: "‚ùå Les pods Ollama ne sont pas tous en √©tat de fonctionnement optimal"
        success_msg: "‚úÖ Tous les pods Ollama sont d√©ploy√©s et op√©rationnels"

  rescue:
    - name: "Diagnostiquer l'√©chec de d√©ploiement de l'application"
      kubernetes.core.k8s_info:
        api_version: v1
        kind: Event
        namespace: "{{ ollama_config.namespace }}"
        field_selectors:
          - "involvedObject.name={{ ollama_config.service_name }}"
      register: deployment_events

    - name: "Collecter les logs des pods en √©chec"
      kubernetes.core.k8s_log:
        namespace: "{{ ollama_config.namespace }}"
        name: "{{ item.metadata.name }}"
        tail_lines: 200
      register: failed_pod_logs
      loop: "{{ ollama_pods.resources | default([]) }}"
      when:
        - ollama_pods.resources is defined
        - item.status.phase != "Running"

    - name: "Afficher les informations de diagnostic"
      debug:
        msg: |
          üîç Diagnostic d'√©chec de d√©ploiement Ollama:
          {% if deployment_events.resources %}
          üìã √âv√©nements r√©cents:
          {% for event in deployment_events.resources[:5] %}
          - {{ event.firstTimestamp }}: {{ event.reason }} - {{ event.message }}
          {% endfor %}
          {% endif %}
          
          {% if failed_pod_logs.results %}
          üìú Logs des pods en √©chec:
          {% for log in failed_pod_logs.results %}
          Pod {{ log.item.metadata.name }}:
          {{ log.log | default('Aucun log disponible') | truncate(500) }}
          {% endfor %}
          {% endif %}

    - fail:
        msg: "Le d√©ploiement de l'application Ollama AI a √©chou√©"

  tags: [ollama, ai-ml, deploy, application]

# =========================================================================
# CONFIGURATION R√âSEAU ET INGRESS
# =========================================================================
- name: "[OLLAMA-DEPLOY] Configurer l'acc√®s r√©seau et l'exposition"
  block:
    - name: "D√©ployer la ressource Ingress avec TLS"
      kubernetes.core.k8s:
        state: present
        definition: "{{ lookup('template', 'ingress.yml.j2') | from_yaml }}"
        wait: true
        wait_timeout: 120
      when: ollama_config.tls_enabled

    - name: "Attendre le provisionnement du certificat TLS"
      kubernetes.core.k8s_info:
        api_version: cert-manager.io/v1
        kind: Certificate
        namespace: "{{ ollama_config.namespace }}"
        name: "{{ ollama_config.service_name }}-tls"
      register: tls_certificate
      until:
        - tls_certificate.resources | length > 0
        - tls_certificate.resources[0].status.conditions | selectattr('type', 'equalto', 'Ready') | selectattr('status', 'equalto', 'True') | list | length > 0
      retries: 30
      delay: 10
      when: ollama_config.tls_enabled

    - name: "V√©rifier la r√©solution DNS"
      uri:
        url: "{{ 'https' if ollama_config.tls_enabled else 'http' }}://{{ ollama_config.full_domain }}/api/tags"
        method: GET
        status_code: [200, 404, 405]
        timeout: 30
        validate_certs: "{{ not (ollama_config.environment == 'development') }}"
      register: dns_check
      retries: 10
      delay: 30
      until: dns_check is not failed

  rescue:
    - name: "G√©rer l'√©chec de configuration r√©seau"
      debug:
        msg: "‚ö†Ô∏è  Configuration r√©seau partiellement √©chou√©e. Ollama sera accessible uniquement en interne"

  tags: [ollama, ai-ml, deploy, network, ingress]

# =========================================================================
# V√âRIFICATIONS DE SANT√â AVANC√âES
# =========================================================================
- name: "[OLLAMA-DEPLOY] Effectuer les v√©rifications de sant√© compl√®tes"
  block:
    - name: "Test de connectivit√© API interne"
      uri:
        url: "http://{{ ollama_config.service_name }}.{{ ollama_config.namespace }}.svc.cluster.local:{{ ollama_config.port }}/api/tags"
        method: GET
        status_code: 200
        timeout: 30
      register: internal_health_check
      retries: 20
      delay: 15
      until: internal_health_check is succeeded

    - name: "Test de connectivit√© API externe"
      uri:
        url: "{{ 'https' if ollama_config.tls_enabled else 'http' }}://{{ ollama_config.full_domain }}/api/tags"
        method: GET
        status_code: 200
        timeout: 30
        validate_certs: "{{ not (ollama_config.environment == 'development') }}"
      register: external_health_check
      retries: 15
      delay: 20
      until: external_health_check is succeeded
      when: ollama_config.tls_enabled

    - name: "V√©rifier la disponibilit√© du GPU"
      uri:
        url: "http://{{ ollama_config.service_name }}.{{ ollama_config.namespace }}.svc.cluster.local:{{ ollama_config.port }}/api/ps"
        method: GET
        status_code: 200
        timeout: 30
      register: gpu_check
      when: ollama_config.gpu_enabled

    - name: "Test de fonctionnalit√© basique de g√©n√©ration"
      uri:
        url: "http://{{ ollama_config.service_name }}.{{ ollama_config.namespace }}.svc.cluster.local:{{ ollama_config.port }}/api/generate"
        method: POST
        body_format: json
        body:
          model: "phi3"
          prompt: "Test"
          stream: false
        status_code: [200, 404]  # 404 acceptable si le mod√®le n'est pas encore charg√©
        timeout: 60
      register: generation_test
      failed_when: false

  rescue:
    - name: "G√©rer l'√©chec des v√©rifications de sant√©"
      debug:
        msg: |
          ‚ö†Ô∏è  Certaines v√©rifications de sant√© ont √©chou√©:
          - API interne: {{ 'OK' if internal_health_check is succeeded else 'ERREUR' }}
          - API externe: {{ 'OK' if external_health_check is succeeded else 'ERREUR' }}
          - GPU: {{ 'OK' if not ollama_config.gpu_enabled or gpu_check is succeeded else 'ERREUR' }}
          - G√©n√©ration: {{ 'OK' if generation_test.status == 200 else 'NON TEST√â' }}

  tags: [ollama, ai-ml, deploy, health-check]

# =========================================================================
# CONFIGURATION DU MONITORING AVANC√â
# =========================================================================
- name: "[OLLAMA-DEPLOY] Configurer le monitoring et l'observabilit√©"
  block:
    - name: "D√©ployer le ServiceMonitor pour Prometheus"
      kubernetes.core.k8s:
        state: present
        definition: "{{ lookup('template', 'servicemonitor.yml.j2') | from_yaml }}"
        wait: true
        wait_timeout: 60
      when: ollama_config.monitoring_enabled

    - name: "Cr√©er les alertes Prometheus personnalis√©es"
      kubernetes.core.k8s:
        state: present
        definition:
          apiVersion: v1
          kind: ConfigMap
          metadata:
            name: "{{ ollama_config.service_name }}-alerts"
            namespace: "{{ ollama_config.monitoring_namespace }}"
            labels:
              prometheus-rule: "true"
              app.kubernetes.io/name: "{{ ollama_config.service_name }}"
              lions.dev/environment: "{{ ollama_config.environment }}"
          data:
            ollama-alerts.yml: |
              groups:
                - name: ollama.alerts
                  rules:
                    - alert: OllamaDown
                      expr: up{job="{{ ollama_config.service_name }}"} == 0
                      for: 5m
                      labels:
                        severity: critical
                        service: ollama
                        environment: "{{ ollama_config.environment }}"
                      annotations:
                        summary: "Ollama AI service is down"
                        description: "Ollama AI service has been down for more than 5 minutes"
              
                    - alert: OllamaHighMemoryUsage
                      expr: container_memory_usage_bytes{pod=~"{{ ollama_config.service_name }}-.*"} / container_spec_memory_limit_bytes > 0.9
                      for: 10m
                      labels:
                        severity: warning
                        service: ollama
                        environment: "{{ ollama_config.environment }}"
                      annotations:
                        summary: "Ollama high memory usage"
                        description: "Ollama pod {{ $labels.pod }} memory usage is above 90%"
              
                    - alert: OllamaModelLoadingFailed
                      expr: increase(ollama_model_load_failures_total[5m]) > 0
                      for: 1m
                      labels:
                        severity: warning
                        service: ollama
                        environment: "{{ ollama_config.environment }}"
                      annotations:
                        summary: "Ollama model loading failures"
                        description: "Ollama has failed to load models in the last 5 minutes"
      when: ollama_config.monitoring_enabled

    - name: "D√©ployer le tableau de bord Grafana"
      kubernetes.core.k8s:
        state: present
        definition:
          apiVersion: v1
          kind: ConfigMap
          metadata:
            name: "{{ ollama_config.service_name }}-dashboard"
            namespace: "{{ ollama_config.monitoring_namespace }}"
            labels:
              grafana_dashboard: "1"
              app.kubernetes.io/name: "{{ ollama_config.service_name }}"
              lions.dev/environment: "{{ ollama_config.environment }}"
              lions.dev/service-type: "ai-ml"
          data:
            ollama-dashboard.json: "{{ lookup('file', '../../../monitoring/dashboards/ollama/application-dashboard.json') | default('{}') }}"
      when: ollama_config.monitoring_enabled

  rescue:
    - name: "G√©rer l'√©chec de configuration du monitoring"
      debug:
        msg: "‚ö†Ô∏è  Configuration du monitoring partiellement √©chou√©e"

  tags: [ollama, ai-ml, deploy, monitoring]

# =========================================================================
# GESTION INTELLIGENTE DES MOD√àLES IA
# =========================================================================
- name: "[OLLAMA-DEPLOY] Gestion avanc√©e des mod√®les IA"
  block:
    - name: "Attendre la disponibilit√© compl√®te d'Ollama"
      kubernetes.core.k8s_info:
        api_version: v1
        kind: Pod
        namespace: "{{ ollama_config.namespace }}"
        label_selectors:
          - "app.kubernetes.io/name={{ ollama_config.service_name }}"
        field_selectors:
          - "status.phase=Running"
      register: ready_pods
      until: ready_pods.resources | length > 0
      retries: 30
      delay: 20

    - name: "D√©finir les mod√®les √† pr√©charger selon l'environnement"
      set_fact:
        models_to_preload: >-
          {%- if ollama_config.environment == 'production' -%}
          ["phi3", "llava", "mistral"]
          {%- elif ollama_config.environment == 'staging' -%}
          ["phi3", "llava"]
          {%- else -%}
          ["phi3"]
          {%- endif -%}

    - name: "Pr√©charger les mod√®les IA de base"
      kubernetes.core.k8s_exec:
        namespace: "{{ ollama_config.namespace }}"
        pod: "{{ ready_pods.resources[0].metadata.name }}"
        command: ["/bin/sh", "-c", "timeout 1800 ollama pull {{ item }} || echo 'Failed to pull {{ item }}'"]
      loop: "{{ models_to_preload }}"
      register: model_preload_results
      when: models_to_preload | length > 0
      failed_when: false
      async: 1800
      poll: 60

    - name: "V√©rifier les mod√®les disponibles"
      kubernetes.core.k8s_exec:
        namespace: "{{ ollama_config.namespace }}"
        pod: "{{ ready_pods.resources[0].metadata.name }}"
        command: ["/bin/sh", "-c", "ollama list"]
      register: available_models_list
      failed_when: false

    - name: "Cr√©er la ConfigMap des mod√®les disponibles"
      kubernetes.core.k8s:
        state: present
        definition:
          apiVersion: v1
          kind: ConfigMap
          metadata:
            name: "{{ ollama_config.service_name }}-models"
            namespace: "{{ ollama_config.namespace }}"
            labels:
              app.kubernetes.io/name: "{{ ollama_config.service_name }}"
              lions.dev/environment: "{{ ollama_config.environment }}"
          data:
            models.list: "{{ available_models_list.stdout | default('No models available') }}"
            preload.status: "{{ model_preload_results.results | map(attribute='rc') | list | join(',') }}"
            last.updated: "{{ ansible_date_time.iso8601 }}"

  rescue:
    - name: "G√©rer l'√©chec de gestion des mod√®les"
      debug:
        msg: "‚ö†Ô∏è  Gestion des mod√®les partiellement √©chou√©e. Ollama est fonctionnel mais certains mod√®les peuvent manquer"

  when: ollama_config.storage_enabled
  tags: [ollama, ai-ml, deploy, models]

# =========================================================================
# FINALISATION ET REPORTING
# =========================================================================
- name: "[OLLAMA-DEPLOY] Finaliser le d√©ploiement et g√©n√©rer le rapport"
  block:
    - name: "Enregistrer le statut de d√©ploiement dans Vault"
      uri:
        url: "{{ ollama_config.vault_addr }}/v1/secret/data/lions/{{ ollama_config.environment }}/ollama/deployment"
        method: POST
        headers:
          X-Vault-Token: "{{ vault_token | default('') }}"
          Content-Type: "application/json"
        body_format: json
        body:
          data:
            status: "deployed"
            deployment_id: "{{ deployment_metadata.id }}"
            version: "{{ ollama_config.version }}"
            namespace: "{{ ollama_config.namespace }}"
            environment: "{{ ollama_config.environment }}"
            deployed_at: "{{ ansible_date_time.iso8601 }}"
            deployed_by: "{{ deployment_metadata.operator }}"
            internal_url: "http://{{ ollama_config.service_name }}.{{ ollama_config.namespace }}.svc.cluster.local:{{ ollama_config.port }}"
            external_url: "{{ 'https' if ollama_config.tls_enabled else 'http' }}://{{ ollama_config.full_domain }}"
            gpu_enabled: "{{ ollama_config.gpu_enabled }}"
            storage_enabled: "{{ ollama_config.storage_enabled }}"
            monitoring_enabled: "{{ ollama_config.monitoring_enabled }}"
            models_available: "{{ available_models_list.stdout | default('') }}"
        status_code: [200, 204]
        timeout: 30
      when:
        - ollama_config.vault_enabled
        - vault_health.status | default(0) == 200

    - name: "G√©n√©rer le rapport de d√©ploiement complet"
      debug:
        msg: |
          
          ‚úÖ D√âPLOIEMENT OLLAMA AI SERVICE LIONS 5.0 TERMIN√â AVEC SUCC√àS !
          
          ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
          üìä R√âSUM√â DU D√âPLOIEMENT
          ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
          
          üè∑Ô∏è  Identification:
              ‚Ä¢ ID D√©ploiement: {{ deployment_metadata.id }}
              ‚Ä¢ Service: {{ ollama_config.service_name }}
              ‚Ä¢ Version: {{ ollama_config.version }}
              ‚Ä¢ Namespace: {{ ollama_config.namespace }}
              ‚Ä¢ Environnement: {{ ollama_config.environment }}
              ‚Ä¢ Op√©rateur: {{ deployment_metadata.operator }}
              ‚Ä¢ D√©ploy√© le: {{ ansible_date_time.iso8601 }}
          
          üåê Configuration R√©seau:
              ‚Ä¢ Domaine: {{ ollama_config.full_domain }}
              ‚Ä¢ Port: {{ ollama_config.port }}
              ‚Ä¢ TLS: {{ 'Activ√©' if ollama_config.tls_enabled else 'D√©sactiv√©' }}
              ‚Ä¢ URL Interne: http://{{ ollama_config.service_name }}.{{ ollama_config.namespace }}.svc.cluster.local:{{ ollama_config.port }}
              ‚Ä¢ URL Externe: {{ 'https' if ollama_config.tls_enabled else 'http' }}://{{ ollama_config.full_domain }}
          
          üîß Configuration Technique:
              ‚Ä¢ GPU: {{ 'Activ√©' if ollama_config.gpu_enabled else 'D√©sactiv√©' }}
              ‚Ä¢ Stockage: {{ ollama_config.storage_size if ollama_config.storage_enabled else 'D√©sactiv√©' }}
              ‚Ä¢ Monitoring: {{ 'Activ√©' if ollama_config.monitoring_enabled else 'D√©sactiv√©' }}
              ‚Ä¢ Vault: {{ 'Activ√©' if ollama_config.vault_enabled else 'D√©sactiv√©' }}
              ‚Ä¢ Politiques R√©seau: {{ 'Activ√©es' if ollama_config.network_policies else 'D√©sactiv√©es' }}
          
          üß† Ressources Allou√©es:
              ‚Ä¢ CPU Request: {{ ollama_resources.requests.cpu }}
              ‚Ä¢ CPU Limit: {{ ollama_resources.limits.cpu }}
              ‚Ä¢ Memory Request: {{ ollama_resources.requests.memory }}
              ‚Ä¢ Memory Limit: {{ ollama_resources.limits.memory }}
          
          ü§ñ Mod√®les IA Disponibles:
          {{ available_models_list.stdout | default('Aucun mod√®le pr√©charg√©') | indent(6, True) }}
          
          ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
          üîó POINTS D'ACC√àS API
          ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
          
          üìù API Documentation: {{ 'https' if ollama_config.tls_enabled else 'http' }}://{{ ollama_config.full_domain }}/api
          üè∑Ô∏è  Liste des mod√®les: {{ 'https' if ollama_config.tls_enabled else 'http' }}://{{ ollama_config.full_domain }}/api/tags
          üí¨ G√©n√©ration de texte: {{ 'https' if ollama_config.tls_enabled else 'http' }}://{{ ollama_config.full_domain }}/api/generate
          üó®Ô∏è  Chat IA: {{ 'https' if ollama_config.tls_enabled else 'http' }}://{{ ollama_config.full_domain }}/api/chat
          üìä Statut des processus: {{ 'https' if ollama_config.tls_enabled else 'http' }}://{{ ollama_config.full_domain }}/api/ps
          
          ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
          üîß COMMANDES D'ADMINISTRATION
          ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
          
          üìã V√©rifier les pods:
              kubectl get pods -n {{ ollama_config.namespace }} -l app.kubernetes.io/name={{ ollama_config.service_name }}
          
          üìú Consulter les logs:
              kubectl logs -n {{ ollama_config.namespace }} -l app.kubernetes.io/name={{ ollama_config.service_name }} -f
          
          üîç Port-forward pour tests locaux:
              kubectl port-forward -n {{ ollama_config.namespace }} svc/{{ ollama_config.service_name }} {{ ollama_config.port }}:{{ ollama_config.port }}
          
          üìä M√©triques Prometheus:
              kubectl get servicemonitor -n {{ ollama_config.namespace }} {{ ollama_config.service_name }}
          
          ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
          üß™ TESTS DE VALIDATION
          ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
          
          üåê Test de connectivit√©:
              curl {{ 'https' if ollama_config.tls_enabled else 'http' }}://{{ ollama_config.full_domain }}/api/tags
          
          üí¨ Test de g√©n√©ration simple:
              curl -X POST {{ 'https' if ollama_config.tls_enabled else 'http' }}://{{ ollama_config.full_domain }}/api/generate \
                   -H "Content-Type: application/json" \
                   -d '{"model":"phi3","prompt":"Hello, comment allez-vous?","stream":false}'
          
          üîç Test de sant√©:
              curl {{ 'https' if ollama_config.tls_enabled else 'http' }}://{{ ollama_config.full_domain }}/api/ps
          
          ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
          
          üéâ Ollama AI Service est maintenant op√©rationnel dans l'environnement {{ ollama_config.environment | upper }} !
          üìö Documentation compl√®te: https://docs.lions.dev/services/ollama
          üîß Support technique: devops@lions.dev

    - name: "D√©finir les variables de r√©sultat final"
      set_fact:
        ollama_deployment_result:
          status: "success"
          deployment_id: "{{ deployment_metadata.id }}"
          service_name: "{{ ollama_config.service_name }}"
          namespace: "{{ ollama_config.namespace }}"
          version: "{{ ollama_config.version }}"
          environment: "{{ ollama_config.environment }}"
          internal_url: "http://{{ ollama_config.service_name }}.{{ ollama_config.namespace }}.svc.cluster.local:{{ ollama_config.port }}"
          external_url: "{{ 'https' if ollama_config.tls_enabled else 'http' }}://{{ ollama_config.full_domain }}"
          gpu_enabled: "{{ ollama_config.gpu_enabled }}"
          storage_enabled: "{{ ollama_config.storage_enabled }}"
          monitoring_enabled: "{{ ollama_config.monitoring_enabled }}"
          models_available: "{{ available_models_list.stdout | default('') }}"
          deployed_at: "{{ ansible_date_time.iso8601 }}"
          deployed_by: "{{ deployment_metadata.operator }}"

  rescue:
    - name: "G√©rer l'√©chec de finalisation"
      debug:
        msg: "‚ö†Ô∏è  Finalisation partiellement √©chou√©e mais d√©ploiement r√©ussi"

  tags: [ollama, ai-ml, deploy, completion]

# =========================================================================
# GESTION D'ERREURS GLOBALE ET ROLLBACK AUTOMATIQUE
# =========================================================================
- name: "[OLLAMA-DEPLOY] Proc√©dure d'urgence en cas d'√©chec critique"
  block:
    - name: "D√©clencher la proc√©dure de rollback automatique"
      include_tasks: "rollback.yml"
      vars:
        rollback_reason: "critical_deployment_failure"
        deployment_id: "{{ deployment_metadata.id }}"
        preserve_data: true
      when:
        - ollama_config.environment == "production"
        - ansible_failed_task is defined

    - name: "Enregistrer l'√©chec dans Vault"
      uri:
        url: "{{ ollama_config.vault_addr }}/v1/secret/data/lions/{{ ollama_config.environment }}/ollama/deployment"
        method: POST
        headers:
          X-Vault-Token: "{{ vault_token | default('') }}"
          Content-Type: "application/json"
        body_format: json
        body:
          data:
            status: "failed"
            deployment_id: "{{ deployment_metadata.id }}"
            failed_at: "{{ ansible_date_time.iso8601 }}"
            error_details: "{{ ansible_failed_result.msg | default('Erreur critique inconnue') }}"
            rollback_initiated: true
        status_code: [200, 204]
        timeout: 30
      when:
        - ollama_config.vault_enabled
        - ansible_failed_task is defined

    - name: "D√©finir le r√©sultat d'√©chec"
      set_fact:
        ollama_deployment_result:
          status: "failed"
          deployment_id: "{{ deployment_metadata.id }}"
          failed_at: "{{ ansible_date_time.iso8601 }}"
          error_message: "{{ ansible_failed_result.msg | default('Erreur critique de d√©ploiement') }}"
          rollback_initiated: true

  when: ansible_failed_task is defined
  tags: [ollama, ai-ml, deploy, error-handling, rollback]