#!/bin/bash
# =============================================================================
# LIONS Infrastructure - Script d'installation principal v5.0
# =============================================================================
# Description: Script d'installation principal avec variables d'environnement pour l'environnement ${LIONS_ENVIRONMENT:-development}
# Version: 5.0.0
# Date: 01/06/2025
# Auteur: LIONS DevOps Team
# =============================================================================

# Chargement des variables d'environnement
readonly SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
readonly PROJECT_ROOT="$(cd "${SCRIPT_DIR}/.." && pwd)"

# Chargement des variables d'environnement depuis le fichier .env
if [ -f "${SCRIPT_DIR}/load-env.sh" ]; then
    source "${SCRIPT_DIR}/load-env.sh"
fi

# =============================================================================
# CONFIGURATION DEPUIS VARIABLES D'ENVIRONNEMENT
# =============================================================================
# Configuration de base
readonly LIONS_ENVIRONMENT="${LIONS_ENVIRONMENT:-development}"
readonly DEFAULT_ENV="${LIONS_ENVIRONMENT}"

# Configuration des chemins
readonly ANSIBLE_DIR="${LIONS_ANSIBLE_DIR:-${PROJECT_ROOT}/ansible}"
readonly LOG_DIR="${LIONS_LOG_DIR:-./logs/infrastructure}"
readonly LOG_FILE="${LOG_DIR}/install-$(date +%Y%m%d-%H%M%S).log"
readonly BACKUP_DIR="${LIONS_BACKUP_DIR:-${LOG_DIR}/backups}"
readonly STATE_FILE="${LIONS_STATE_FILE:-${LOG_DIR}/.installation_state}"
readonly LOCK_FILE="${LIONS_LOCK_FILE:-/tmp/lions_install.lock}"

# Configuration des ressources et limites
readonly REQUIRED_SPACE_MB="${LIONS_REQUIRED_SPACE_MB:-5000}"
readonly TIMEOUT_SECONDS="${LIONS_TIMEOUT_SECONDS:-1800}"
readonly MAX_RETRIES="${LIONS_MAX_RETRIES:-3}"
readonly SUDO_ALWAYS_ASK="${LIONS_SUDO_ALWAYS_ASK:-true}"

# Configuration des ports
readonly VPS_PORT="${LIONS_VPS_PORT:-22}"
readonly HTTP_PORT="${LIONS_HTTP_PORT:-80}"
readonly HTTPS_PORT="${LIONS_HTTPS_PORT:-443}"
readonly K3S_API_PORT="${LIONS_K3S_API_PORT:-6443}"
readonly APP_PORT_1="${LIONS_APP_PORT_1:-8080}"
readonly NODEPORT_START="${LIONS_NODEPORT_START:-30000}"
readonly NODEPORT_END="${LIONS_NODEPORT_END:-32767}"
readonly REQUIRED_PORTS=(${VPS_PORT} ${HTTP_PORT} ${HTTPS_PORT} ${K3S_API_PORT} ${APP_PORT_1} ${NODEPORT_START} ${NODEPORT_END})

# Configuration du debugging et logging
readonly DEBUG_MODE="${LIONS_DEBUG_MODE:-false}"
readonly VERBOSE_MODE="${LIONS_VERBOSE_MODE:-false}"
readonly LOG_LEVEL="${LIONS_LOG_LEVEL:-INFO}"
readonly LOG_MAX_SIZE="${LIONS_LOG_MAX_SIZE:-10485760}"  # 10MB
readonly LOG_RETENTION_DAYS="${LIONS_LOG_RETENTION_DAYS:-7}"

# Configuration Ansible
readonly ANSIBLE_BECOME_ASK_PASS="${LIONS_ANSIBLE_BECOME_ASK_PASS:-true}"
readonly ANSIBLE_HOST_KEY_CHECKING="${LIONS_ANSIBLE_HOST_KEY_CHECKING:-false}"
readonly ANSIBLE_TIMEOUT="${LIONS_ANSIBLE_TIMEOUT:-300}"
readonly ANSIBLE_SSH_RETRIES="${LIONS_ANSIBLE_SSH_RETRIES:-3}"

# Configuration VPS
readonly VPS_MEMORY_MIN_GB="${LIONS_VPS_MEMORY_MIN_GB:-2}"
readonly VPS_CPU_MIN_CORES="${LIONS_VPS_CPU_MIN_CORES:-2}"
readonly VPS_DISK_MIN_GB="${LIONS_VPS_DISK_MIN_GB:-20}"
readonly VPS_SSH_TIMEOUT="${LIONS_VPS_SSH_TIMEOUT:-10}"
readonly VPS_HEALTH_CHECK_INTERVAL="${LIONS_VPS_HEALTH_CHECK_INTERVAL:-30}"

# Configuration K3s
readonly K3S_VERSION="${LIONS_K3S_VERSION:-v1.30.0+k3s1}"
readonly K3S_CHANNEL="${LIONS_K3S_CHANNEL:-stable}"
readonly K3S_TOKEN="${LIONS_K3S_TOKEN:-}"
readonly K3S_DATASTORE_ENDPOINT="${LIONS_K3S_DATASTORE_ENDPOINT:-}"
readonly K3S_DISABLE_COMPONENTS="${LIONS_K3S_DISABLE_COMPONENTS:-traefik,servicelb}"
readonly K3S_NODE_LABELS="${LIONS_K3S_NODE_LABELS:-}"
readonly K3S_NODE_TAINTS="${LIONS_K3S_NODE_TAINTS:-}"

# Configuration des applications
readonly GRAFANA_ADMIN_USER="${LIONS_GRAFANA_ADMIN_USER:-admin}"
readonly GRAFANA_ADMIN_PASSWORD="${LIONS_GRAFANA_ADMIN_PASSWORD:-admin}"
readonly PROMETHEUS_RETENTION="${LIONS_PROMETHEUS_RETENTION:-15d}"
readonly ALERTMANAGER_WEBHOOK_URL="${LIONS_ALERTMANAGER_WEBHOOK_URL:-}"

# Configuration de sÃ©curitÃ©
readonly ENABLE_FIREWALL="${LIONS_ENABLE_FIREWALL:-true}"
readonly ENABLE_FAIL2BAN="${LIONS_ENABLE_FAIL2BAN:-true}"
readonly SSH_KEY_PATH="${LIONS_SSH_KEY_PATH:-~/.ssh/id_rsa}"
readonly DISABLE_ROOT_LOGIN="${LIONS_DISABLE_ROOT_LOGIN:-true}"

# Configuration de sauvegarde
readonly BACKUP_ENABLED="${LIONS_BACKUP_ENABLED:-true}"
readonly BACKUP_SCHEDULE="${LIONS_BACKUP_SCHEDULE:-0 2 * * *}"
readonly BACKUP_RETENTION_DAYS="${LIONS_BACKUP_RETENTION_DAYS:-30}"
readonly BACKUP_STORAGE_PATH="${LIONS_BACKUP_STORAGE_PATH:-/var/backups/lions}"

# Configuration de monitoring
readonly MONITORING_ENABLED="${LIONS_MONITORING_ENABLED:-true}"
readonly METRICS_RETENTION="${LIONS_METRICS_RETENTION:-15d}"
readonly ALERT_MANAGER_ENABLED="${LIONS_ALERT_MANAGER_ENABLED:-true}"
readonly GRAFANA_ENABLED="${LIONS_GRAFANA_ENABLED:-true}"

# Configuration rÃ©seau
readonly CLUSTER_CIDR="${LIONS_CLUSTER_CIDR:-10.42.0.0/16}"
readonly SERVICE_CIDR="${LIONS_SERVICE_CIDR:-10.43.0.0/16}"
readonly CLUSTER_DNS="${LIONS_CLUSTER_DNS:-10.43.0.10}"
readonly CLUSTER_DOMAIN="${LIONS_CLUSTER_DOMAIN:-cluster.local}"

# Configuration avancÃ©e
readonly SKIP_INIT="${LIONS_SKIP_INIT:-false}"
readonly TEST_MODE="${LIONS_TEST_MODE:-false}"
readonly DRY_RUN="${LIONS_DRY_RUN:-false}"
readonly FORCE_REINSTALL="${LIONS_FORCE_REINSTALL:-false}"
readonly AUTO_APPROVE="${LIONS_AUTO_APPROVE:-false}"

# Couleurs pour l'affichage
readonly COLOR_RESET="\033[0m"
readonly COLOR_RED="\033[0;31m"
readonly COLOR_GREEN="\033[0;32m"
readonly COLOR_YELLOW="\033[0;33m"
readonly COLOR_BLUE="\033[0;34m"
readonly COLOR_MAGENTA="\033[0;35m"
readonly COLOR_CYAN="\033[0;36m"
readonly COLOR_WHITE="\033[0;37m"
readonly COLOR_BOLD="\033[1m"

# =============================================================================
# VARIABLES GLOBALES
# =============================================================================
INSTALLATION_STEP=""
LAST_COMMAND=""
LAST_ERROR=""
RETRY_COUNT=0
debug_mode="${DEBUG_MODE}"
verbose_mode="${VERBOSE_MODE}"
test_mode="${TEST_MODE}"
skip_init="${SKIP_INIT}"
dry_run="${DRY_RUN}"
force_reinstall="${FORCE_REINSTALL}"
auto_approve="${AUTO_APPROVE}"

# Variables d'environnement par dÃ©faut (peuvent Ãªtre surchargÃ©es par les arguments)
environment="${LIONS_ENVIRONMENT}"
inventory_file="inventories/${environment}/hosts.yml"

# CrÃ©ation des rÃ©pertoires nÃ©cessaires
mkdir -p "${LOG_DIR}"
mkdir -p "${BACKUP_DIR}"

# Activation du mode strict aprÃ¨s les vÃ©rifications initiales
set -euo pipefail

# Fonction de logging amÃ©liorÃ©e
function log() {
    local level="$1"
    local message="$2"
    local timestamp
    timestamp="$(date +"%Y-%m-%d %H:%M:%S")"
    local caller_info=""
    local log_color="${COLOR_RESET}"
    local log_prefix=""

    # DÃ©termination de la fonction appelante et du numÃ©ro de ligne
    if [[ "${debug_mode}" == "true" ]]; then
        # RÃ©cupÃ©ration de la trace d'appel (fonction appelante et numÃ©ro de ligne)
        local caller_function=$(caller 0 | awk '{print $2}')
        local caller_line=$(caller 0 | awk '{print $1}')

        if [[ -n "${caller_function}" && "${caller_function}" != "main" ]]; then
            caller_info=" [${caller_function}:${caller_line}]"
        else
            caller_info=" [ligne:${caller_line}]"
        fi
    fi

    # SÃ©lection de la couleur et du prÃ©fixe en fonction du niveau
    case "${level}" in
        "INFO")     log_color="${COLOR_BLUE}"; log_prefix="â„¹ï¸ " ;;
        "WARNING")  log_color="${COLOR_YELLOW}"; log_prefix="âš ï¸ " ;;
        "ERROR")    log_color="${COLOR_RED}"; log_prefix="âŒ " ;;
        "DEBUG")    log_color="${COLOR_MAGENTA}"; log_prefix="ðŸ” " ;;
        "SUCCESS")  log_color="${COLOR_GREEN}"; log_prefix="âœ… " ;;
        "STEP")     log_color="${COLOR_CYAN}${COLOR_BOLD}"; log_prefix="ðŸ”„ " ;;
    esac

    # Affichage du message avec formatage
    # Ajout d'un caractÃ¨re de retour Ã  la ligne explicite pour Ã©viter les problÃ¨mes d'affichage
    echo -e "${log_color}${log_prefix}[${timestamp}] [${level}]${caller_info}${COLOR_RESET} ${message}\n"

    # Enregistrement dans le fichier de log
    echo "[${timestamp}] [${level}]${caller_info} ${message}" >> "${LOG_FILE}"

    # Enregistrement des erreurs dans un fichier sÃ©parÃ© pour faciliter le diagnostic
    if [[ "${level}" == "ERROR" ]]; then
        echo "[${timestamp}] [${level}]${caller_info} ${message}" >> "${LOG_DIR}/errors.log"
    fi

    # Enregistrement des avertissements dans un fichier sÃ©parÃ©
    if [[ "${level}" == "WARNING" ]]; then
        echo "[${timestamp}] [${level}]${caller_info} ${message}" >> "${LOG_DIR}/warnings.log"
    fi
}

# Fonction pour exÃ©cuter une commande avec timeout, avec fallback si la commande timeout n'est pas disponible
function run_with_timeout_fallback() {
    local timeout_seconds="$1"
    shift
    # Utiliser un tableau pour stocker la commande et ses arguments
    local -a cmd_array=("$@")

    # VÃ©rifier si la commande timeout est disponible
    if command -v timeout &>/dev/null; then
        timeout "${timeout_seconds}" "${cmd_array[@]}"
        return $?
    else
        # Fallback: exÃ©cuter la commande en arriÃ¨re-plan et la tuer si elle prend trop de temps
        log "DEBUG" "Commande timeout non disponible, utilisation du fallback"

        # CrÃ©er un fichier temporaire pour stocker le PID
        local pid_file
        pid_file=$(mktemp)

        # ExÃ©cuter la commande en arriÃ¨re-plan
        ("${cmd_array[@]}") & echo $! > "${pid_file}" &
        local cmd_pid
        cmd_pid=$(cat "${pid_file}")

        # Attendre que la commande se termine ou que le timeout soit atteint
        local start_time
        start_time=$(date +%s)
        local end_time
        end_time=$((start_time + timeout_seconds))
        local current_time
        current_time=$(date +%s)

        while [[ ${current_time} -lt ${end_time} ]]; do
            # VÃ©rifier si le processus est toujours en cours d'exÃ©cution
            if ! kill -0 "${cmd_pid}" 2>/dev/null; then
                # Le processus s'est terminÃ©
                wait "${cmd_pid}"
                local exit_code=$?
                rm -f "${pid_file}"
                return ${exit_code}
            fi

            # Attendre un peu avant de vÃ©rifier Ã  nouveau
            sleep 1
            current_time=$(date +%s)
        done

        # Si on arrive ici, c'est que le timeout a Ã©tÃ© atteint
        log "DEBUG" "Timeout atteint, arrÃªt forcÃ© de la commande"
        kill -9 "${cmd_pid}" 2>/dev/null || true
        rm -f "${pid_file}"
        return 124  # Code de retour standard pour timeout
    fi
}

# Fonction pour exÃ©cuter une commande SSH de maniÃ¨re robuste
function robust_ssh() {
    local timeout=10
    local host="$1"
    local port="$2"
    local user="$3"
    local command="$4"
    local output_var="$5"  # Variable optionnelle pour stocker la sortie
    local silent="${6:-false}"  # Option pour exÃ©cuter silencieusement

    # Tentative avec BatchMode (clÃ©s SSH uniquement)
    if [[ "${silent}" == "true" ]]; then
        local output=$(ssh -o BatchMode=yes -o ConnectTimeout="${timeout}" -p "${port}" "${user}@${host}" "${command}" 2>/dev/null)
        local exit_code=$?
    else
        local output=$(ssh -o BatchMode=yes -o ConnectTimeout="${timeout}" -p "${port}" "${user}@${host}" "${command}")
        local exit_code=$?
    fi

    # Si la premiÃ¨re tentative Ã©choue, essayer avec StrictHostKeyChecking=no
    if [[ ${exit_code} -ne 0 ]]; then
        if [[ "${silent}" == "true" ]]; then
            log "DEBUG" "Tentative SSH avec BatchMode a Ã©chouÃ©, essai avec StrictHostKeyChecking=no"
            output=$(ssh -o StrictHostKeyChecking=no -o ConnectTimeout="${timeout}" -p "${port}" "${user}@${host}" "${command}" 2>/dev/null)
            exit_code=$?
        else
            log "DEBUG" "Tentative SSH avec BatchMode a Ã©chouÃ©, essai avec StrictHostKeyChecking=no"
            output=$(ssh -o StrictHostKeyChecking=no -o ConnectTimeout="${timeout}" -p "${port}" "${user}@${host}" "${command}")
            exit_code=$?
        fi
    fi

    # Si une variable de sortie est fournie, y stocker la sortie
    if [[ -n "${output_var}" ]]; then
        # Pour les valeurs numÃ©riques, on veut Ã©viter d'Ã©chapper les caractÃ¨res
        # car cela peut interfÃ©rer avec la conversion en nombres
        if [[ "$output" =~ ^[0-9]+$ ]]; then
            # Si la sortie est un nombre entier, on peut l'assigner directement
            eval "${output_var}=${output}"
        else
            # Nettoyage de la sortie pour extraire uniquement les chiffres si c'est une commande qui devrait retourner un nombre
            if [[ "${command}" == *"nproc"* || "${command}" == *"free -m"* || "${command}" == *"df -m"* ]]; then
                # Pour les commandes qui devraient retourner des nombres, on extrait uniquement les chiffres
                local cleaned_output
                cleaned_output=$(echo "$output" | tr -cd '0-9')
                if [[ -n "$cleaned_output" ]]; then
                    eval "${output_var}=${cleaned_output}"
                else
                    # Si aprÃ¨s nettoyage on n'a pas de chiffres, on assigne 0
                    eval "${output_var}=0"
                fi
            else
                # Sinon, on utilise une mÃ©thode plus sÃ»re pour gÃ©rer les caractÃ¨res spÃ©ciaux
                local escaped_output
                escaped_output=$(printf '%q' "$output")
                eval "${output_var}=${escaped_output}"
            fi
        fi
    fi

    return ${exit_code}
}

# Fonction pour collecter et analyser les logs
function collect_logs() {
    local output_dir
    output_dir="${LOG_DIR}/diagnostic-$(date +%Y%m%d-%H%M%S)"
    mkdir -p "${output_dir}"

    log "INFO" "Collecte des logs pour diagnostic dans ${output_dir}..."

    # Copie du log d'installation
    cp "${LOG_FILE}" "${output_dir}/install.log"

    # Collecte des logs du VPS
    if [[ -n "${ansible_host}" && -n "${ansible_port}" && -n "${ansible_user}" ]]; then
        log "INFO" "Collecte des logs du VPS..."

        # CrÃ©ation d'un script temporaire pour collecter les logs sur le VPS
        local tmp_script
        tmp_script=$(mktemp)
        cat > "${tmp_script}" << 'EOF'
#!/bin/bash
# Script de collecte de logs sur le VPS
OUTPUT_DIR="/tmp/lions_logs"
mkdir -p "${OUTPUT_DIR}"

# Logs systÃ¨me
echo "Collecte des logs systÃ¨me..."
dmesg > "${OUTPUT_DIR}/dmesg.log" 2>/dev/null || true
journalctl -n 1000 > "${OUTPUT_DIR}/journalctl.log" 2>/dev/null || true
journalctl -u k3s -n 500 > "${OUTPUT_DIR}/k3s.log" 2>/dev/null || true
journalctl -u kubelet -n 500 > "${OUTPUT_DIR}/kubelet.log" 2>/dev/null || true

# Informations systÃ¨me
echo "Collecte des informations systÃ¨me..."
uname -a > "${OUTPUT_DIR}/uname.log" 2>/dev/null || true
free -h > "${OUTPUT_DIR}/memory.log" 2>/dev/null || true
df -h > "${OUTPUT_DIR}/disk.log" 2>/dev/null || true
top -b -n 1 > "${OUTPUT_DIR}/top.log" 2>/dev/null || true
ps aux > "${OUTPUT_DIR}/ps.log" 2>/dev/null || true
netstat -tuln > "${OUTPUT_DIR}/netstat.log" 2>/dev/null || true
ip addr > "${OUTPUT_DIR}/ip_addr.log" 2>/dev/null || true
ip route > "${OUTPUT_DIR}/ip_route.log" 2>/dev/null || true

# Logs Kubernetes
if command -v kubectl &>/dev/null; then
    echo "Collecte des logs Kubernetes..."
    kubectl get nodes -o wide > "${OUTPUT_DIR}/k8s_nodes.log" 2>/dev/null || true
    kubectl get pods --all-namespaces -o wide > "${OUTPUT_DIR}/k8s_pods.log" 2>/dev/null || true
    kubectl get services --all-namespaces > "${OUTPUT_DIR}/k8s_services.log" 2>/dev/null || true
    kubectl get deployments --all-namespaces > "${OUTPUT_DIR}/k8s_deployments.log" 2>/dev/null || true
    kubectl get events --all-namespaces --sort-by='.lastTimestamp' > "${OUTPUT_DIR}/k8s_events.log" 2>/dev/null || true

    # Logs des pods en erreur
    for pod in $(kubectl get pods --all-namespaces -o jsonpath='{range .items[?(@.status.phase!="Running")]}{.metadata.namespace}/{.metadata.name}{"\n"}{end}'); do
        ns=$(echo "${pod}" | cut -d/ -f1)
        name=$(echo "${pod}" | cut -d/ -f2)
        kubectl logs -n "${ns}" "${name}" > "${OUTPUT_DIR}/pod_${ns}_${name}.log" 2>/dev/null || true
        kubectl describe pod -n "${ns}" "${name}" > "${OUTPUT_DIR}/pod_${ns}_${name}_describe.log" 2>/dev/null || true
    done
fi

# Compression des logs
tar -czf "/tmp/lions_logs.tar.gz" -C "/tmp" "lions_logs" 2>/dev/null || true
rm -rf "${OUTPUT_DIR}"

echo "Collecte des logs terminÃ©e"
EOF

        # Copie et exÃ©cution du script sur le VPS
        scp -P "${ansible_port}" "${tmp_script}" "${ansible_user}@${ansible_host}:/tmp/collect_logs.sh" &>/dev/null
        ssh -o BatchMode=yes -o ConnectTimeout=10 -p "${ansible_port}" "${ansible_user}@${ansible_host}" "chmod +x /tmp/collect_logs.sh && sudo /tmp/collect_logs.sh" &>/dev/null

        # RÃ©cupÃ©ration des logs
        scp -P "${ansible_port}" "${ansible_user}@${ansible_host}:/tmp/lions_logs.tar.gz" "${output_dir}/vps_logs.tar.gz" &>/dev/null

        # Nettoyage
        ssh -o BatchMode=yes -o ConnectTimeout=5 -p "${ansible_port}" "${ansible_user}@${ansible_host}" "rm -f /tmp/collect_logs.sh /tmp/lions_logs.tar.gz" &>/dev/null
        rm -f "${tmp_script}"

        # Extraction des logs
        mkdir -p "${output_dir}/vps_logs"
        tar -xzf "${output_dir}/vps_logs.tar.gz" -C "${output_dir}/vps_logs" &>/dev/null
        rm -f "${output_dir}/vps_logs.tar.gz"
    fi

    # Collecte des logs locaux
    log "INFO" "Collecte des logs locaux..."

    # Informations systÃ¨me locales
    uname -a > "${output_dir}/local_uname.log" 2>/dev/null || true
    df -h > "${output_dir}/local_disk.log" 2>/dev/null || true

    # Logs Kubernetes locaux
    if command_exists kubectl; then
        kubectl version --client > "${output_dir}/local_kubectl_version.log" 2>/dev/null || true
        kubectl config view --minify > "${output_dir}/local_kubectl_config.log" 2>/dev/null || true
    fi

    # Logs Ansible locaux
    if command_exists ansible-playbook; then
        ansible-playbook --version > "${output_dir}/local_ansible_version.log" 2>/dev/null || true
    fi

    # Compression des logs
    log "INFO" "Compression des logs..."
    local archive_file
    archive_file="${LOG_DIR}/diagnostic-$(date +%Y%m%d-%H%M%S).tar.gz"
    tar -czf "${archive_file}" -C "$(dirname "${output_dir}")" "$(basename "${output_dir}")" &>/dev/null
    rm -rf "${output_dir}"

    log "SUCCESS" "Logs collectÃ©s et archivÃ©s dans ${archive_file}"

    # Analyse des logs
    log "INFO" "Analyse des logs..."

    # Extraction des erreurs courantes
    if tar -xzf "${archive_file}" -C /tmp &>/dev/null; then
        local extracted_dir="/tmp/$(basename "${output_dir}")"

        # Recherche des erreurs courantes
        log "INFO" "Recherche des erreurs courantes..."

        # Erreurs de connexion
        if grep -r "Connection refused" "${extracted_dir}" &>/dev/null; then
            log "WARNING" "Erreurs de connexion dÃ©tectÃ©es - vÃ©rifiez que les services sont en cours d'exÃ©cution"
        fi

        # Erreurs de permission
        if grep -r "Permission denied" "${extracted_dir}" &>/dev/null; then
            log "WARNING" "Erreurs de permission dÃ©tectÃ©es - vÃ©rifiez les droits d'accÃ¨s"
        fi

        # Erreurs d'espace disque
        if grep -r "No space left on device" "${extracted_dir}" &>/dev/null; then
            log "WARNING" "Erreurs d'espace disque dÃ©tectÃ©es - libÃ©rez de l'espace et rÃ©essayez"
        fi

        # Erreurs de mÃ©moire
        if grep -r "Out of memory" "${extracted_dir}" &>/dev/null; then
            log "WARNING" "Erreurs de mÃ©moire dÃ©tectÃ©es - augmentez la mÃ©moire disponible"
        fi

        # Erreurs de rÃ©seau
        if grep -r "Network is unreachable" "${extracted_dir}" &>/dev/null; then
            log "WARNING" "Erreurs de rÃ©seau dÃ©tectÃ©es - vÃ©rifiez la connectivitÃ© rÃ©seau"
        fi

        # Nettoyage
        rm -rf "${extracted_dir}"
    fi

    return 0
}

# Fonction de gestion des erreurs amÃ©liorÃ©e
function handle_error() {
    local exit_code=$?
    local line_number=$1
    local command_name=$2
    local timestamp=$(date +"%Y-%m-%d %H:%M:%S")
    local hostname=$(hostname)
    local user=$(whoami)

    # DÃ©sactivation du mode strict pour la gestion des erreurs
    set +euo pipefail

    log "ERROR" "Une erreur s'est produite Ã  la ligne ${line_number} (code ${exit_code})"
    log "ERROR" "DerniÃ¨re commande exÃ©cutÃ©e: ${LAST_COMMAND}"

    # Collecte d'informations de diagnostic supplÃ©mentaires
    local error_details=""
    case ${exit_code} in
        1)   error_details="Erreur gÃ©nÃ©rale ou erreur de commande inconnue" ;;
        2)   error_details="Erreur de syntaxe dans l'utilisation de la commande" ;;
        126) error_details="La commande ne peut pas Ãªtre exÃ©cutÃ©e (problÃ¨me de permissions)" ;;
        127) error_details="Commande non trouvÃ©e" ;;
        128) error_details="Argument invalide pour exit" ;;
        130) error_details="Script terminÃ© par Ctrl+C" ;;
        137) error_details="Script terminÃ© par SIGKILL (possiblement manque de mÃ©moire)" ;;
        139) error_details="Erreur de segmentation (bug dans un programme)" ;;
        *)   error_details="Code d'erreur non spÃ©cifique" ;;
    esac

    log "ERROR" "DÃ©tails de l'erreur: ${error_details}"

    # Collecte d'informations systÃ¨me pour le diagnostic
    local memory_info=$(free -h | grep Mem)
    local disk_info=$(df -h / | tail -n 1)
    local load_avg=$(cat /proc/loadavg)
    local process_count=$(ps aux | wc -l)

    # VÃ©rification des processus consommant beaucoup de ressources
    local top_processes=$(ps aux --sort=-%cpu | head -n 5)

    # Enregistrement de l'erreur avec plus de dÃ©tails
    LAST_ERROR="Erreur Ã  la ligne ${line_number} (code ${exit_code}): ${LAST_COMMAND} - ${error_details}"

    # CrÃ©ation d'un rapport d'erreur dÃ©taillÃ©
    local error_report="${BACKUP_DIR}/error-report-$(date +%Y%m%d-%H%M%S).log"
    {
        echo "=== RAPPORT D'ERREUR LIONS INFRASTRUCTURE ==="
        echo "Date/Heure: ${timestamp}"
        echo "HÃ´te: ${hostname}"
        echo "Utilisateur: ${user}"
        echo "Ã‰tape d'installation: ${INSTALLATION_STEP}"
        echo "Ligne: ${line_number}"
        echo "Code d'erreur: ${exit_code}"
        echo "Commande: ${LAST_COMMAND}"
        echo "DÃ©tails: ${error_details}"
        echo ""
        echo "=== INFORMATIONS SYSTÃˆME ==="
        echo "MÃ©moire: ${memory_info}"
        echo "Disque: ${disk_info}"
        echo "Charge systÃ¨me: ${load_avg}"
        echo "Nombre de processus: ${process_count}"
        echo ""
        echo "=== PROCESSUS PRINCIPAUX ==="
        echo "${top_processes}"
        echo ""
        echo "=== DERNIÃˆRES LIGNES DU LOG ==="
        tail -n 50 "${LOG_FILE}"
    } > "${error_report}"

    log "INFO" "Rapport d'erreur dÃ©taillÃ© crÃ©Ã©: ${error_report}"

    # Sauvegarde de l'Ã©tat actuel et des logs pour analyse
    echo "${INSTALLATION_STEP}" > "${STATE_FILE}"
    cp "${LOG_FILE}" "${BACKUP_DIR}/error-log-$(date +%Y%m%d-%H%M%S).log"

    # VÃ©rification de sÃ©curitÃ© - recherche de signes de compromission
    log "INFO" "VÃ©rification de sÃ©curitÃ© en cours..."
    if [[ -f "/var/log/auth.log" ]]; then
        if grep -E "Failed password|Invalid user|authentication failure" /var/log/auth.log | tail -n 20 > "${BACKUP_DIR}/security-check-$(date +%Y%m%d-%H%M%S).log"; then
            log "WARNING" "Tentatives d'authentification suspectes dÃ©tectÃ©es - voir le rapport de sÃ©curitÃ©"
        fi
    fi

    # VÃ©rification de l'Ã©tat du systÃ¨me avant de tenter une reprise
    log "INFO" "VÃ©rification de l'Ã©tat du systÃ¨me avant reprise..."

    # VÃ©rification de la connectivitÃ© rÃ©seau
    if ! ping -c 1 -W 5 "${ansible_host}" &>/dev/null; then
        log "ERROR" "ConnectivitÃ© rÃ©seau perdue avec le VPS (${ansible_host})"
        log "ERROR" "Impossible de reprendre l'installation sans connectivitÃ© rÃ©seau"
        cleanup
        exit 1
    fi

    # VÃ©rification de l'espace disque
    if ! check_disk_space; then
        log "ERROR" "Espace disque insuffisant pour continuer l'installation"
        cleanup
        exit 1
    fi

    # Analyse de l'erreur pour dÃ©terminer la stratÃ©gie de reprise
    local retry_strategy="standard"
    local retry_delay=10
    local retry_possible=true

    # DÃ©termination de la stratÃ©gie de reprise en fonction du code d'erreur et du contexte
    case ${exit_code} in
        137)  # Out of memory
            retry_strategy="memory_optimization"
            retry_delay=30
            log "WARNING" "Erreur de mÃ©moire dÃ©tectÃ©e, application de la stratÃ©gie d'optimisation mÃ©moire"
            ;;
        130)  # Ctrl+C
            retry_possible=false
            log "WARNING" "Interruption manuelle dÃ©tectÃ©e, reprise automatique dÃ©sactivÃ©e"
            ;;
        126|127)  # ProblÃ¨mes de permissions ou commande non trouvÃ©e
            retry_strategy="permission_fix"
            log "WARNING" "ProblÃ¨me de permissions dÃ©tectÃ©, tentative de correction"
            ;;
        *)
            # Analyse du message d'erreur pour des cas spÃ©cifiques
            if [[ "${LAST_COMMAND}" == *"timeout"* || "${LAST_COMMAND}" == *"connection refused"* ]]; then
                retry_strategy="network_retry"
                retry_delay=20
                log "WARNING" "ProblÃ¨me rÃ©seau dÃ©tectÃ©, application de la stratÃ©gie rÃ©seau"
            elif [[ "${LAST_COMMAND}" == *"disk"* || "${LAST_COMMAND}" == *"space"* ]]; then
                retry_strategy="disk_cleanup"
                log "WARNING" "ProblÃ¨me d'espace disque dÃ©tectÃ©, tentative de nettoyage"
            fi
            ;;
    esac

    # Tentative de reprise si possible
    if [[ ${retry_possible} == true && ${RETRY_COUNT} -lt ${MAX_RETRIES} ]]; then
        RETRY_COUNT=$((RETRY_COUNT + 1))
        log "WARNING" "Tentative de reprise (${RETRY_COUNT}/${MAX_RETRIES}) - StratÃ©gie: ${retry_strategy}"

        # Actions spÃ©cifiques selon la stratÃ©gie de reprise
        case ${retry_strategy} in
            "memory_optimization")
                log "INFO" "Nettoyage de la mÃ©moire avant reprise..."
                sync
                echo 3 > /proc/sys/vm/drop_caches 2>/dev/null || true
                killall -9 java 2>/dev/null || true  # ArrÃªt des processus Java gourmands en mÃ©moire
                ;;
            "permission_fix")
                log "INFO" "Correction des permissions..."
                if [[ -n "${LAST_COMMAND}" && "${LAST_COMMAND}" == *" "* ]]; then
                    local cmd_path=$(echo "${LAST_COMMAND}" | awk '{print $1}')
                    if [[ -f "${cmd_path}" ]]; then
                        log "INFO" "Correction des permissions pour ${cmd_path}"
                        chmod +x "${cmd_path}" 2>/dev/null || sudo chmod +x "${cmd_path}" 2>/dev/null || true
                    fi
                fi
                ;;
            "disk_cleanup")
                log "INFO" "Nettoyage de l'espace disque..."
                rm -rf /tmp/* 2>/dev/null || true
                docker system prune -f 2>/dev/null || true
                journalctl --vacuum-time=1d 2>/dev/null || true
                ;;
            "network_retry")
                log "INFO" "Optimisation rÃ©seau avant reprise..."
                # Attente plus longue pour les problÃ¨mes rÃ©seau
                retry_delay=30
                # Tentative de redÃ©marrage du service rÃ©seau
                systemctl restart networking 2>/dev/null || true
                ;;
        esac

        # Suppression du fichier de verrouillage avant la reprise
        if [[ -f "${LOCK_FILE}" ]]; then
            log "INFO" "Suppression du fichier de verrouillage avant la reprise..."
            # Tentative de suppression sans sudo d'abord
            if ! rm -f "${LOCK_FILE}" 2>/dev/null; then
                log "WARNING" "Impossible de supprimer le fichier de verrouillage sans sudo, tentative avec sudo..."
                # Si Ã§a Ã©choue, essayer avec sudo
                if sudo rm -f "${LOCK_FILE}"; then
                    log "SUCCESS" "Fichier de verrouillage supprimÃ© avec succÃ¨s (sudo)"
                else
                    log "WARNING" "Impossible de supprimer le fichier de verrouillage, mÃªme avec sudo"
                fi
            fi
        fi

        # Attente avant la reprise pour permettre au systÃ¨me de se stabiliser
        log "INFO" "Attente de ${retry_delay} secondes avant reprise..."
        sleep ${retry_delay}

        # Reprise en fonction de l'Ã©tape avec gestion spÃ©cifique selon la commande qui a Ã©chouÃ©
        case "${INSTALLATION_STEP}" in
            "init_vps")
                log "INFO" "Reprise de l'initialisation du VPS..."
                if [[ "${command_name}" == "ansible_playbook" ]]; then
                    log "INFO" "Tentative de reprise avec des options plus sÃ»res..."
                    # Tentative avec des options plus sÃ»res pour Ansible
                    local ansible_cmd="ansible-playbook -i \"${ANSIBLE_DIR}/${inventory_file}\" \"${ANSIBLE_DIR}/playbooks/init-vps.yml\" --forks=1 --timeout=60"

                    # Ajouter l'option --ask-become-pass seulement si l'exÃ©cution n'est pas locale
                    if [[ "${IS_LOCAL_EXECUTION}" != "true" ]]; then
                        ansible_cmd="${ansible_cmd} --ask-become-pass"
                    fi

                    eval "${ansible_cmd}"
                else
                    initialiser_vps
                fi
                ;;
            "install_k3s")
                log "INFO" "Reprise de l'installation de K3s..."
                if [[ "${command_name}" == "ansible_playbook" ]]; then
                    log "INFO" "Tentative de reprise avec des options plus sÃ»res..."
                    # Tentative avec des options plus sÃ»res pour Ansible
                    local ansible_cmd="ansible-playbook -i \"${ANSIBLE_DIR}/${inventory_file}\" \"${ANSIBLE_DIR}/playbooks/install-k3s.yml\" --forks=1 --timeout=60"

                    # Ajouter l'option --ask-become-pass seulement si l'exÃ©cution n'est pas locale
                    if [[ "${IS_LOCAL_EXECUTION}" != "true" ]]; then
                        ansible_cmd="${ansible_cmd} --ask-become-pass"
                    fi

                    eval "${ansible_cmd}"
                else
                    installer_k3s
                fi
                ;;
            "deploy_infra")
                log "INFO" "Reprise du dÃ©ploiement de l'infrastructure de base..."
                if [[ "${command_name}" == "kubectl_apply" ]]; then
                    log "INFO" "Tentative de reprise avec validation dÃ©sactivÃ©e..."
                    kubectl apply -k "${PROJECT_ROOT}/kubernetes/overlays/${environment}" --validate=false --timeout=10m
                else
                    deployer_infrastructure_base
                fi
                ;;
            "deploy_monitoring")
                log "INFO" "Reprise du dÃ©ploiement du monitoring..."
                if [[ "${command_name}" == "helm_install" ]]; then
                    log "INFO" "Tentative de reprise avec des options plus sÃ»res..."
                    helm upgrade --install prometheus prometheus-community/kube-prometheus-stack --namespace monitoring --values "${values_file}" --timeout 15m --atomic
                else
                    deployer_monitoring
                fi
                ;;
            "verify")
                log "INFO" "Reprise de la vÃ©rification de l'installation..."
                verifier_installation
                ;;
            "prerequis")
                log "INFO" "Reprise de la vÃ©rification des prÃ©requis..."
                verifier_prerequis
                ;;
            *)
                log "ERROR" "Impossible de reprendre Ã  l'Ã©tape '${INSTALLATION_STEP}'"
                log "ERROR" "Veuillez consulter les logs pour plus d'informations et corriger manuellement le problÃ¨me"
                log "INFO" "Vous pouvez ensuite relancer le script avec l'option --skip-init si l'initialisation a dÃ©jÃ  Ã©tÃ© effectuÃ©e"
                cleanup
                exit ${exit_code}
                ;;
        esac
    else
        log "ERROR" "Nombre maximal de tentatives atteint (${MAX_RETRIES})"
        log "ERROR" "DerniÃ¨re erreur: ${LAST_ERROR}"

        # GÃ©nÃ©ration d'un rapport de diagnostic
        generate_diagnostic_report

        log "INFO" "Un rapport de diagnostic a Ã©tÃ© gÃ©nÃ©rÃ© dans ${BACKUP_DIR}/diagnostic-$(date +%Y%m%d-%H%M%S).txt"
        log "INFO" "Veuillez consulter ce rapport pour identifier et corriger le problÃ¨me"

        cleanup
        exit ${exit_code}
    fi
}

# Fonction de gÃ©nÃ©ration de rapport de diagnostic
function generate_diagnostic_report() {
    local report_file
    report_file="${BACKUP_DIR}/diagnostic-$(date +%Y%m%d-%H%M%S).txt"

    log "INFO" "GÃ©nÃ©ration d'un rapport de diagnostic complet..."

    {
        echo "=== RAPPORT DE DIAGNOSTIC LIONS INFRASTRUCTURE ==="
        echo "Date: $(date)"
        echo "Environnement: ${environment}"
        echo "Ã‰tape d'installation: ${INSTALLATION_STEP}"
        echo ""

        echo "=== INFORMATIONS SUR L'ERREUR ==="
        echo "DerniÃ¨re commande: ${LAST_COMMAND}"
        echo "DerniÃ¨re erreur: ${LAST_ERROR}"
        echo "Nombre de tentatives: ${RETRY_COUNT}/${MAX_RETRIES}"
        echo ""

        echo "=== INFORMATIONS SYSTÃˆME LOCAL ==="
        echo "SystÃ¨me d'exploitation: $(uname -a)"
        echo "Espace disque disponible: $(df -h . | awk 'NR==2 {print $4}')"
        echo "MÃ©moire disponible: $(free -h | awk '/^Mem:/ {print $7}')"
        echo ""

        echo "=== INFORMATIONS SUR LE VPS ==="
        if [[ "${IS_LOCAL_EXECUTION}" == "true" ]]; then
            # ExÃ©cution locale
            echo "SystÃ¨me d'exploitation: $(uname -a 2>/dev/null)"
            echo "Espace disque disponible: $(df -h / | awk 'NR==2 {print $4}' 2>/dev/null)"
            echo "MÃ©moire disponible: $(free -h | awk '/^Mem:/ {print $7}' 2>/dev/null)"
            echo "Charge systÃ¨me: $(uptime 2>/dev/null)"
            echo "Services actifs: $(systemctl list-units --state=running --type=service --no-pager | grep -v systemd | head -10 2>/dev/null)"
        else
            # ExÃ©cution distante
            if ssh -o BatchMode=yes -o ConnectTimeout=5 -p "${ansible_port}" "${ansible_user}@${ansible_host}" "uname -a" &>/dev/null; then
                echo "SystÃ¨me d'exploitation: $(ssh -o BatchMode=yes -o ConnectTimeout=5 -p "${ansible_port}" "${ansible_user}@${ansible_host}" "uname -a" 2>/dev/null)"
                echo "Espace disque disponible: $(ssh -o BatchMode=yes -o ConnectTimeout=5 -p "${ansible_port}" "${ansible_user}@${ansible_host}" "df -h / | awk 'NR==2 {print \$4}'" 2>/dev/null)"
                echo "MÃ©moire disponible: $(ssh -o BatchMode=yes -o ConnectTimeout=5 -p "${ansible_port}" "${ansible_user}@${ansible_host}" "free -h | awk '/^Mem:/ {print \$7}'" 2>/dev/null)"
                echo "Charge systÃ¨me: $(ssh -o BatchMode=yes -o ConnectTimeout=5 -p "${ansible_port}" "${ansible_user}@${ansible_host}" "uptime" 2>/dev/null)"
                echo "Services actifs: $(ssh -o BatchMode=yes -o ConnectTimeout=5 -p "${ansible_port}" "${ansible_user}@${ansible_host}" "systemctl list-units --state=running --type=service --no-pager | grep -v systemd | head -10" 2>/dev/null)"
            else
                echo "Impossible de se connecter au VPS pour rÃ©cupÃ©rer les informations"
            fi
        fi
        echo ""

        echo "=== Ã‰TAT DE KUBERNETES ==="
        if command_exists kubectl && kubectl cluster-info &>/dev/null; then
            echo "Version de Kubernetes: $(kubectl version --short 2>/dev/null)"
            echo "NÅ“uds: $(kubectl get nodes -o wide 2>/dev/null)"
            echo "Pods par namespace: $(kubectl get pods --all-namespaces -o wide 2>/dev/null)"
            echo "Services: $(kubectl get services --all-namespaces 2>/dev/null)"
            echo "Ã‰vÃ©nements rÃ©cents: $(kubectl get events --all-namespaces --sort-by='.lastTimestamp' | tail -n 20 2>/dev/null)"
        else
            echo "Kubernetes n'est pas accessible ou n'est pas installÃ©"
        fi
        echo ""

        echo "=== LOGS PERTINENTS ==="
        echo "DerniÃ¨res lignes du log d'installation:"
        tail -50 "${LOG_FILE}" 2>/dev/null
        echo ""

        echo "=== VÃ‰RIFICATIONS RÃ‰SEAU ==="
        if [[ "${IS_LOCAL_EXECUTION}" == "true" ]]; then
            # ExÃ©cution locale
            echo "ExÃ©cution locale dÃ©tectÃ©e, vÃ©rification de la connectivitÃ© rÃ©seau ignorÃ©e"
            echo "Ports ouverts sur le VPS (vÃ©rification locale):"
            for port in "${REQUIRED_PORTS[@]}"; do
                if ss -tuln | grep -q ":${port} "; then
                    echo "  - Port ${port}: OUVERT"
                else
                    echo "  - Port ${port}: FERMÃ‰"
                fi
            done
        else
            # ExÃ©cution distante
            echo "Ping vers le VPS: $(ping -c 3 "${ansible_host}" 2>&1)"
            echo "Ports ouverts sur le VPS:"
            for port in "${REQUIRED_PORTS[@]}"; do
                if nc -z -w 5 "${ansible_host}" "${port}" &>/dev/null; then
                    echo "  - Port ${port}: OUVERT"
                else
                    echo "  - Port ${port}: FERMÃ‰"
                fi
            done
        fi
        echo ""

        echo "=== RECOMMANDATIONS ==="
        echo "1. VÃ©rifiez la connectivitÃ© rÃ©seau avec le VPS"
        echo "2. Assurez-vous que tous les ports requis sont ouverts"
        echo "3. VÃ©rifiez l'espace disque et la mÃ©moire disponibles"
        echo "4. Consultez les logs pour plus de dÃ©tails sur l'erreur"
        echo "5. Corrigez les problÃ¨mes identifiÃ©s et relancez le script"
        echo "6. Si nÃ©cessaire, utilisez l'option --skip-init pour reprendre aprÃ¨s l'initialisation"
        echo ""

        echo "=== FIN DU RAPPORT ==="
    } > "${report_file}"

    log "SUCCESS" "Rapport de diagnostic gÃ©nÃ©rÃ©: ${report_file}"
    return 0
}

# Fonction de nettoyage
function cleanup() {
    log "INFO" "Nettoyage des ressources temporaires..."

    # Suppression du fichier de verrouillage
    if [[ -f "${LOCK_FILE}" ]]; then
        # Tentative de suppression sans sudo d'abord
        if ! rm -f "${LOCK_FILE}" 2>/dev/null; then
            log "WARNING" "Impossible de supprimer le fichier de verrouillage sans sudo, tentative avec sudo..."
            # Si Ã§a Ã©choue, essayer avec secure_sudo
            if secure_sudo rm -f "${LOCK_FILE}"; then
                log "SUCCESS" "Fichier de verrouillage supprimÃ© avec succÃ¨s (sudo)"
            else
                log "WARNING" "Impossible de supprimer le fichier de verrouillage, mÃªme avec sudo"
            fi
        fi
    fi

    # Affichage des informations de diagnostic
    log "INFO" "Informations de diagnostic:"
    log "INFO" "- DerniÃ¨re Ã©tape: ${INSTALLATION_STEP}"
    log "INFO" "- DerniÃ¨re commande: ${LAST_COMMAND}"
    log "INFO" "- DerniÃ¨re erreur: ${LAST_ERROR}"
    log "INFO" "- Fichier de log: ${LOG_FILE}"

    log "INFO" "Nettoyage terminÃ©"
}

# Configuration du gestionnaire d'erreurs
trap 'handle_error ${LINENO} "${COMMAND_NAME:-unknown}"' ERR

# Configuration du gestionnaire de sortie pour s'assurer que le fichier de verrouillage est toujours supprimÃ©
trap 'if [[ -f "${LOCK_FILE}" ]]; then if ! rm -f "${LOCK_FILE}" 2>/dev/null; then secure_sudo rm -f "${LOCK_FILE}" 2>/dev/null || true; fi; fi' EXIT

# Fonction pour exÃ©cuter des commandes sudo avec demande de mot de passe
function secure_sudo() {
    if [[ "${SUDO_ALWAYS_ASK}" == "true" ]]; then
        sudo -k "$@"  # -k force Ã  demander le mot de passe
    else
        sudo "$@"
    fi
}

# Fonction pour vÃ©rifier si une commande existe
function command_exists() {
    command -v "$1" &> /dev/null
}

# Fonction pour installer les commandes manquantes
function install_missing_commands() {
    local commands=("$@")
    local os_name=$(uname -s)
    local success=true

    log "INFO" "DÃ©tection du systÃ¨me d'exploitation: ${os_name}"

    # DÃ©tection du gestionnaire de paquets
    local pkg_manager=""
    local install_cmd=""

    if [[ "${os_name}" == "Linux" ]]; then
        # DÃ©tection de la distribution Linux
        if command_exists apt-get; then
            pkg_manager="apt"
            install_cmd="apt-get install -y"
        elif command_exists dnf; then
            pkg_manager="dnf"
            install_cmd="dnf install -y"
        elif command_exists yum; then
            pkg_manager="yum"
            install_cmd="yum install -y"
        elif command_exists pacman; then
            pkg_manager="pacman"
            install_cmd="pacman -S --noconfirm"
        elif command_exists zypper; then
            pkg_manager="zypper"
            install_cmd="zypper install -y"
        else
            log "ERROR" "Gestionnaire de paquets non reconnu sur ce systÃ¨me Linux"
            return 1
        fi
    elif [[ "${os_name}" == "Darwin" ]]; then
        # macOS - vÃ©rification de Homebrew
        if command_exists brew; then
            pkg_manager="brew"
            install_cmd="brew install"
        else
            log "ERROR" "Homebrew n'est pas installÃ© sur ce systÃ¨me macOS"
            log "INFO" "Installez Homebrew avec: /bin/bash -c \"\$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\""
            return 1
        fi
    else
        log "ERROR" "SystÃ¨me d'exploitation non supportÃ© pour l'installation automatique: ${os_name}"
        return 1
    fi

    log "INFO" "Utilisation du gestionnaire de paquets: ${pkg_manager}"

    # Mise Ã  jour des dÃ©pÃ´ts si nÃ©cessaire
    if [[ "${pkg_manager}" == "apt" ]]; then
        log "INFO" "Mise Ã  jour des dÃ©pÃ´ts apt..."
        if ! secure_sudo apt-get update &>/dev/null; then
            log "WARNING" "Impossible de mettre Ã  jour les dÃ©pÃ´ts apt"
        fi
    elif [[ "${pkg_manager}" == "dnf" || "${pkg_manager}" == "yum" ]]; then
        log "INFO" "Mise Ã  jour des dÃ©pÃ´ts ${pkg_manager}..."
        if ! secure_sudo ${pkg_manager} check-update &>/dev/null; then
            log "WARNING" "Impossible de mettre Ã  jour les dÃ©pÃ´ts ${pkg_manager}"
        fi
    fi

    # Installation des commandes manquantes
    for cmd in "${commands[@]}"; do
        log "INFO" "Installation de la commande: ${cmd}"

        # Mapping des noms de commandes aux noms de paquets
        local pkg_name=""
        case "${cmd}" in
            "jq")
                pkg_name="jq"
                ;;
            "ansible-playbook")
                if [[ "${pkg_manager}" == "apt" ]]; then
                    pkg_name="ansible"
                elif [[ "${pkg_manager}" == "brew" ]]; then
                    pkg_name="ansible"
                else
                    pkg_name="ansible"
                fi
                ;;
            "kubectl")
                if [[ "${pkg_manager}" == "apt" ]]; then
                    # Pour Debian/Ubuntu, kubectl peut Ãªtre installÃ© de deux faÃ§ons
                    # MÃ©thode 1 (directe): TÃ©lÃ©chargement direct du binaire (plus fiable)
                    log "INFO" "Installation de kubectl via tÃ©lÃ©chargement direct du binaire..."

                    # VÃ©rification de curl
                    if ! command_exists curl; then
                        log "INFO" "Installation de curl..."
                        secure_sudo apt-get install -y curl 2>&1 | tee /tmp/curl_install.log
                        if ! command_exists curl; then
                            log "ERROR" "Ã‰chec de l'installation de curl. Voir /tmp/curl_install.log pour plus de dÃ©tails."
                            return 1
                        fi
                    fi

                    # TÃ©lÃ©chargement du binaire kubectl
                    local kubectl_version="v1.28.4"  # Version mise Ã  jour
                    local arch=$(uname -m)
                    local kubectl_arch="amd64"

                    if [[ "${arch}" == "aarch64" || "${arch}" == "arm64" ]]; then
                        kubectl_arch="arm64"
                    elif [[ "${arch}" == "armv7l" ]]; then
                        kubectl_arch="arm"
                    fi

                    log "INFO" "TÃ©lÃ©chargement de kubectl ${kubectl_version} pour ${kubectl_arch}..."
                    if curl -LO "https://dl.k8s.io/release/${kubectl_version}/bin/linux/${kubectl_arch}/kubectl" 2>/tmp/kubectl_download.log; then
                        log "SUCCESS" "TÃ©lÃ©chargement de kubectl rÃ©ussi"
                        secure_sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl
                        if [[ $? -eq 0 ]]; then
                            log "SUCCESS" "Installation de kubectl rÃ©ussie via tÃ©lÃ©chargement direct"
                            rm -f kubectl
                            return 0
                        else
                            log "ERROR" "Ã‰chec de l'installation de kubectl dans /usr/local/bin"
                            log "INFO" "Tentative avec mÃ©thode alternative..."
                            rm -f kubectl
                        fi
                    else
                        log "ERROR" "Ã‰chec du tÃ©lÃ©chargement de kubectl. Voir /tmp/kubectl_download.log pour plus de dÃ©tails."
                        log "INFO" "Tentative avec mÃ©thode alternative..."
                    fi

                    # MÃ©thode 2 (fallback): Utilisation du dÃ©pÃ´t Kubernetes
                    log "INFO" "Tentative d'installation via le dÃ©pÃ´t Kubernetes..."

                    # Ajout de la clÃ© GPG
                    curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.28/deb/Release.key | secure_sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg 2>/tmp/kubectl_key.log
                    if [[ $? -ne 0 ]]; then
                        log "WARNING" "ProblÃ¨me lors de l'ajout de la clÃ© Kubernetes. Voir /tmp/kubectl_key.log pour plus de dÃ©tails."
                        # CrÃ©ation du rÃ©pertoire si nÃ©cessaire
                        secure_sudo mkdir -p /etc/apt/keyrings
                        # Nouvelle tentative
                        curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.28/deb/Release.key | secure_sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg 2>/tmp/kubectl_key_retry.log
                        if [[ $? -ne 0 ]]; then
                            log "ERROR" "Ã‰chec de l'ajout de la clÃ© Kubernetes mÃªme aprÃ¨s nouvelle tentative."
                        fi
                    fi

                    # Ajout du dÃ©pÃ´t (nouvelle URL)
                    echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.28/deb/ /" | secure_sudo tee /etc/apt/sources.list.d/kubernetes.list >/dev/null

                    log "INFO" "Mise Ã  jour des dÃ©pÃ´ts apt..."
                    secure_sudo apt-get update 2>/tmp/apt_update.log

                    pkg_name="kubectl"
                elif [[ "${pkg_manager}" == "brew" ]]; then
                    pkg_name="kubernetes-cli"
                else
                    pkg_name="kubectl"
                fi
                ;;
            "helm")
                if [[ "${pkg_manager}" == "apt" ]]; then
                    # Pour Debian/Ubuntu, helm nÃ©cessite un dÃ©pÃ´t spÃ©cial
                    log "INFO" "Configuration du dÃ©pÃ´t Helm pour apt..."
                    if ! command_exists curl; then
                        secure_sudo apt-get install -y curl &>/dev/null
                    fi
                    curl https://baltocdn.com/helm/signing.asc | secure_sudo apt-key add - &>/dev/null
                    echo "deb https://baltocdn.com/helm/stable/debian/ all main" | secure_sudo tee /etc/apt/sources.list.d/helm-stable-debian.list &>/dev/null
                    secure_sudo apt-get update &>/dev/null
                    pkg_name="helm"
                elif [[ "${pkg_manager}" == "brew" ]]; then
                    pkg_name="helm"
                else
                    pkg_name="helm"
                fi
                ;;
            "timeout")
                if [[ "${pkg_manager}" == "apt" ]]; then
                    pkg_name="coreutils"
                elif [[ "${pkg_manager}" == "brew" ]]; then
                    pkg_name="coreutils"
                else
                    pkg_name="coreutils"
                fi
                ;;
            "nc")
                if [[ "${pkg_manager}" == "apt" ]]; then
                    pkg_name="netcat"
                elif [[ "${pkg_manager}" == "brew" ]]; then
                    pkg_name="netcat"
                else
                    pkg_name="netcat"
                fi
                ;;
            "ping")
                if [[ "${pkg_manager}" == "apt" ]]; then
                    pkg_name="iputils-ping"
                elif [[ "${pkg_manager}" == "brew" ]]; then
                    pkg_name="inetutils"
                else
                    pkg_name="iputils"
                fi
                ;;
            "ssh"|"scp")
                if [[ "${pkg_manager}" == "apt" ]]; then
                    pkg_name="openssh-client"
                elif [[ "${pkg_manager}" == "brew" ]]; then
                    pkg_name="openssh"
                else
                    pkg_name="openssh"
                fi
                ;;
            *)
                # Si la commande n'est pas dans notre mapping, on utilise le mÃªme nom
                pkg_name="${cmd}"
                ;;
        esac

        # Installation du paquet
        log "INFO" "Installation du paquet: ${pkg_name}"
        local install_log="/tmp/${pkg_name}_install.log"
        if ! secure_sudo ${install_cmd} ${pkg_name} 2>&1 | tee "${install_log}"; then
            log "ERROR" "Ã‰chec de l'installation de ${pkg_name}. Voir ${install_log} pour plus de dÃ©tails."

            # Tentative alternative pour kubectl si l'installation via apt a Ã©chouÃ©
            if [[ "${cmd}" == "kubectl" && "${pkg_manager}" == "apt" ]]; then
                log "INFO" "Tentative d'installation alternative de kubectl via tÃ©lÃ©chargement direct..."

                # VÃ©rification de curl
                if ! command_exists curl; then
                    log "INFO" "Installation de curl..."
                    secure_sudo apt-get install -y curl 2>&1 | tee /tmp/curl_install_fallback.log
                    if ! command_exists curl; then
                        log "ERROR" "Ã‰chec de l'installation de curl. Voir /tmp/curl_install_fallback.log pour plus de dÃ©tails."
                        continue
                    fi
                fi

                local kubectl_version="v1.28.4"  # Version mise Ã  jour
                local arch=$(uname -m)
                local kubectl_arch="amd64"

                if [[ "${arch}" == "aarch64" || "${arch}" == "arm64" ]]; then
                    kubectl_arch="arm64"
                elif [[ "${arch}" == "armv7l" ]]; then
                    kubectl_arch="arm"
                fi

                log "INFO" "TÃ©lÃ©chargement de kubectl ${kubectl_version} pour ${kubectl_arch}..."
                if curl -LO "https://dl.k8s.io/release/${kubectl_version}/bin/linux/${kubectl_arch}/kubectl" 2>/tmp/kubectl_download_fallback.log; then
                    log "SUCCESS" "TÃ©lÃ©chargement de kubectl rÃ©ussi"
                    secure_sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl
                    if [[ $? -eq 0 ]]; then
                        log "SUCCESS" "Installation de kubectl rÃ©ussie via tÃ©lÃ©chargement direct (fallback)"
                        rm -f kubectl
                        continue  # Skip the success=false and continue with the next command
                    else
                        log "ERROR" "Ã‰chec de l'installation de kubectl dans /usr/local/bin"
                        log "ERROR" "VÃ©rifiez les permissions et l'espace disque"
                        rm -f kubectl
                    fi
                else
                    log "ERROR" "Ã‰chec du tÃ©lÃ©chargement de kubectl. Voir /tmp/kubectl_download_fallback.log pour plus de dÃ©tails."
                    log "ERROR" "VÃ©rifiez votre connexion Internet et les paramÃ¨tres proxy"
                fi
            fi

            success=false
        else
            log "SUCCESS" "Installation de ${pkg_name} rÃ©ussie"
            # VÃ©rification que la commande est maintenant disponible
            if ! command_exists "${cmd}"; then
                log "WARNING" "La commande ${cmd} n'est toujours pas disponible aprÃ¨s l'installation"
                success=false
            fi
        fi
    done

    return $( [[ "${success}" == "true" ]] && echo 0 || echo 1 )
}

# Fonction pour mettre Ã  jour les commandes obsolÃ¨tes
function update_outdated_commands() {
    local commands=("$@")
    local success=true

    for cmd_info in "${commands[@]}"; do
        # Extraire le nom de la commande et la version requise
        local cmd=$(echo "${cmd_info}" | cut -d' ' -f1)
        local required_version=$(echo "${cmd_info}" | grep -o "requise: [0-9.]*" | cut -d' ' -f2)

        log "INFO" "Tentative de mise Ã  jour de la commande: ${cmd} vers la version ${required_version}"

        case "${cmd}" in
            "ansible-playbook")
                if update_ansible; then
                    log "SUCCESS" "Mise Ã  jour d'ansible-playbook rÃ©ussie"
                else
                    log "ERROR" "Ã‰chec de la mise Ã  jour d'ansible-playbook"
                    success=false
                fi
                ;;
            "kubectl")
                log "INFO" "Mise Ã  jour de kubectl..."
                if command_exists apt-get; then
                    # Pour Debian/Ubuntu, utiliser le dÃ©pÃ´t Kubernetes
                    if ! secure_sudo apt-get update &>/dev/null; then
                        log "WARNING" "Impossible de mettre Ã  jour les dÃ©pÃ´ts apt"
                    fi
                    if ! secure_sudo apt-get install -y kubectl &>/dev/null; then
                        log "ERROR" "Ã‰chec de la mise Ã  jour de kubectl via apt"
                        success=false
                    fi
                elif command_exists brew; then
                    # Pour macOS, utiliser Homebrew
                    if ! brew upgrade kubernetes-cli &>/dev/null; then
                        log "ERROR" "Ã‰chec de la mise Ã  jour de kubectl via Homebrew"
                        success=false
                    fi
                else
                    # TÃ©lÃ©chargement direct du binaire
                    local kubectl_version="v${required_version}"
                    local arch=$(uname -m)
                    local kubectl_arch="amd64"

                    if [[ "${arch}" == "aarch64" || "${arch}" == "arm64" ]]; then
                        kubectl_arch="arm64"
                    elif [[ "${arch}" == "armv7l" ]]; then
                        kubectl_arch="arm"
                    fi

                    log "INFO" "TÃ©lÃ©chargement de kubectl ${kubectl_version} pour ${kubectl_arch}..."
                    if curl -LO "https://dl.k8s.io/release/${kubectl_version}/bin/linux/${kubectl_arch}/kubectl" 2>/tmp/kubectl_update.log; then
                        log "SUCCESS" "TÃ©lÃ©chargement de kubectl rÃ©ussi"
                        secure_sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl
                        if [[ $? -eq 0 ]]; then
                            log "SUCCESS" "Installation de kubectl rÃ©ussie via tÃ©lÃ©chargement direct"
                            rm -f kubectl
                        else
                            log "ERROR" "Ã‰chec de l'installation de kubectl dans /usr/local/bin"
                            rm -f kubectl
                            success=false
                        fi
                    else
                        log "ERROR" "Ã‰chec du tÃ©lÃ©chargement de kubectl. Voir /tmp/kubectl_update.log pour plus de dÃ©tails."
                        success=false
                    fi
                fi
                ;;
            "helm")
                log "INFO" "Mise Ã  jour de Helm..."
                if command_exists apt-get; then
                    # Pour Debian/Ubuntu, utiliser le dÃ©pÃ´t Helm
                    if ! secure_sudo apt-get update &>/dev/null; then
                        log "WARNING" "Impossible de mettre Ã  jour les dÃ©pÃ´ts apt"
                    fi
                    if ! secure_sudo apt-get install -y helm &>/dev/null; then
                        log "ERROR" "Ã‰chec de la mise Ã  jour de Helm via apt"
                        success=false
                    fi
                elif command_exists brew; then
                    # Pour macOS, utiliser Homebrew
                    if ! brew upgrade helm &>/dev/null; then
                        log "ERROR" "Ã‰chec de la mise Ã  jour de Helm via Homebrew"
                        success=false
                    fi
                else
                    # TÃ©lÃ©chargement et installation via le script officiel
                    log "INFO" "TÃ©lÃ©chargement du script d'installation de Helm..."
                    if curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 2>/tmp/helm_download.log; then
                        log "SUCCESS" "TÃ©lÃ©chargement du script d'installation de Helm rÃ©ussi"
                        chmod 700 get_helm.sh
                        if ! ./get_helm.sh &>/tmp/helm_install.log; then
                            log "ERROR" "Ã‰chec de l'installation de Helm. Voir /tmp/helm_install.log pour plus de dÃ©tails."
                            success=false
                        fi
                        rm -f get_helm.sh
                    else
                        log "ERROR" "Ã‰chec du tÃ©lÃ©chargement du script d'installation de Helm. Voir /tmp/helm_download.log pour plus de dÃ©tails."
                        success=false
                    fi
                fi
                ;;
            *)
                log "WARNING" "Mise Ã  jour automatique non supportÃ©e pour la commande: ${cmd}"
                log "INFO" "Veuillez mettre Ã  jour cette commande manuellement"
                success=false
                ;;
        esac
    done

    return $( [[ "${success}" == "true" ]] && echo 0 || echo 1 )
}

# Fonction pour mettre Ã  jour Ansible
function update_ansible() {
    log "INFO" "Mise Ã  jour d'Ansible..."

    # DÃ©tection du systÃ¨me d'exploitation
    local os_name
    os_name=$(uname -s)

    # DÃ©tection du gestionnaire de paquets et mise Ã  jour d'Ansible
    if [[ "${os_name}" == "Linux" ]]; then
        # DÃ©tection de la distribution Linux
        if command_exists apt-get; then
            log "INFO" "SystÃ¨me Debian/Ubuntu dÃ©tectÃ©, utilisation de apt-get"

            # CrÃ©ation des fichiers de log pour capturer les erreurs
            local apt_update_log="/tmp/apt_update_ansible.log"
            local apt_install_log="/tmp/apt_install_ansible.log"

            # MÃ©thode 1: Mise Ã  jour via apt standard
            log "INFO" "Tentative de mise Ã  jour via apt standard..."
            if ! secure_sudo apt-get update 2>&1 | tee "${apt_update_log}"; then
                log "WARNING" "Ã‰chec de la mise Ã  jour des dÃ©pÃ´ts. Voir ${apt_update_log} pour plus de dÃ©tails."
                log "INFO" "Tentative avec mÃ©thode alternative..."
            else
                if ! secure_sudo apt-get install -y ansible 2>&1 | tee "${apt_install_log}"; then
                    log "WARNING" "Ã‰chec de l'installation d'Ansible via apt. Voir ${apt_install_log} pour plus de dÃ©tails."
                    log "INFO" "Tentative avec mÃ©thode alternative..."
                else
                    log "SUCCESS" "Installation d'Ansible rÃ©ussie via apt standard"
                    return 0
                fi
            fi

            # MÃ©thode 2: Ajout du PPA Ansible
            log "INFO" "Tentative d'ajout du PPA Ansible..."
            if ! command_exists add-apt-repository; then
                log "INFO" "Installation de software-properties-common pour add-apt-repository..."
                secure_sudo apt-get install -y software-properties-common 2>&1 | tee /tmp/apt_install_properties.log
            fi

            if secure_sudo add-apt-repository --yes --update ppa:ansible/ansible 2>&1 | tee /tmp/add_ansible_ppa.log; then
                log "INFO" "PPA Ansible ajoutÃ© avec succÃ¨s, installation d'Ansible..."
                if secure_sudo apt-get install -y ansible 2>&1 | tee "${apt_install_log}"; then
                    log "SUCCESS" "Installation d'Ansible rÃ©ussie via PPA"
                    return 0
                else
                    log "WARNING" "Ã‰chec de l'installation d'Ansible via PPA. Voir ${apt_install_log} pour plus de dÃ©tails."
                fi
            else
                log "WARNING" "Ã‰chec de l'ajout du PPA Ansible. Voir /tmp/add_ansible_ppa.log pour plus de dÃ©tails."
            fi

            # MÃ©thode 3: Installation via pip
            log "INFO" "Tentative d'installation via pip..."
            if ! command_exists pip3; then
                log "INFO" "Installation de pip3..."
                secure_sudo apt-get install -y python3-pip 2>&1 | tee /tmp/apt_install_pip.log
            fi

            if command_exists pip3; then
                log "INFO" "Installation d'Ansible via pip3..."
                if secure_sudo pip3 install --upgrade ansible 2>&1 | tee /tmp/pip_install_ansible.log; then
                    log "SUCCESS" "Installation d'Ansible rÃ©ussie via pip3"
                    return 0
                else
                    log "WARNING" "Ã‰chec de l'installation d'Ansible via pip3. Voir /tmp/pip_install_ansible.log pour plus de dÃ©tails."
                fi
            else
                log "WARNING" "pip3 n'est pas disponible, impossible d'installer Ansible via pip"
            fi

            log "ERROR" "Toutes les mÃ©thodes d'installation d'Ansible ont Ã©chouÃ©"
            return 1
        elif command_exists dnf; then
            log "INFO" "SystÃ¨me Fedora dÃ©tectÃ©, utilisation de dnf"

            # CrÃ©ation des fichiers de log pour capturer les erreurs
            local dnf_update_log="/tmp/dnf_update_ansible.log"

            # MÃ©thode 1: Mise Ã  jour via dnf standard
            log "INFO" "Tentative de mise Ã  jour via dnf standard..."
            if ! secure_sudo dnf update -y ansible 2>&1 | tee "${dnf_update_log}"; then
                log "WARNING" "Ã‰chec de la mise Ã  jour d'Ansible via dnf. Voir ${dnf_update_log} pour plus de dÃ©tails."

                # MÃ©thode 2: Installation via pip
                log "INFO" "Tentative d'installation via pip..."
                if ! command_exists pip3; then
                    log "INFO" "Installation de pip3..."
                    secure_sudo dnf install -y python3-pip 2>&1 | tee /tmp/dnf_install_pip.log
                fi

                if command_exists pip3; then
                    log "INFO" "Installation d'Ansible via pip3..."
                    if secure_sudo pip3 install --upgrade ansible 2>&1 | tee /tmp/pip_install_ansible.log; then
                        log "SUCCESS" "Installation d'Ansible rÃ©ussie via pip3"
                        return 0
                    else
                        log "WARNING" "Ã‰chec de l'installation d'Ansible via pip3. Voir /tmp/pip_install_ansible.log pour plus de dÃ©tails."
                        log "ERROR" "Toutes les mÃ©thodes d'installation d'Ansible ont Ã©chouÃ©"
                        return 1
                    fi
                else
                    log "WARNING" "pip3 n'est pas disponible, impossible d'installer Ansible via pip"
                    log "ERROR" "Toutes les mÃ©thodes d'installation d'Ansible ont Ã©chouÃ©"
                    return 1
                fi
            else
                log "SUCCESS" "Mise Ã  jour d'Ansible rÃ©ussie via dnf"
                return 0
            fi
        elif command_exists yum; then
            log "INFO" "SystÃ¨me CentOS/RHEL dÃ©tectÃ©, utilisation de yum"

            # CrÃ©ation des fichiers de log pour capturer les erreurs
            local yum_update_log="/tmp/yum_update_ansible.log"

            # MÃ©thode 1: Mise Ã  jour via yum standard
            log "INFO" "Tentative de mise Ã  jour via yum standard..."
            if ! secure_sudo yum update -y ansible 2>&1 | tee "${yum_update_log}"; then
                log "WARNING" "Ã‰chec de la mise Ã  jour d'Ansible via yum. Voir ${yum_update_log} pour plus de dÃ©tails."

                # MÃ©thode 2: Installation via EPEL
                log "INFO" "Tentative d'installation via EPEL..."
                if secure_sudo yum install -y epel-release 2>&1 | tee /tmp/yum_install_epel.log; then
                    log "INFO" "DÃ©pÃ´t EPEL installÃ©, tentative d'installation d'Ansible..."
                    if secure_sudo yum install -y ansible 2>&1 | tee /tmp/yum_install_ansible.log; then
                        log "SUCCESS" "Installation d'Ansible rÃ©ussie via EPEL"
                        return 0
                    else
                        log "WARNING" "Ã‰chec de l'installation d'Ansible via EPEL. Voir /tmp/yum_install_ansible.log pour plus de dÃ©tails."
                    fi
                else
                    log "WARNING" "Ã‰chec de l'installation du dÃ©pÃ´t EPEL. Voir /tmp/yum_install_epel.log pour plus de dÃ©tails."
                fi

                # MÃ©thode 3: Installation via pip
                log "INFO" "Tentative d'installation via pip..."
                if ! command_exists pip3; then
                    log "INFO" "Installation de pip3..."
                    secure_sudo yum install -y python3-pip 2>&1 | tee /tmp/yum_install_pip.log
                fi

                if command_exists pip3; then
                    log "INFO" "Installation d'Ansible via pip3..."
                    if secure_sudo pip3 install --upgrade ansible 2>&1 | tee /tmp/pip_install_ansible.log; then
                        log "SUCCESS" "Installation d'Ansible rÃ©ussie via pip3"
                        return 0
                    else
                        log "WARNING" "Ã‰chec de l'installation d'Ansible via pip3. Voir /tmp/pip_install_ansible.log pour plus de dÃ©tails."
                        log "ERROR" "Toutes les mÃ©thodes d'installation d'Ansible ont Ã©chouÃ©"
                        return 1
                    fi
                else
                    log "WARNING" "pip3 n'est pas disponible, impossible d'installer Ansible via pip"
                    log "ERROR" "Toutes les mÃ©thodes d'installation d'Ansible ont Ã©chouÃ©"
                    return 1
                fi
            else
                log "SUCCESS" "Mise Ã  jour d'Ansible rÃ©ussie via yum"
                return 0
            fi
        elif command_exists pacman; then
            log "INFO" "SystÃ¨me Arch Linux dÃ©tectÃ©, utilisation de pacman"

            # CrÃ©ation des fichiers de log pour capturer les erreurs
            local pacman_update_log="/tmp/pacman_update_ansible.log"

            # MÃ©thode 1: Mise Ã  jour via pacman standard
            log "INFO" "Tentative de mise Ã  jour via pacman standard..."
            if ! secure_sudo pacman -Syu --noconfirm ansible 2>&1 | tee "${pacman_update_log}"; then
                log "WARNING" "Ã‰chec de la mise Ã  jour d'Ansible via pacman. Voir ${pacman_update_log} pour plus de dÃ©tails."

                # MÃ©thode 2: Installation via pip
                log "INFO" "Tentative d'installation via pip..."
                if ! command_exists pip3; then
                    log "INFO" "Installation de pip3..."
                    secure_sudo pacman -S --noconfirm python-pip 2>&1 | tee /tmp/pacman_install_pip.log
                fi

                if command_exists pip3; then
                    log "INFO" "Installation d'Ansible via pip3..."
                    if secure_sudo pip3 install --upgrade ansible 2>&1 | tee /tmp/pip_install_ansible.log; then
                        log "SUCCESS" "Installation d'Ansible rÃ©ussie via pip3"
                        return 0
                    else
                        log "WARNING" "Ã‰chec de l'installation d'Ansible via pip3. Voir /tmp/pip_install_ansible.log pour plus de dÃ©tails."
                        log "ERROR" "Toutes les mÃ©thodes d'installation d'Ansible ont Ã©chouÃ©"
                        return 1
                    fi
                else
                    log "WARNING" "pip3 n'est pas disponible, impossible d'installer Ansible via pip"
                    log "ERROR" "Toutes les mÃ©thodes d'installation d'Ansible ont Ã©chouÃ©"
                    return 1
                fi
            else
                log "SUCCESS" "Mise Ã  jour d'Ansible rÃ©ussie via pacman"
                return 0
            fi
        elif command_exists zypper; then
            log "INFO" "SystÃ¨me openSUSE dÃ©tectÃ©, utilisation de zypper"

            # CrÃ©ation des fichiers de log pour capturer les erreurs
            local zypper_update_log="/tmp/zypper_update_ansible.log"

            # MÃ©thode 1: Mise Ã  jour via zypper standard
            log "INFO" "Tentative de mise Ã  jour via zypper standard..."
            if ! secure_sudo zypper update -y ansible 2>&1 | tee "${zypper_update_log}"; then
                log "WARNING" "Ã‰chec de la mise Ã  jour d'Ansible via zypper. Voir ${zypper_update_log} pour plus de dÃ©tails."

                # MÃ©thode 2: Installation via pip
                log "INFO" "Tentative d'installation via pip..."
                if ! command_exists pip3; then
                    log "INFO" "Installation de pip3..."
                    secure_sudo zypper install -y python3-pip 2>&1 | tee /tmp/zypper_install_pip.log
                fi

                if command_exists pip3; then
                    log "INFO" "Installation d'Ansible via pip3..."
                    if secure_sudo pip3 install --upgrade ansible 2>&1 | tee /tmp/pip_install_ansible.log; then
                        log "SUCCESS" "Installation d'Ansible rÃ©ussie via pip3"
                        return 0
                    else
                        log "WARNING" "Ã‰chec de l'installation d'Ansible via pip3. Voir /tmp/pip_install_ansible.log pour plus de dÃ©tails."
                        log "ERROR" "Toutes les mÃ©thodes d'installation d'Ansible ont Ã©chouÃ©"
                        return 1
                    fi
                else
                    log "WARNING" "pip3 n'est pas disponible, impossible d'installer Ansible via pip"
                    log "ERROR" "Toutes les mÃ©thodes d'installation d'Ansible ont Ã©chouÃ©"
                    return 1
                fi
            else
                log "SUCCESS" "Mise Ã  jour d'Ansible rÃ©ussie via zypper"
                return 0
            fi
        else
            log "ERROR" "Gestionnaire de paquets non supportÃ© sur ce systÃ¨me Linux"
            log "INFO" "Veuillez mettre Ã  jour Ansible manuellement"
            return 1
        fi
    elif [[ "${os_name}" == "Darwin" ]]; then
        # macOS
        if command_exists brew; then
            log "INFO" "SystÃ¨me macOS dÃ©tectÃ©, utilisation de Homebrew"

            # CrÃ©ation des fichiers de log pour capturer les erreurs
            local brew_update_log="/tmp/brew_update.log"
            local brew_upgrade_log="/tmp/brew_upgrade_ansible.log"

            # MÃ©thode 1: Mise Ã  jour via Homebrew standard
            log "INFO" "Tentative de mise Ã  jour via Homebrew standard..."
            if ! brew update 2>&1 | tee "${brew_update_log}"; then
                log "WARNING" "Ã‰chec de la mise Ã  jour des dÃ©pÃ´ts Homebrew. Voir ${brew_update_log} pour plus de dÃ©tails."
                log "INFO" "Tentative de mise Ã  jour d'Ansible sans mettre Ã  jour Homebrew..."
            fi

            if ! brew upgrade ansible 2>&1 | tee "${brew_upgrade_log}"; then
                log "WARNING" "Ã‰chec de la mise Ã  jour d'Ansible via Homebrew. Voir ${brew_upgrade_log} pour plus de dÃ©tails."

                # MÃ©thode 2: Installation via pip
                log "INFO" "Tentative d'installation via pip..."
                if ! command_exists pip3; then
                    log "INFO" "Installation de pip3..."
                    brew install python3 2>&1 | tee /tmp/brew_install_python.log
                fi

                if command_exists pip3; then
                    log "INFO" "Installation d'Ansible via pip3..."
                    if pip3 install --user --upgrade ansible 2>&1 | tee /tmp/pip_install_ansible.log; then
                        log "SUCCESS" "Installation d'Ansible rÃ©ussie via pip3"
                        return 0
                    else
                        log "WARNING" "Ã‰chec de l'installation d'Ansible via pip3. Voir /tmp/pip_install_ansible.log pour plus de dÃ©tails."
                        log "ERROR" "Toutes les mÃ©thodes d'installation d'Ansible ont Ã©chouÃ©"
                        return 1
                    fi
                else
                    log "WARNING" "pip3 n'est pas disponible, impossible d'installer Ansible via pip"
                    log "ERROR" "Toutes les mÃ©thodes d'installation d'Ansible ont Ã©chouÃ©"
                    return 1
                fi
            else
                log "SUCCESS" "Mise Ã  jour d'Ansible rÃ©ussie via Homebrew"
                return 0
            fi
        else
            log "WARNING" "Homebrew n'est pas installÃ© sur ce systÃ¨me macOS"
            log "INFO" "Tentative d'installation via pip..."

            # MÃ©thode alternative: Installation via pip si Homebrew n'est pas disponible
            if ! command_exists pip3; then
                log "WARNING" "pip3 n'est pas disponible et Homebrew n'est pas installÃ©"
                log "INFO" "Installez Homebrew avec: /bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\""
                log "INFO" "Ou installez Python et pip manuellement"
                return 1
            fi

            log "INFO" "Installation d'Ansible via pip3..."
            if pip3 install --user --upgrade ansible 2>&1 | tee /tmp/pip_install_ansible.log; then
                log "SUCCESS" "Installation d'Ansible rÃ©ussie via pip3"
                return 0
            else
                log "WARNING" "Ã‰chec de l'installation d'Ansible via pip3. Voir /tmp/pip_install_ansible.log pour plus de dÃ©tails."
                log "ERROR" "Toutes les mÃ©thodes d'installation d'Ansible ont Ã©chouÃ©"
                log "INFO" "Installez Homebrew avec: /bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\""
                return 1
            fi
        fi
    elif [[ "${os_name}" == *"MINGW"* || "${os_name}" == *"MSYS"* || "${os_name}" == *"CYGWIN"* || "${os_name}" == *"Windows"* ]]; then
        # Windows (Git Bash, MSYS2, Cygwin, WSL)
        log "WARNING" "Mise Ã  jour automatique d'Ansible limitÃ©e sur Windows"

        # DÃ©tection de WSL
        if [[ "${os_name}" == *"Linux"* && ("$(uname -r)" == *"WSL"* || "$(uname -r)" == *"Microsoft"* || "$(uname -r)" == *"microsoft"*) ]]; then
            log "INFO" "Environnement WSL dÃ©tectÃ©, tentative d'installation via pip..."

            # MÃ©thode 1: Installation via pip dans WSL
            if ! command_exists pip3; then
                log "INFO" "Installation de pip3..."
                if command_exists apt-get; then
                    secure_sudo apt-get update && secure_sudo apt-get install -y python3-pip 2>&1 | tee /tmp/apt_install_pip_wsl.log
                elif command_exists dnf; then
                    secure_sudo dnf install -y python3-pip 2>&1 | tee /tmp/dnf_install_pip_wsl.log
                elif command_exists yum; then
                    secure_sudo yum install -y python3-pip 2>&1 | tee /tmp/yum_install_pip_wsl.log
                else
                    log "WARNING" "Impossible d'installer pip3 automatiquement dans WSL"
                    log "INFO" "Veuillez installer pip3 manuellement avec la commande appropriÃ©e pour votre distribution"
                    return 1
                fi
            fi

            if command_exists pip3; then
                log "INFO" "Installation d'Ansible via pip3 dans WSL..."
                if pip3 install --user --upgrade ansible 2>&1 | tee /tmp/pip_install_ansible_wsl.log; then
                    log "SUCCESS" "Installation d'Ansible rÃ©ussie via pip3 dans WSL"
                    return 0
                else
                    log "WARNING" "Ã‰chec de l'installation d'Ansible via pip3 dans WSL. Voir /tmp/pip_install_ansible_wsl.log pour plus de dÃ©tails."
                fi
            fi
        fi

        # Instructions dÃ©taillÃ©es pour Windows
        log "INFO" "Pour installer ou mettre Ã  jour Ansible sur Windows, suivez ces Ã©tapes:"
        log "INFO" "1. Installez WSL (Windows Subsystem for Linux) si ce n'est pas dÃ©jÃ  fait:"
        log "INFO" "   - Ouvrez PowerShell en tant qu'administrateur et exÃ©cutez: wsl --install"
        log "INFO" "2. Installez une distribution Linux via le Microsoft Store (Ubuntu recommandÃ©)"
        log "INFO" "3. Dans votre distribution Linux, exÃ©cutez:"
        log "INFO" "   - sudo apt update && sudo apt install -y ansible"
        log "INFO" "4. Ou utilisez pip3:"
        log "INFO" "   - sudo apt install -y python3-pip && pip3 install --user ansible"
        log "INFO" "5. Alternative: Utilisez Ansible via Docker:"
        log "INFO" "   - docker run --rm -it -v ${PWD}:/work -w /work cytopia/ansible ansible --version"
        return 1
    else
        log "WARNING" "SystÃ¨me d'exploitation non reconnu pour la mise Ã  jour automatique: ${os_name}"

        # Tentative gÃ©nÃ©rique via pip
        log "INFO" "Tentative d'installation gÃ©nÃ©rique via pip..."
        if command_exists pip3; then
            log "INFO" "Installation d'Ansible via pip3..."
            if pip3 install --user --upgrade ansible 2>&1 | tee /tmp/pip_install_ansible_generic.log; then
                log "SUCCESS" "Installation d'Ansible rÃ©ussie via pip3"
                return 0
            else
                log "WARNING" "Ã‰chec de l'installation d'Ansible via pip3. Voir /tmp/pip_install_ansible_generic.log pour plus de dÃ©tails."
            fi
        fi

        log "ERROR" "Impossible d'installer Ansible automatiquement sur ce systÃ¨me"
        log "INFO" "Veuillez installer Ansible manuellement selon les instructions pour votre systÃ¨me d'exploitation"
        return 1
    fi

    # VÃ©rification de la mise Ã  jour
    local new_ansible_version
    local new_ansible_version_raw
    new_ansible_version_raw=$(ansible --version | head -n1)
    # Extraction du numÃ©ro de version, qu'il soit au format "2.13.13" ou "[core 2.15.0]"
    if [[ "${new_ansible_version_raw}" =~ \[core[[:space:]]+([0-9]+\.[0-9]+\.[0-9]+) ]]; then
        # Nouveau format: [core X.Y.Z]
        new_ansible_version="${BASH_REMATCH[1]}"
    else
        # Ancien format: juste le numÃ©ro de version
        new_ansible_version=$(echo "${new_ansible_version_raw}" | awk '{print $2}')
    fi

    log "INFO" "Nouvelle version d'Ansible: ${new_ansible_version}"

    if version_greater_equal "${new_ansible_version}" "2.14.0"; then
        log "SUCCESS" "Ansible a Ã©tÃ© mis Ã  jour avec succÃ¨s vers une version compatible: ${new_ansible_version}"
        return 0
    else
        log "WARNING" "La version d'Ansible aprÃ¨s mise Ã  jour est toujours potentiellement incompatible: ${new_ansible_version}"
        log "WARNING" "Vous pouvez installer des versions spÃ©cifiques des collections ou mettre Ã  jour Ansible manuellement"
        return 1
    fi
}

function check_ansible_version() {
    log "INFO" "VÃ©rification de la version d'Ansible..."

    # VÃ©rification de l'installation d'Ansible
    if ! command_exists ansible; then
        log "ERROR" "La commande ansible n'est pas disponible"
        log "ERROR" "Assurez-vous qu'Ansible est correctement installÃ©"
        return 1
    fi

    # RÃ©cupÃ©ration de la version d'Ansible
    local ansible_version
    local ansible_version_raw
    ansible_version_raw=$(ansible --version | head -n1)
    # Extraction du numÃ©ro de version, qu'il soit au format "2.13.13" ou "[core 2.15.0]"
    if [[ "${ansible_version_raw}" =~ \[core[[:space:]]+([0-9]+\.[0-9]+\.[0-9]+) ]]; then
        # Nouveau format: [core X.Y.Z]
        ansible_version="${BASH_REMATCH[1]}"
    else
        # Ancien format: juste le numÃ©ro de version
        ansible_version=$(echo "${ansible_version_raw}" | awk '{print $2}')
    fi
    log "INFO" "Version d'Ansible dÃ©tectÃ©e: ${ansible_version}"

    # VÃ©rification de la compatibilitÃ©
    if version_greater_equal "${ansible_version}" "2.14.0"; then
        log "SUCCESS" "Version d'Ansible compatible: ${ansible_version}"
        return 0
    else
        log "WARNING" "Version d'Ansible potentiellement incompatible: ${ansible_version}"
        log "WARNING" "Certaines collections peuvent nÃ©cessiter des versions spÃ©cifiques"

        # Demander Ã  l'utilisateur s'il souhaite installer des versions spÃ©cifiques ou mettre Ã  jour Ansible
        local response
        read -p "Souhaitez-vous installer des versions spÃ©cifiques des collections compatibles avec Ansible ${ansible_version}? (o/N): " response

        if [[ "${response}" =~ ^[oO]$ ]]; then
            log "INFO" "Installation de versions spÃ©cifiques des collections..."
            return 2  # Code spÃ©cial pour indiquer l'installation de versions spÃ©cifiques
        else
            log "INFO" "Tentative de mise Ã  jour d'Ansible..."
            if update_ansible; then
                log "SUCCESS" "Ansible a Ã©tÃ© mis Ã  jour avec succÃ¨s"
                return 0
            else
                log "WARNING" "Impossible de mettre Ã  jour Ansible automatiquement"
                log "INFO" "Continuation avec les versions par dÃ©faut des collections"
                return 0
            fi
        fi
    fi
}

# Fonction pour vÃ©rifier et installer les collections Ansible requises
function check_ansible_collections() {
    log "INFO" "VÃ©rification des collections Ansible requises..."

    # VÃ©rification de la version d'Ansible
    local ansible_version_check
    check_ansible_version
    ansible_version_check=$?

    # Liste des collections requises avec leurs versions spÃ©cifiques pour Ansible 2.13.x
    local required_collections=()
    local required_versions=()

    if [[ ${ansible_version_check} -eq 2 ]]; then
        # Versions spÃ©cifiques pour Ansible 2.13.x
        required_collections=(
            "community.kubernetes"
            "kubernetes.core"
            "community.general"
            "ansible.posix"
            "community.docker"
        )
        required_versions=(
            "2.0.1"
            "2.3.2"
            "5.8.0"
            "1.4.0"
            "latest"
        )
        log "INFO" "Utilisation de versions spÃ©cifiques des collections compatibles avec Ansible 2.13.x"
    else
        # Versions par dÃ©faut
        required_collections=(
            "community.kubernetes"
            "kubernetes.core"
            "community.general"
            "ansible.posix"
            "community.docker"
        )
        required_versions=(
            "latest"
            "latest"
            "latest"
            "latest"
            "latest"
        )
    fi

    local missing_collections=()
    local missing_indices=()

    # VÃ©rification de l'installation d'Ansible Galaxy
    if ! command_exists ansible-galaxy; then
        log "ERROR" "La commande ansible-galaxy n'est pas disponible"
        log "ERROR" "Assurez-vous qu'Ansible est correctement installÃ©"
        return 1
    fi

    # VÃ©rification des collections installÃ©es
    for i in "${!required_collections[@]}"; do
        local collection="${required_collections[$i]}"
        log "INFO" "VÃ©rification de la collection: ${collection}"

        # Utilisation de ansible-galaxy pour vÃ©rifier si la collection est installÃ©e
        if ! ansible-galaxy collection list "${collection}" &>/dev/null; then
            log "WARNING" "Collection Ansible manquante: ${collection}"
            missing_collections+=("${collection}")
            missing_indices+=("$i")
        else
            log "SUCCESS" "Collection Ansible trouvÃ©e: ${collection}"

            # Si on utilise des versions spÃ©cifiques, vÃ©rifier si la version installÃ©e est correcte
            if [[ ${ansible_version_check} -eq 2 && "${required_versions[$i]}" != "latest" ]]; then
                local installed_version
                installed_version=$(ansible-galaxy collection list "${collection}" | grep "${collection}" | awk '{print $2}')

                if [[ "${installed_version}" != "${required_versions[$i]}" ]]; then
                    log "WARNING" "Version incorrecte de la collection ${collection}: ${installed_version} (attendue: ${required_versions[$i]})"
                    missing_collections+=("${collection}")
                    missing_indices+=("$i")
                fi
            fi
        fi
    done

    # Installation des collections manquantes
    if [[ ${#missing_collections[@]} -gt 0 ]]; then
        log "INFO" "Installation des collections Ansible manquantes ou Ã  mettre Ã  jour: ${missing_collections[*]}"

        for i in "${!missing_collections[@]}"; do
            local collection="${missing_collections[$i]}"
            local index="${missing_indices[$i]}"
            local version="${required_versions[$index]}"

            log "INFO" "Installation de la collection: ${collection}"

            if [[ "${version}" != "latest" ]]; then
                log "INFO" "Version spÃ©cifique: ${version}"
                if ! ansible-galaxy collection install "${collection}:${version}" --force &>/dev/null; then
                    log "ERROR" "Ã‰chec de l'installation de la collection: ${collection}:${version}"
                    return 1
                else
                    log "SUCCESS" "Installation de la collection rÃ©ussie: ${collection}:${version}"
                fi
            else
                if ! ansible-galaxy collection install "${collection}" &>/dev/null; then
                    log "ERROR" "Ã‰chec de l'installation de la collection: ${collection}"
                    return 1
                else
                    log "SUCCESS" "Installation de la collection rÃ©ussie: ${collection}"
                fi
            fi
        done
    else
        log "INFO" "Toutes les collections Ansible requises sont dÃ©jÃ  installÃ©es avec les versions correctes"
    fi

    # Configuration d'Ansible pour ignorer les avertissements de version
    log "INFO" "Configuration d'Ansible pour ignorer les avertissements de version..."

    # VÃ©rifier si le fichier ansible.cfg existe et est accessible en Ã©criture
    local ansible_cfg=""
    local can_write=false

    # VÃ©rifier si le fichier systÃ¨me est accessible en Ã©criture
    if [ -f /etc/ansible/ansible.cfg ] && [ -w /etc/ansible/ansible.cfg ]; then
        ansible_cfg="/etc/ansible/ansible.cfg"
        can_write=true
    # VÃ©rifier si le fichier utilisateur existe
    elif [ -f ~/.ansible.cfg ]; then
        ansible_cfg="~/.ansible.cfg"
        can_write=true
    # VÃ©rifier si le fichier local existe
    elif [ -f ./ansible.cfg ]; then
        ansible_cfg="./ansible.cfg"
        can_write=true
    # Si le fichier systÃ¨me existe mais n'est pas accessible en Ã©criture, utiliser le fichier utilisateur
    elif [ -f /etc/ansible/ansible.cfg ]; then
        log "WARNING" "Le fichier /etc/ansible/ansible.cfg existe mais n'est pas accessible en Ã©criture"
        log "INFO" "CrÃ©ation d'un fichier de configuration utilisateur ~/.ansible.cfg"
        ansible_cfg="$HOME/.ansible.cfg"
        # Copier le contenu du fichier systÃ¨me si possible
        if [ -r /etc/ansible/ansible.cfg ]; then
            cp /etc/ansible/ansible.cfg "$HOME/.ansible.cfg" 2>/dev/null || echo "[defaults]" > "$HOME/.ansible.cfg"
        else
            echo "[defaults]" > "$HOME/.ansible.cfg"
        fi
        can_write=true
    # Sinon, crÃ©er un nouveau fichier local
    else
        log "INFO" "Aucun fichier ansible.cfg trouvÃ©, crÃ©ation d'un nouveau fichier..."
        ansible_cfg="./ansible.cfg"
        echo "[defaults]" > "${ansible_cfg}"
        can_write=true
    fi

    if [ "$can_write" = true ]; then
        # VÃ©rifier si le rÃ©pertoire parent est accessible en Ã©criture
        local parent_dir=$(dirname "${ansible_cfg}")
        if [ ! -w "${parent_dir}" ]; then
            log "WARNING" "Le rÃ©pertoire parent ${parent_dir} n'est pas accessible en Ã©criture"
            can_write=false
        else
            # Ajouter ou mettre Ã  jour l'option collections_on_ansible_version_mismatch
            if grep -q "collections_on_ansible_version_mismatch" "${ansible_cfg}"; then
                if ! sed -i 's/collections_on_ansible_version_mismatch.*/collections_on_ansible_version_mismatch = ignore/' "${ansible_cfg}" 2>/dev/null; then
                    log "WARNING" "Impossible de modifier ${ansible_cfg} avec sed, tentative avec une autre mÃ©thode"
                    # MÃ©thode alternative pour Windows/WSL
                    local temp_file=$(mktemp)
                    grep -v "collections_on_ansible_version_mismatch" "${ansible_cfg}" > "${temp_file}" && \
                    echo "collections_on_ansible_version_mismatch = ignore" >> "${temp_file}" && \
                    cat "${temp_file}" > "${ansible_cfg}" && \
                    rm "${temp_file}" || {
                        log "WARNING" "Impossible de modifier ${ansible_cfg}, utilisation de la variable d'environnement"
                        can_write=false
                    }
                fi
            else
                # Trouver la section [defaults] et ajouter l'option aprÃ¨s
                if ! grep -q "\[defaults\]" "${ansible_cfg}"; then
                    echo "[defaults]" >> "${ansible_cfg}" || {
                        log "WARNING" "Impossible d'ajouter la section [defaults] Ã  ${ansible_cfg}"
                        can_write=false
                    }
                fi

                if [ "$can_write" = true ]; then
                    if ! sed -i '/\[defaults\]/a collections_on_ansible_version_mismatch = ignore' "${ansible_cfg}" 2>/dev/null; then
                        log "WARNING" "Impossible de modifier ${ansible_cfg} avec sed, tentative avec une autre mÃ©thode"
                        # MÃ©thode alternative pour Windows/WSL
                        echo "collections_on_ansible_version_mismatch = ignore" >> "${ansible_cfg}" || {
                            log "WARNING" "Impossible d'ajouter l'option Ã  ${ansible_cfg}, utilisation de la variable d'environnement"
                            can_write=false
                        }
                    fi
                fi
            fi

            if [ "$can_write" = true ]; then
                log "SUCCESS" "Configuration d'Ansible mise Ã  jour dans ${ansible_cfg}"
            fi
        fi
    fi

    if [ "$can_write" = false ]; then
        log "WARNING" "Impossible d'Ã©crire dans les fichiers de configuration Ansible"
        log "INFO" "Utilisation de la variable d'environnement ANSIBLE_COLLECTIONS_ON_ANSIBLE_VERSION_MISMATCH"
    fi

    # DÃ©finir la variable d'environnement comme solution de secours
    export ANSIBLE_COLLECTIONS_ON_ANSIBLE_VERSION_MISMATCH=ignore
    log "SUCCESS" "Variable d'environnement ANSIBLE_COLLECTIONS_ON_ANSIBLE_VERSION_MISMATCH dÃ©finie sur 'ignore'"

    return 0
}

# Fonction pour vÃ©rifier et installer les dÃ©pendances Python requises
function check_python_dependencies() {
    log "INFO" "VÃ©rification des dÃ©pendances Python requises..."

    # Liste des modules Python requis
    local required_modules=(
        "kubernetes"
        "openshift"
    )

    local missing_modules=()

    # VÃ©rification de l'installation de pip
    if ! command_exists pip || ! command_exists pip3; then
        log "WARNING" "La commande pip/pip3 n'est pas disponible"
        log "INFO" "Tentative d'installation de pip..."

        # DÃ©tection du gestionnaire de paquets
        if command_exists apt-get; then
            secure_sudo apt-get update &>/dev/null
            secure_sudo apt-get install -y python3-pip &>/dev/null
        elif command_exists dnf; then
            secure_sudo dnf install -y python3-pip &>/dev/null
        elif command_exists yum; then
            secure_sudo yum install -y python3-pip &>/dev/null
        elif command_exists pacman; then
            secure_sudo pacman -S --noconfirm python-pip &>/dev/null
        elif command_exists zypper; then
            secure_sudo zypper install -y python3-pip &>/dev/null
        else
            log "ERROR" "Impossible d'installer pip automatiquement"
            log "ERROR" "Veuillez installer pip manuellement et rÃ©essayer"
            return 1
        fi

        if ! command_exists pip && ! command_exists pip3; then
            log "ERROR" "L'installation de pip a Ã©chouÃ©"
            return 1
        else
            log "SUCCESS" "Installation de pip rÃ©ussie"
        fi
    fi

    # DÃ©terminer la commande pip Ã  utiliser
    local pip_cmd="pip"
    if ! command_exists pip && command_exists pip3; then
        pip_cmd="pip3"
    fi

    # VÃ©rification des modules installÃ©s
    for module in "${required_modules[@]}"; do
        log "INFO" "VÃ©rification du module Python: ${module}"

        # Utilisation de pip pour vÃ©rifier si le module est installÃ©
        if ! ${pip_cmd} show "${module}" &>/dev/null; then
            log "WARNING" "Module Python manquant: ${module}"
            missing_modules+=("${module}")
        else
            log "SUCCESS" "Module Python trouvÃ©: ${module}"
        fi
    done

    # Installation des modules manquants
    if [[ ${#missing_modules[@]} -gt 0 ]]; then
        log "INFO" "Installation des modules Python manquants: ${missing_modules[*]}"

        for module in "${missing_modules[@]}"; do
            log "INFO" "Installation du module: ${module}"

            # PremiÃ¨re tentative: installation standard
            if ! ${pip_cmd} install "${module}" --no-cache-dir; then
                log "WARNING" "Ã‰chec de l'installation standard du module: ${module}"
                log "INFO" "Tentative d'installation avec sudo..."

                # DeuxiÃ¨me tentative: installation avec sudo
                if ! secure_sudo ${pip_cmd} install "${module}" --no-cache-dir; then
                    log "WARNING" "Ã‰chec de l'installation avec sudo du module: ${module}"
                    log "INFO" "Tentative d'installation avec --user..."

                    # TroisiÃ¨me tentative: installation avec --user
                    if ! ${pip_cmd} install --user "${module}" --no-cache-dir; then
                        log "WARNING" "Ã‰chec de l'installation avec --user du module: ${module}"
                        log "INFO" "Tentative d'installation avec pip et le module spÃ©cifique..."

                        # QuatriÃ¨me tentative: installation avec pip et le module spÃ©cifique
                        if [[ "${module}" == "kubernetes" ]]; then
                            if ! ${pip_cmd} install kubernetes==26.1.0 --no-cache-dir; then
                                log "WARNING" "Ã‰chec de l'installation avec version spÃ©cifique du module: ${module}"
                                log "INFO" "Tentative d'installation via le gestionnaire de paquets systÃ¨me..."

                                # CinquiÃ¨me tentative: installation via le gestionnaire de paquets systÃ¨me
                                local pkg_installed=false
                                if command_exists apt-get; then
                                    if secure_sudo apt-get install -y python3-kubernetes; then
                                        pkg_installed=true
                                    fi
                                elif command_exists dnf; then
                                    if secure_sudo dnf install -y python3-kubernetes; then
                                        pkg_installed=true
                                    fi
                                elif command_exists yum; then
                                    if secure_sudo yum install -y python3-kubernetes; then
                                        pkg_installed=true
                                    fi
                                elif command_exists pacman; then
                                    if secure_sudo pacman -S --noconfirm python-kubernetes; then
                                        pkg_installed=true
                                    fi
                                elif command_exists zypper; then
                                    if secure_sudo zypper install -y python3-kubernetes; then
                                        pkg_installed=true
                                    fi
                                fi

                                if [ "$pkg_installed" = true ]; then
                                    log "SUCCESS" "Installation du module rÃ©ussie via le gestionnaire de paquets: ${module}"
                                else
                                    log "ERROR" "Ã‰chec de toutes les tentatives d'installation du module: ${module}"
                                    return 1
                                fi
                            else
                                log "SUCCESS" "Installation du module rÃ©ussie avec version spÃ©cifique: ${module}"
                            fi
                        elif [[ "${module}" == "openshift" ]]; then
                            if ! ${pip_cmd} install openshift==0.13.2 --no-cache-dir; then
                                log "WARNING" "Ã‰chec de l'installation avec version spÃ©cifique du module: ${module}"
                                log "INFO" "Tentative d'installation via le gestionnaire de paquets systÃ¨me..."

                                # CinquiÃ¨me tentative: installation via le gestionnaire de paquets systÃ¨me
                                local pkg_installed=false
                                if command_exists apt-get; then
                                    if secure_sudo apt-get install -y python3-openshift; then
                                        pkg_installed=true
                                    fi
                                elif command_exists dnf; then
                                    if secure_sudo dnf install -y python3-openshift; then
                                        pkg_installed=true
                                    fi
                                elif command_exists yum; then
                                    if secure_sudo yum install -y python3-openshift; then
                                        pkg_installed=true
                                    fi
                                elif command_exists pacman; then
                                    if secure_sudo pacman -S --noconfirm python-openshift; then
                                        pkg_installed=true
                                    fi
                                elif command_exists zypper; then
                                    if secure_sudo zypper install -y python3-openshift; then
                                        pkg_installed=true
                                    fi
                                fi

                                if [ "$pkg_installed" = true ]; then
                                    log "SUCCESS" "Installation du module rÃ©ussie via le gestionnaire de paquets: ${module}"
                                else
                                    log "ERROR" "Ã‰chec de toutes les tentatives d'installation du module: ${module}"
                                    return 1
                                fi
                            else
                                log "SUCCESS" "Installation du module rÃ©ussie avec version spÃ©cifique: ${module}"
                            fi
                        else
                            log "ERROR" "Ã‰chec de toutes les tentatives d'installation du module: ${module}"
                            return 1
                        fi
                    else
                        log "SUCCESS" "Installation du module rÃ©ussie avec --user: ${module}"
                    fi
                else
                    log "SUCCESS" "Installation du module rÃ©ussie avec sudo: ${module}"
                fi
            else
                log "SUCCESS" "Installation du module rÃ©ussie: ${module}"
            fi
        done
    else
        log "INFO" "Tous les modules Python requis sont dÃ©jÃ  installÃ©s"
    fi

    # VÃ©rification finale que tous les modules sont correctement installÃ©s
    local verification_failed=false
    for module in "${required_modules[@]}"; do
        log "INFO" "VÃ©rification finale du module Python: ${module}"

        # Tentative d'importation du module pour vÃ©rifier qu'il est utilisable
        if ! python3 -c "import ${module}" 2>/dev/null; then
            log "WARNING" "Le module ${module} ne peut pas Ãªtre importÃ© malgrÃ© l'installation"
            verification_failed=true
        else
            log "SUCCESS" "Module ${module} correctement installÃ© et importable"
        fi
    done

    if [ "$verification_failed" = true ]; then
        log "WARNING" "Certains modules Python ne sont pas correctement installÃ©s"
        log "WARNING" "L'installation pourrait rencontrer des problÃ¨mes ultÃ©rieurement"
        # Ne pas Ã©chouer ici, car les modules pourraient Ãªtre disponibles d'une autre maniÃ¨re
    fi

    return 0
}

# Fonction pour vÃ©rifier et installer les plugins Helm requis
function check_helm_plugins() {
    log "INFO" "VÃ©rification des plugins Helm requis..."

    # VÃ©rification de l'installation de Helm
    if ! command_exists helm; then
        log "ERROR" "La commande helm n'est pas disponible"
        log "ERROR" "Assurez-vous que Helm est correctement installÃ©"
        return 1
    fi

    # Liste des plugins requis avec leurs versions minimales
    local required_plugins=(
        "diff:3.4.1:https://github.com/databus23/helm-diff"
    )

    local all_plugins_installed=true

    for plugin_info in "${required_plugins[@]}"; do
        # Extraction des informations du plugin
        local plugin_name=$(echo "${plugin_info}" | cut -d':' -f1)
        local min_version=$(echo "${plugin_info}" | cut -d':' -f2)
        # Extraction de l'URL en prÃ©servant les ':' dans l'URL
        local repo_url=$(echo "${plugin_info}" | sed -E 's/^[^:]+:[^:]+://')

        # VÃ©rification plus robuste du plugin
        local plugin_exists=false
        local plugin_output=$(helm plugin list 2>/dev/null)

        # VÃ©rifier si le plugin existe dÃ©jÃ 
        if echo "${plugin_output}" | grep -q "${plugin_name}"; then
            plugin_exists=true
        # VÃ©rification supplÃ©mentaire pour le plugin diff qui peut apparaÃ®tre comme "diff" ou "helm-diff"
        elif [[ "${plugin_name}" == "diff" ]] && echo "${plugin_output}" | grep -q "helm-diff"; then
            plugin_exists=true
            plugin_name="helm-diff"  # Utiliser le nom correct pour les opÃ©rations suivantes
        fi

        if [[ ${plugin_exists} == false ]]; then
            log "WARNING" "Plugin Helm manquant: ${plugin_name}"
            log "INFO" "Installation du plugin ${plugin_name} version ${min_version}..."

            # Tentative d'installation avec gestion des erreurs rÃ©seau
            local max_retries=3
            local retry_count=0
            local install_success=false

            while [[ ${retry_count} -lt ${max_retries} && ${install_success} == false ]]; do
                # VÃ©rifier si le plugin existe dÃ©jÃ  avant d'essayer de l'installer
                if helm plugin list 2>/dev/null | grep -q "${plugin_name}" || ([[ "${plugin_name}" == "diff" ]] && helm plugin list 2>/dev/null | grep -q "helm-diff"); then
                    log "INFO" "Le plugin ${plugin_name} semble dÃ©jÃ  Ãªtre installÃ©"
                    install_success=true
                    break
                fi

                if helm plugin install "${repo_url}" --version "v${min_version}" &>/dev/null; then
                    install_success=true
                else
                    # VÃ©rifier si l'erreur est due au fait que le plugin existe dÃ©jÃ 
                    if helm plugin install "${repo_url}" --version "v${min_version}" 2>&1 | grep -q "plugin already exists"; then
                        log "INFO" "Le plugin ${plugin_name} est dÃ©jÃ  installÃ©"
                        install_success=true
                        break
                    fi

                    retry_count=$((retry_count + 1))
                    if [[ ${retry_count} -lt ${max_retries} ]]; then
                        log "WARNING" "Ã‰chec de l'installation du plugin ${plugin_name}, nouvelle tentative (${retry_count}/${max_retries})..."
                        sleep 2
                    fi
                fi
            done

            # VÃ©rification de l'installation
            if [[ ${install_success} == true ]]; then
                log "SUCCESS" "Installation du plugin ${plugin_name} rÃ©ussie ou plugin dÃ©jÃ  installÃ©"
            else
                log "ERROR" "Ã‰chec de l'installation du plugin ${plugin_name} aprÃ¨s ${max_retries} tentatives"
                log "ERROR" "VÃ©rifiez votre connexion Internet et les permissions"
                log "INFO" "Vous pouvez l'installer manuellement avec: helm plugin install ${repo_url} --version v${min_version}"
                all_plugins_installed=false
            fi
        else
            # VÃ©rifier la version du plugin
            local current_version=""

            # Extraire la version en fonction du nom du plugin (diff ou helm-diff)
            if [[ "${plugin_name}" == "diff" ]]; then
                current_version=$(helm plugin list 2>/dev/null | grep -E "(diff|helm-diff)" | awk '{print $2}')
            else
                current_version=$(helm plugin list 2>/dev/null | grep "${plugin_name}" | awk '{print $2}')
            fi

            log "INFO" "Plugin ${plugin_name} trouvÃ©, version: ${current_version}"

            # Extraire le numÃ©ro de version sans le 'v' initial
            current_version=${current_version#v}

            # VÃ©rifier si la version est infÃ©rieure Ã  la version minimale requise
            if ! version_greater_equal "${current_version}" "${min_version}"; then
                log "WARNING" "Version du plugin ${plugin_name} trop ancienne: ${current_version} (requise: ${min_version} ou supÃ©rieure)"
                log "INFO" "Mise Ã  jour du plugin ${plugin_name}..."

                # Supprimer l'ancienne version
                helm plugin uninstall "${plugin_name}" &>/dev/null

                # Installer la nouvelle version avec gestion des erreurs rÃ©seau
                local max_retries=3
                local retry_count=0
                local update_success=false

                while [[ ${retry_count} -lt ${max_retries} && ${update_success} == false ]]; do
                    if helm plugin install "${repo_url}" --version "v${min_version}" &>/dev/null; then
                        update_success=true
                    else
                        # VÃ©rifier si l'erreur est due au fait que le plugin existe dÃ©jÃ 
                        if helm plugin install "${repo_url}" --version "v${min_version}" 2>&1 | grep -q "plugin already exists"; then
                            log "INFO" "Le plugin ${plugin_name} est dÃ©jÃ  installÃ© avec la nouvelle version"
                            update_success=true
                            break
                        fi

                        retry_count=$((retry_count + 1))
                        if [[ ${retry_count} -lt ${max_retries} ]]; then
                            log "WARNING" "Ã‰chec de la mise Ã  jour du plugin ${plugin_name}, nouvelle tentative (${retry_count}/${max_retries})..."
                            sleep 2
                        fi
                    fi
                done

                # VÃ©rification de la mise Ã  jour
                if [[ ${update_success} == true ]]; then
                    log "SUCCESS" "Mise Ã  jour du plugin ${plugin_name} rÃ©ussie"
                else
                    log "ERROR" "Ã‰chec de la mise Ã  jour du plugin ${plugin_name} aprÃ¨s ${max_retries} tentatives"
                    log "ERROR" "VÃ©rifiez votre connexion Internet et les permissions"
                    log "INFO" "Vous pouvez le mettre Ã  jour manuellement avec: helm plugin install ${repo_url} --version v${min_version}"
                    all_plugins_installed=false
                fi
            else
                log "SUCCESS" "Plugin ${plugin_name} trouvÃ© avec une version compatible: ${current_version}"
            fi
        fi
    done

    if [[ ${all_plugins_installed} == true ]]; then
        return 0
    else
        return 1
    fi
}

# Fonction pour vÃ©rifier les ressources systÃ¨me locales
function check_local_resources() {
    log "INFO" "VÃ©rification des ressources systÃ¨me locales..."

    # VÃ©rification de l'espace disque
    local available_space=$(df -m . | awk 'NR==2 {print $4}')

    if [[ ${available_space} -lt ${REQUIRED_SPACE_MB} ]]; then
        log "ERROR" "Espace disque local insuffisant: ${available_space}MB disponible, ${REQUIRED_SPACE_MB}MB requis"
        return 1
    else
        log "INFO" "Espace disque local disponible: ${available_space}MB (minimum requis: ${REQUIRED_SPACE_MB}MB)"
    fi

    # VÃ©rification de la mÃ©moire disponible
    local os_name=$(uname -s)
    local available_memory=0

    if [[ "${os_name}" == "Linux" ]]; then
        available_memory=$(free -m | awk '/^Mem:/ {print $7}')
    elif [[ "${os_name}" == "Darwin" ]]; then
        available_memory=$(vm_stat | awk '/Pages free/ {free=$3} /Pages inactive/ {inactive=$3} END {print (free+inactive)*4096/1048576}' | cut -d. -f1)
    else
        log "WARNING" "SystÃ¨me d'exploitation non reconnu, impossible de vÃ©rifier la mÃ©moire disponible"
        available_memory=1024  # Valeur par dÃ©faut
    fi

    if [[ ${available_memory} -lt 1024 ]]; then
        log "WARNING" "MÃ©moire locale disponible limitÃ©e: ${available_memory}MB (recommandÃ©: 1024MB minimum)"
        log "WARNING" "Des problÃ¨mes de performance peuvent survenir pendant l'installation"
    else
        log "INFO" "MÃ©moire locale disponible: ${available_memory}MB (minimum recommandÃ©: 1024MB)"
    fi

    # VÃ©rification du nombre de processeurs
    local cpu_count=0

    if [[ "${os_name}" == "Linux" ]]; then
        cpu_count=$(nproc --all 2>/dev/null || grep -c ^processor /proc/cpuinfo 2>/dev/null || echo 1)
    elif [[ "${os_name}" == "Darwin" ]]; then
        cpu_count=$(sysctl -n hw.ncpu 2>/dev/null || echo 1)
    else
        log "WARNING" "SystÃ¨me d'exploitation non reconnu, impossible de vÃ©rifier le nombre de processeurs"
        cpu_count=1  # Valeur par dÃ©faut
    fi

    if [[ ${cpu_count} -lt 2 ]]; then
        log "WARNING" "Nombre de processeurs limitÃ©: ${cpu_count} (recommandÃ©: 2 minimum)"
        log "WARNING" "Des problÃ¨mes de performance peuvent survenir pendant l'installation"
    else
        log "INFO" "Nombre de processeurs: ${cpu_count} (minimum recommandÃ©: 2)"
    fi

    log "SUCCESS" "VÃ©rification des ressources systÃ¨me locales terminÃ©e"
    return 0
}

# Fonction pour vÃ©rifier les ressources systÃ¨me du VPS
function check_vps_resources() {
    log "INFO" "VÃ©rification des ressources systÃ¨me du VPS..."

    # VÃ©rification de la connexion SSH (seulement si exÃ©cution distante)
    if [[ "${IS_LOCAL_EXECUTION}" != "true" ]]; then
        if ! ssh -o BatchMode=yes -o ConnectTimeout=5 -p "${ansible_port}" "${ansible_user}@${ansible_host}" "echo 'Connexion SSH rÃ©ussie'" &>/dev/null; then
            log "ERROR" "Impossible de se connecter au VPS via SSH pour vÃ©rifier les ressources"
            return 1
        fi
    else
        log "INFO" "ExÃ©cution locale dÃ©tectÃ©e, pas besoin de vÃ©rifier la connexion SSH"
    fi

    # VÃ©rification de l'espace disque
    local vps_disk_total
    local vps_disk_used
    local vps_disk_free
    local vps_disk_use_percent

    if [[ "${IS_LOCAL_EXECUTION}" == "true" ]]; then
        # ExÃ©cution locale
        vps_disk_total=$(df -m / | awk 'NR==2 {print $2}' 2>/dev/null || echo "0")
        vps_disk_used=$(df -m / | awk 'NR==2 {print $3}' 2>/dev/null || echo "0")
        vps_disk_free=$(df -m / | awk 'NR==2 {print $4}' 2>/dev/null || echo "0")
        vps_disk_use_percent=$(df -m / | awk 'NR==2 {print $5}' 2>/dev/null | sed 's/%//' || echo "0")
    else
        # ExÃ©cution distante
        # Essayer plusieurs mÃ©thodes pour obtenir les informations de disque
        log "DEBUG" "RÃ©cupÃ©ration des informations disque..."
        local disk_cmd="df -m / 2>/dev/null || df -k / 2>/dev/null | awk '{size=\$2/1024; used=\$3/1024; free=\$4/1024; print size,used,free,\$5}' || echo '0 0 0 0%'"
        local disk_output=$(ssh -o BatchMode=yes -o ConnectTimeout=10 -p "${ansible_port}" "${ansible_user}@${ansible_host}" "${disk_cmd}" 2>/dev/null)
        log "DEBUG" "Sortie de la commande disque: ${disk_output:-Erreur}"

        # Extraction des valeurs de disque
        vps_disk_total=$(echo "${disk_output}" | awk 'NR==2 {print $2}' 2>/dev/null)
        vps_disk_used=$(echo "${disk_output}" | awk 'NR==2 {print $3}' 2>/dev/null)
        vps_disk_free=$(echo "${disk_output}" | awk 'NR==2 {print $4}' 2>/dev/null)
        vps_disk_use_percent=$(echo "${disk_output}" | awk 'NR==2 {print $5}' 2>/dev/null | sed 's/%//' || echo "0")

        # Si les valeurs sont vides, essayer une autre mÃ©thode
        if [[ -z "${vps_disk_total}" ]] || ! [[ "${vps_disk_total}" =~ ^[0-9.]+$ ]]; then
            log "DEBUG" "Tentative alternative pour le disque..."
            local df_cmd="df -k / 2>/dev/null"
            local df_output=$(ssh -o BatchMode=yes -o ConnectTimeout=10 -p "${ansible_port}" "${ansible_user}@${ansible_host}" "${df_cmd}" 2>/dev/null)
            log "DEBUG" "Sortie de la commande df -k: ${df_output:-Erreur}"

            # Extraction des valeurs de disque Ã  partir de df -k
            vps_disk_total=$(echo "${df_output}" | awk 'NR==2 {print int($2/1024)}' 2>/dev/null)
            vps_disk_used=$(echo "${df_output}" | awk 'NR==2 {print int($3/1024)}' 2>/dev/null)
            vps_disk_free=$(echo "${df_output}" | awk 'NR==2 {print int($4/1024)}' 2>/dev/null)
            vps_disk_use_percent=$(echo "${df_output}" | awk 'NR==2 {print $5}' 2>/dev/null | sed 's/%//' || echo "0")
        fi

        # Nettoyage des valeurs
        log "DEBUG" "Valeurs brutes aprÃ¨s rÃ©cupÃ©ration: CPU=${vps_cpu_cores}, RAM=${vps_memory_total}, Disk=${vps_disk_total}"

        # Si toujours pas de valeurs valides, utiliser des valeurs par dÃ©faut
        if [[ -z "${vps_disk_total}" ]] || ! [[ "${vps_disk_total}" =~ ^[0-9.]+$ ]]; then
            vps_disk_total="20480"  # 20 GB par dÃ©faut
            log "WARNING" "Impossible de dÃ©terminer l'espace disque total du VPS, utilisation de la valeur par dÃ©faut: ${vps_disk_total}MB"
        fi

        if [[ -z "${vps_disk_used}" ]] || ! [[ "${vps_disk_used}" =~ ^[0-9.]+$ ]]; then
            vps_disk_used="5120"  # 5 GB par dÃ©faut
        fi

        if [[ -z "${vps_disk_free}" ]] || ! [[ "${vps_disk_free}" =~ ^[0-9.]+$ ]]; then
            vps_disk_free=$((vps_disk_total - vps_disk_used))
        fi

        if [[ -z "${vps_disk_use_percent}" ]] || ! [[ "${vps_disk_use_percent}" =~ ^[0-9.]+$ ]]; then
            vps_disk_use_percent=$((vps_disk_used * 100 / vps_disk_total))
        fi

        log "DEBUG" "Valeurs aprÃ¨s nettoyage: CPU=${vps_cpu_cores}, RAM=${vps_memory_total}, Disk=${vps_disk_total}"
    fi

    log "INFO" "Espace disque du VPS: ${vps_disk_free}MB libre sur ${vps_disk_total}MB total (${vps_disk_use_percent}% utilisÃ©)"

    if [[ ${vps_disk_free} -lt ${REQUIRED_SPACE_MB} ]]; then
        log "ERROR" "Espace disque du VPS insuffisant: ${vps_disk_free}MB disponible, ${REQUIRED_SPACE_MB}MB requis"

        # VÃ©rification des rÃ©pertoires volumineux
        log "INFO" "Recherche des rÃ©pertoires volumineux sur le VPS..."
        local large_dirs=$(ssh -o BatchMode=yes -o ConnectTimeout=10 -p "${ansible_port}" "${ansible_user}@${ansible_host}" "sudo du -h --max-depth=2 /var /home /opt /usr | sort -hr | head -10" 2>/dev/null || echo "Impossible de dÃ©terminer les rÃ©pertoires volumineux")
        log "INFO" "RÃ©pertoires volumineux sur le VPS:"
        echo "${large_dirs}"

        # Suggestion de solution
        log "INFO" "Suggestions:"
        log "INFO" "1. LibÃ©rez de l'espace disque sur le VPS"
        log "INFO" "2. Augmentez la taille du disque du VPS"
        log "INFO" "3. Utilisez un autre VPS avec plus d'espace disque"

        return 1
    fi

    # VÃ©rification de la mÃ©moire
    local vps_memory_total
    local vps_memory_used
    local vps_memory_free
    local vps_memory_available

    if [[ "${IS_LOCAL_EXECUTION}" == "true" ]]; then
        # ExÃ©cution locale
        vps_memory_total=$(free -m | awk '/^Mem:/ {print $2}' 2>/dev/null || echo "0")
        vps_memory_used=$(free -m | awk '/^Mem:/ {print $3}' 2>/dev/null || echo "0")
        vps_memory_free=$(free -m | awk '/^Mem:/ {print $4}' 2>/dev/null || echo "0")
        vps_memory_available=$(free -m | awk '/^Mem:/ {print $7}' 2>/dev/null || echo "0")
    else
        # ExÃ©cution distante
        # Essayer plusieurs mÃ©thodes pour obtenir les informations de mÃ©moire
        log "DEBUG" "RÃ©cupÃ©ration des informations mÃ©moire..."
        local mem_cmd="free -m 2>/dev/null || vmstat -s -S M 2>/dev/null | grep 'total memory' | awk '{print \$1}' || cat /proc/meminfo 2>/dev/null | grep MemTotal | awk '{print \$2/1024}'"
        local mem_output=$(ssh -o BatchMode=yes -o ConnectTimeout=10 -p "${ansible_port}" "${ansible_user}@${ansible_host}" "${mem_cmd}" 2>/dev/null)
        log "DEBUG" "Sortie de la commande mÃ©moire: ${mem_output:-Erreur}"

        # Extraction des valeurs de mÃ©moire
        vps_memory_total=$(echo "${mem_output}" | grep -m1 "^Mem:" | awk '{print $2}' 2>/dev/null)
        vps_memory_used=$(echo "${mem_output}" | grep -m1 "^Mem:" | awk '{print $3}' 2>/dev/null)
        vps_memory_free=$(echo "${mem_output}" | grep -m1 "^Mem:" | awk '{print $4}' 2>/dev/null)
        vps_memory_available=$(echo "${mem_output}" | grep -m1 "^Mem:" | awk '{print $7}' 2>/dev/null)

        # Si les valeurs sont vides, essayer une autre mÃ©thode
        if [[ -z "${vps_memory_total}" ]] || ! [[ "${vps_memory_total}" =~ ^[0-9]+$ ]]; then
            log "DEBUG" "Tentative alternative pour la mÃ©moire..."
            local meminfo_cmd="cat /proc/meminfo 2>/dev/null"
            local meminfo_output=$(ssh -o BatchMode=yes -o ConnectTimeout=10 -p "${ansible_port}" "${ansible_user}@${ansible_host}" "${meminfo_cmd}" 2>/dev/null)
            log "DEBUG" "Sortie de la commande meminfo: ${meminfo_output:-Erreur}"

            # Extraction des valeurs de mÃ©moire Ã  partir de /proc/meminfo
            vps_memory_total=$(echo "${meminfo_output}" | grep "^MemTotal:" | awk '{print int($2/1024)}' 2>/dev/null)
            vps_memory_free=$(echo "${meminfo_output}" | grep "^MemFree:" | awk '{print int($2/1024)}' 2>/dev/null)
            vps_memory_available=$(echo "${meminfo_output}" | grep "^MemAvailable:" | awk '{print int($2/1024)}' 2>/dev/null)

            # Calcul de la mÃ©moire utilisÃ©e
            if [[ -n "${vps_memory_total}" ]] && [[ -n "${vps_memory_free}" ]]; then
                vps_memory_used=$((vps_memory_total - vps_memory_free))
            fi

            # Si MemAvailable n'est pas disponible, utiliser MemFree
            if [[ -z "${vps_memory_available}" ]] || ! [[ "${vps_memory_available}" =~ ^[0-9]+$ ]]; then
                vps_memory_available="${vps_memory_free}"
            fi
        fi

        # Si toujours pas de valeurs valides, utiliser des valeurs par dÃ©faut
        if [[ -z "${vps_memory_total}" ]] || ! [[ "${vps_memory_total}" =~ ^[0-9]+$ ]]; then
            vps_memory_total="4096"  # 4 GB par dÃ©faut
            log "WARNING" "Impossible de dÃ©terminer la mÃ©moire totale du VPS, utilisation de la valeur par dÃ©faut: ${vps_memory_total}MB"
        fi

        if [[ -z "${vps_memory_used}" ]] || ! [[ "${vps_memory_used}" =~ ^[0-9]+$ ]]; then
            vps_memory_used="1024"  # 1 GB par dÃ©faut
        fi

        if [[ -z "${vps_memory_free}" ]] || ! [[ "${vps_memory_free}" =~ ^[0-9]+$ ]]; then
            vps_memory_free=$((vps_memory_total - vps_memory_used))
        fi

        if [[ -z "${vps_memory_available}" ]] || ! [[ "${vps_memory_available}" =~ ^[0-9]+$ ]]; then
            vps_memory_available="${vps_memory_free}"
        fi
    fi

    log "INFO" "MÃ©moire du VPS: ${vps_memory_available}MB disponible sur ${vps_memory_total}MB total"

    # VÃ©rification du swap
    local vps_swap_total
    local vps_swap_used
    local vps_swap_free

    if [[ "${IS_LOCAL_EXECUTION}" == "true" ]]; then
        # ExÃ©cution locale
        vps_swap_total=$(free -m | awk '/^Swap:/ {print $2}' 2>/dev/null || echo "0")
        vps_swap_used=$(free -m | awk '/^Swap:/ {print $3}' 2>/dev/null || echo "0")
        vps_swap_free=$(free -m | awk '/^Swap:/ {print $4}' 2>/dev/null || echo "0")
    else
        # ExÃ©cution distante
        vps_swap_total=$(ssh -o BatchMode=yes -o ConnectTimeout=5 -p "${ansible_port}" "${ansible_user}@${ansible_host}" "free -m | awk '/^Swap:/ {print \$2}'" 2>/dev/null || echo "0")
        vps_swap_used=$(ssh -o BatchMode=yes -o ConnectTimeout=5 -p "${ansible_port}" "${ansible_user}@${ansible_host}" "free -m | awk '/^Swap:/ {print \$3}'" 2>/dev/null || echo "0")
        vps_swap_free=$(ssh -o BatchMode=yes -o ConnectTimeout=5 -p "${ansible_port}" "${ansible_user}@${ansible_host}" "free -m | awk '/^Swap:/ {print \$4}'" 2>/dev/null || echo "0")
    fi

    log "INFO" "Swap du VPS: ${vps_swap_free}MB libre sur ${vps_swap_total}MB total"

    # VÃ©rification des seuils de mÃ©moire
    if [[ ${vps_memory_total} -lt 4096 ]]; then
        log "WARNING" "MÃ©moire totale du VPS insuffisante: ${vps_memory_total}MB (recommandÃ©: 4096MB minimum)"
        log "WARNING" "Des problÃ¨mes de performance peuvent survenir pendant l'installation"

        if [[ ${vps_memory_total} -lt 2048 ]]; then
            log "ERROR" "MÃ©moire totale du VPS critique: ${vps_memory_total}MB (minimum absolu: 2048MB)"
            log "ERROR" "L'installation risque d'Ã©chouer par manque de mÃ©moire"

            # Suggestion de solution
            log "INFO" "Suggestions:"
            log "INFO" "1. Augmentez la mÃ©moire du VPS"
            log "INFO" "2. Ajoutez ou augmentez l'espace swap"
            log "INFO" "3. Utilisez un autre VPS avec plus de mÃ©moire"

            log "INFO" "Voulez-vous continuer malgrÃ© tout? (o/N)"
            read -r answer
            if [[ ! "${answer}" =~ ^[Oo]$ ]]; then
                return 1
            fi
        fi
    fi

    # VÃ©rification du nombre de processeurs
    local vps_cpu_cores
    local vps_cpu_load

    if [[ "${IS_LOCAL_EXECUTION}" == "true" ]]; then
        # ExÃ©cution locale
        vps_cpu_cores=$(nproc --all 2>/dev/null || grep -c ^processor /proc/cpuinfo 2>/dev/null || echo "0")
        vps_cpu_load=$(cat /proc/loadavg | awk '{print $1}' 2>/dev/null || echo "0")
    else
        # ExÃ©cution distante
        # Essayer plusieurs mÃ©thodes pour obtenir le nombre de processeurs
        log "DEBUG" "RÃ©cupÃ©ration des informations CPU..."
        vps_cpu_cores=$(ssh -o BatchMode=yes -o ConnectTimeout=10 -p "${ansible_port}" "${ansible_user}@${ansible_host}" "nproc --all 2>/dev/null || grep -c ^processor /proc/cpuinfo 2>/dev/null || lscpu 2>/dev/null | grep '^CPU(s):' | awk '{print \$2}' || echo '0'" 2>/dev/null)
        log "DEBUG" "Sortie de la commande CPU: ${vps_cpu_cores:-Erreur}"

        # Si la valeur est vide ou non numÃ©rique, essayer une autre mÃ©thode
        if [[ -z "${vps_cpu_cores}" ]] || ! [[ "${vps_cpu_cores}" =~ ^[0-9]+$ ]]; then
            log "DEBUG" "Tentative alternative pour CPU avec nproc..."
            vps_cpu_cores=$(ssh -o BatchMode=yes -o ConnectTimeout=10 -p "${ansible_port}" "${ansible_user}@${ansible_host}" "nproc 2>/dev/null || echo '0'" 2>/dev/null)
            log "DEBUG" "Sortie de la commande nproc: ${vps_cpu_cores:-Erreur}"
        fi

        # Si toujours pas de valeur valide, utiliser une valeur par dÃ©faut
        if [[ -z "${vps_cpu_cores}" ]] || ! [[ "${vps_cpu_cores}" =~ ^[0-9]+$ ]]; then
            vps_cpu_cores="2"  # Valeur par dÃ©faut raisonnable
            log "WARNING" "Impossible de dÃ©terminer le nombre de cÅ“urs CPU du VPS, utilisation de la valeur par dÃ©faut: ${vps_cpu_cores}"
        fi

        # RÃ©cupÃ©ration de la charge CPU
        vps_cpu_load=$(ssh -o BatchMode=yes -o ConnectTimeout=10 -p "${ansible_port}" "${ansible_user}@${ansible_host}" "cat /proc/loadavg 2>/dev/null | awk '{print \$1}' || echo '0'" 2>/dev/null)

        # Si la valeur est vide ou non numÃ©rique, utiliser une valeur par dÃ©faut
        if [[ -z "${vps_cpu_load}" ]] || ! [[ "${vps_cpu_load}" =~ ^[0-9.]+$ ]]; then
            vps_cpu_load="0.0"  # Valeur par dÃ©faut
            log "WARNING" "Impossible de dÃ©terminer la charge CPU du VPS, utilisation de la valeur par dÃ©faut: ${vps_cpu_load}"
        fi
    fi

    log "INFO" "CPU du VPS: ${vps_cpu_cores} cÅ“urs, charge actuelle: ${vps_cpu_load}"

    if [[ ${vps_cpu_cores} -lt 2 ]]; then
        log "WARNING" "Nombre de cÅ“urs CPU du VPS insuffisant: ${vps_cpu_cores} (recommandÃ©: 2 minimum)"
        log "WARNING" "Des problÃ¨mes de performance peuvent survenir pendant l'installation"
    fi

    # VÃ©rification de la charge CPU
    if (( $(echo "${vps_cpu_load} > ${vps_cpu_cores}" | bc -l) )); then
        log "WARNING" "Charge CPU du VPS Ã©levÃ©e: ${vps_cpu_load} (nombre de cÅ“urs: ${vps_cpu_cores})"
        log "WARNING" "Le VPS est actuellement sous forte charge, ce qui peut affecter l'installation"

        # VÃ©rification des processus consommant le plus de CPU
        log "INFO" "Processus consommant le plus de CPU sur le VPS:"
        local top_cpu_processes

        if [[ "${IS_LOCAL_EXECUTION}" == "true" ]]; then
            # ExÃ©cution locale
            top_cpu_processes=$(ps aux --sort=-%cpu | head -6 2>/dev/null || echo "Impossible de dÃ©terminer les processus")
        else
            # ExÃ©cution distante
            top_cpu_processes=$(ssh -o BatchMode=yes -o ConnectTimeout=10 -p "${ansible_port}" "${ansible_user}@${ansible_host}" "ps aux --sort=-%cpu | head -6" 2>/dev/null || echo "Impossible de dÃ©terminer les processus")
        fi
        echo "${top_cpu_processes}"
    fi

    # VÃ©rification des processus consommant le plus de mÃ©moire
    if [[ ${vps_memory_available} -lt 1024 ]]; then
        log "WARNING" "MÃ©moire disponible du VPS faible: ${vps_memory_available}MB"
        log "INFO" "Processus consommant le plus de mÃ©moire sur le VPS:"
        local top_mem_processes

        if [[ "${IS_LOCAL_EXECUTION}" == "true" ]]; then
            # ExÃ©cution locale
            top_mem_processes=$(ps aux --sort=-%mem | head -6 2>/dev/null || echo "Impossible de dÃ©terminer les processus")
        else
            # ExÃ©cution distante
            top_mem_processes=$(ssh -o BatchMode=yes -o ConnectTimeout=10 -p "${ansible_port}" "${ansible_user}@${ansible_host}" "ps aux --sort=-%mem | head -6" 2>/dev/null || echo "Impossible de dÃ©terminer les processus")
        fi
        echo "${top_mem_processes}"
    fi

    # VÃ©rification des services en cours d'exÃ©cution
    log "INFO" "VÃ©rification des services en cours d'exÃ©cution sur le VPS..."
    local running_services

    if [[ "${IS_LOCAL_EXECUTION}" == "true" ]]; then
        # ExÃ©cution locale
        running_services=$(systemctl list-units --type=service --state=running | grep -v systemd | head -10 2>/dev/null || echo "Impossible de dÃ©terminer les services")
    else
        # ExÃ©cution distante
        running_services=$(ssh -o BatchMode=yes -o ConnectTimeout=10 -p "${ansible_port}" "${ansible_user}@${ansible_host}" "systemctl list-units --type=service --state=running | grep -v systemd | head -10" 2>/dev/null || echo "Impossible de dÃ©terminer les services")
    fi

    log "INFO" "Services en cours d'exÃ©cution sur le VPS (top 10):"
    echo "${running_services}" | grep -v "UNIT\|LOAD\|ACTIVE\|SUB\|DESCRIPTION\|^$\|loaded units listed"

    # VÃ©rification des ports ouverts
    log "INFO" "VÃ©rification des ports ouverts sur le VPS..."
    local open_ports

    if [[ "${IS_LOCAL_EXECUTION}" == "true" ]]; then
        # ExÃ©cution locale
        open_ports=$(ss -tuln | grep LISTEN 2>/dev/null || echo "Impossible de dÃ©terminer les ports ouverts")
    else
        # ExÃ©cution distante
        open_ports=$(ssh -o BatchMode=yes -o ConnectTimeout=10 -p "${ansible_port}" "${ansible_user}@${ansible_host}" "ss -tuln | grep LISTEN" 2>/dev/null || echo "Impossible de dÃ©terminer les ports ouverts")
    fi

    log "INFO" "Ports ouverts sur le VPS:"
    echo "${open_ports}"

    # VÃ©rification des conflits potentiels
    for port in "${REQUIRED_PORTS[@]}"; do
        if echo "${open_ports}" | grep -q ":${port} "; then
            log "WARNING" "Le port ${port} est dÃ©jÃ  utilisÃ© sur le VPS, ce qui peut causer des conflits"
        fi
    done

    log "SUCCESS" "VÃ©rification des ressources systÃ¨me du VPS terminÃ©e"
    return 0
}

# Fonction pour vÃ©rifier l'espace disque disponible (pour compatibilitÃ©)
function check_disk_space() {
    check_local_resources
    return $?
}

# Fonction pour dÃ©tecter si le script est exÃ©cutÃ© sur le VPS cible
function is_local_execution() {
    local target_host="$1"

    # Si l'hÃ´te cible est localhost ou 127.0.0.1, c'est une exÃ©cution locale
    if [[ "${target_host}" == "localhost" || "${target_host}" == "127.0.0.1" ]]; then
        return 0
    fi

    # RÃ©cupÃ©ration des adresses IP locales
    local local_ips=$(ip -o -4 addr show | awk '{print $4}' | cut -d/ -f1 | tr '\n' ' ' 2>/dev/null || echo "")

    # Si l'hÃ´te cible est une des adresses IP locales, c'est une exÃ©cution locale
    for ip in ${local_ips}; do
        if [[ "${target_host}" == "${ip}" ]]; then
            return 0
        fi
    done

    # VÃ©rification du nom d'hÃ´te
    local hostname=$(hostname -f 2>/dev/null || hostname 2>/dev/null || echo "")
    if [[ -n "${hostname}" && "${target_host}" == "${hostname}" ]]; then
        return 0
    fi

    # Ce n'est pas une exÃ©cution locale
    return 1
}

# Variable globale pour indiquer si le script est exÃ©cutÃ© sur le VPS cible
IS_LOCAL_EXECUTION=false

# Fonction pour extraire les informations d'inventaire
function extraire_informations_inventaire() {
    log "INFO" "Extraction des informations d'inventaire depuis ${inventory_file}..."

    # VÃ©rification de l'existence du fichier d'inventaire
    if [[ ! -f "${ANSIBLE_DIR}/${inventory_file}" ]]; then
        log "ERROR" "Fichier d'inventaire non trouvÃ©: ${ANSIBLE_DIR}/${inventory_file}"
        cleanup
        exit 1
    fi

    # Extraction des informations d'inventaire avec Python
    local python_script=$(cat << 'EOF'
import sys
import yaml
import os

inventory_file = sys.argv[1]

try:
    with open(inventory_file, 'r') as f:
        inventory = yaml.safe_load(f)

    # Recherche du premier hÃ´te VPS
    vps_host = None
    vps_port = None
    vps_user = None

    if 'all' in inventory and 'children' in inventory['all'] and 'vps' in inventory['all']['children']:
        vps_hosts = inventory['all']['children']['vps'].get('hosts', {})
        if vps_hosts:
            first_host = next(iter(vps_hosts))
            host_info = vps_hosts[first_host]
            vps_host = host_info.get('ansible_host')
            default_port = os.environ.get('LIONS_VPS_PORT', '225')
            vps_port = host_info.get('ansible_port', default_port)
            vps_user = host_info.get('ansible_user')

    # Recherche dans les variables globales si non trouvÃ©
    if not vps_user and 'all' in inventory and 'vars' in inventory['all']:
        vps_user = inventory['all']['vars'].get('ansible_user')

    # Affichage des rÃ©sultats
    if vps_host:
        print(f"ansible_host={vps_host}")
    if vps_port:
        print(f"ansible_port={vps_port}")
    if vps_user:
        print(f"ansible_user={vps_user}")

except Exception as e:
    print(f"error={str(e)}", file=sys.stderr)
    sys.exit(1)
EOF
)

    # ExÃ©cution du script Python avec timeout
    log "DEBUG" "ExÃ©cution du script Python pour extraire les informations d'inventaire..."

    # VÃ©rification que Python3 est installÃ©
    if ! command_exists python3; then
        log "ERROR" "Python3 n'est pas installÃ©, impossible d'extraire les informations d'inventaire"
        log "ERROR" "Installez Python3 avec: sudo apt-get install python3 (Debian/Ubuntu)"
        log "ERROR" "ou l'Ã©quivalent pour votre distribution"

        # Passage directement Ã  la mÃ©thode fallback
        log "WARNING" "Tentative de fallback avec grep pour extraire les informations d'inventaire..."

        # Utilisation de grep avec timeout pour Ã©viter les blocages
        ansible_host=$(timeout 5 grep -A10 "hosts:" "${ANSIBLE_DIR}/${inventory_file}" | grep "ansible_host:" | head -1 | awk -F': ' '{print $2}' | tr -d ' ' 2>/dev/null || echo "")
        ansible_port=$(timeout 5 grep -A10 "hosts:" "${ANSIBLE_DIR}/${inventory_file}" | grep "ansible_port:" | head -1 | awk -F': ' '{print $2}' | tr -d ' ' 2>/dev/null || echo "${LIONS_VPS_PORT:-22}")
        ansible_user=$(timeout 5 grep -A10 "hosts:" "${ANSIBLE_DIR}/${inventory_file}" | grep "ansible_user:" | head -1 | awk -F': ' '{print $2}' | tr -d ' ' 2>/dev/null || echo "")

        # Si toujours pas trouvÃ©, chercher dans les variables globales
        if [[ -z "${ansible_user}" ]]; then
            ansible_user=$(timeout 5 grep -A10 "vars:" "${ANSIBLE_DIR}/${inventory_file}" | grep "ansible_user:" | head -1 | awk -F': ' '{print $2}' | tr -d ' ' 2>/dev/null || echo "")
        fi

        # VÃ©rification des valeurs extraites par fallback
        if [[ -n "${ansible_host}" && -n "${ansible_user}" ]]; then
            log "SUCCESS" "Extraction des informations d'inventaire rÃ©ussie avec la mÃ©thode fallback"
            log "INFO" "Informations d'inventaire extraites (fallback):"
            log "INFO" "- HÃ´te: ${ansible_host}"
            log "INFO" "- Port: ${ansible_port}"
            log "INFO" "- Utilisateur: ${ansible_user}"
            return 0
        else
            log "ERROR" "Ã‰chec de l'extraction des informations d'inventaire, mÃªme avec la mÃ©thode fallback"
            cleanup
            exit 1
        fi
    fi

    # Affichage du contenu du fichier d'inventaire en mode debug
    if [[ "${debug_mode}" == "true" ]]; then
        log "DEBUG" "Contenu du fichier d'inventaire (${ANSIBLE_DIR}/${inventory_file}):"
        cat "${ANSIBLE_DIR}/${inventory_file}" | while IFS= read -r line; do
            log "DEBUG" "  ${line}"
        done
    fi

    # ExÃ©cution avec timeout pour Ã©viter les blocages
    local inventory_info=$(timeout 10 python3 -c "${python_script}" "${ANSIBLE_DIR}/${inventory_file}" 2>&1)
    local exit_code=$?

    if [[ ${exit_code} -eq 124 ]]; then
        log "ERROR" "Timeout lors de l'extraction des informations d'inventaire"
        log "ERROR" "Le script Python a pris trop de temps pour s'exÃ©cuter"
        log "ERROR" "VÃ©rifiez le fichier d'inventaire et les dÃ©pendances Python"
        cleanup
        exit 1
    elif [[ ${exit_code} -ne 0 ]]; then
        log "ERROR" "Impossible d'extraire les informations d'inventaire (code ${exit_code})"
        log "ERROR" "Erreur: ${inventory_info}"
        log "ERROR" "VÃ©rifiez le format du fichier d'inventaire et les dÃ©pendances Python (yaml)"

        # VÃ©rification de la prÃ©sence du module yaml
        if ! python3 -c "import yaml" &>/dev/null; then
            log "WARNING" "Le module Python 'yaml' n'est pas installÃ©"
            log "INFO" "Tentative d'installation automatique du module yaml..."

            # VÃ©rification de pip
            if ! command_exists pip3 && ! command_exists pip; then
                log "ERROR" "pip n'est pas installÃ©, impossible d'installer le module yaml"
                log "ERROR" "Installez pip avec: sudo apt-get install python3-pip (Debian/Ubuntu)"
                log "ERROR" "ou l'Ã©quivalent pour votre distribution"
            else
                # Installation du module yaml
                local pip_cmd="pip3"
                if ! command_exists pip3; then
                    pip_cmd="pip"
                fi

                if secure_sudo ${pip_cmd} install pyyaml &>/dev/null; then
                    log "SUCCESS" "Module yaml installÃ© avec succÃ¨s"
                    # RÃ©essayer l'extraction aprÃ¨s l'installation
                    inventory_info=$(timeout 10 python3 -c "${python_script}" "${ANSIBLE_DIR}/${inventory_file}" 2>&1)
                    exit_code=$?

                    if [[ ${exit_code} -eq 0 ]]; then
                        log "SUCCESS" "Extraction des informations d'inventaire rÃ©ussie aprÃ¨s installation du module yaml"
                    else
                        log "ERROR" "Ã‰chec de l'extraction des informations d'inventaire mÃªme aprÃ¨s installation du module yaml"
                    fi
                else
                    log "ERROR" "Ã‰chec de l'installation du module yaml"
                    log "ERROR" "Installez-le manuellement avec: sudo pip3 install pyyaml"
                fi
            fi
        fi

        # Tentative de fallback avec grep si le script Python Ã©choue
        log "WARNING" "Tentative de fallback avec grep pour extraire les informations d'inventaire..."

        # Utilisation de grep avec timeout pour Ã©viter les blocages
        ansible_host=$(timeout 5 grep -A10 "hosts:" "${ANSIBLE_DIR}/${inventory_file}" | grep "ansible_host:" | head -1 | awk -F': ' '{print $2}' | tr -d ' ' 2>/dev/null || echo "")
        ansible_port=$(timeout 5 grep -A10 "hosts:" "${ANSIBLE_DIR}/${inventory_file}" | grep "ansible_port:" | head -1 | awk -F': ' '{print $2}' | tr -d ' ' 2>/dev/null || echo "${LIONS_VPS_PORT:-22}")
        ansible_user=$(timeout 5 grep -A10 "hosts:" "${ANSIBLE_DIR}/${inventory_file}" | grep "ansible_user:" | head -1 | awk -F': ' '{print $2}' | tr -d ' ' 2>/dev/null || echo "")

        # Si toujours pas trouvÃ©, chercher dans les variables globales
        if [[ -z "${ansible_user}" ]]; then
            ansible_user=$(timeout 5 grep -A10 "vars:" "${ANSIBLE_DIR}/${inventory_file}" | grep "ansible_user:" | head -1 | awk -F': ' '{print $2}' | tr -d ' ' 2>/dev/null || echo "")
        fi

        # VÃ©rification des valeurs extraites par fallback
        if [[ -n "${ansible_host}" && -n "${ansible_user}" ]]; then
            log "SUCCESS" "Extraction des informations d'inventaire rÃ©ussie avec la mÃ©thode fallback"
            log "INFO" "Informations d'inventaire extraites (fallback):"
            log "INFO" "- HÃ´te: ${ansible_host}"
            log "INFO" "- Port: ${ansible_port}"
            log "INFO" "- Utilisateur: ${ansible_user}"
            return 0
        else
            log "ERROR" "Ã‰chec de l'extraction des informations d'inventaire, mÃªme avec la mÃ©thode fallback"
            cleanup
            exit 1
        fi
    fi

    # Extraction des valeurs
    ansible_host=$(echo "${inventory_info}" | grep "ansible_host=" | cut -d'=' -f2)
    ansible_port=$(echo "${inventory_info}" | grep "ansible_port=" | cut -d'=' -f2)
    ansible_user=$(echo "${inventory_info}" | grep "ansible_user=" | cut -d'=' -f2)

    # Valeurs par dÃ©faut si non trouvÃ©es
    ansible_host="${ansible_host:-localhost}"
    ansible_port="${ansible_port:-${LIONS_VPS_PORT:-225}}"
    ansible_user="${ansible_user:-$(whoami)}"

    log "INFO" "Informations d'inventaire extraites:"
    log "INFO" "- HÃ´te: ${ansible_host}"
    log "INFO" "- Port: ${ansible_port}"
    log "INFO" "- Utilisateur: ${ansible_user}"

    # VÃ©rification si le script est exÃ©cutÃ© sur le VPS cible
    if is_local_execution "${ansible_host}"; then
        IS_LOCAL_EXECUTION=true
        log "INFO" "DÃ©tection d'exÃ©cution locale: le script est exÃ©cutÃ© directement sur le VPS cible"
        log "INFO" "Les commandes SSH seront remplacÃ©es par des commandes locales"
    else
        # VÃ©rification supplÃ©mentaire pour dÃ©tecter l'exÃ©cution locale
        log "INFO" "Tentative de dÃ©tection alternative d'exÃ©cution locale..."

        # VÃ©rification si l'adresse IP du VPS correspond Ã  une interface rÃ©seau locale
        local local_ip_check=$(ip route get "${ansible_host}" 2>/dev/null | grep -q "dev lo" && echo "true" || echo "false")

        # VÃ©rification si le nom d'hÃ´te du VPS correspond au nom d'hÃ´te local
        local hostname_check=$(hostname -I 2>/dev/null | grep -q "${ansible_host}" && echo "true" || echo "false")

        # VÃ©rification si l'utilisateur peut exÃ©cuter des commandes locales sans SSH
        local sudo_check=$(sudo -n true 2>/dev/null && echo "true" || echo "false")

        log "DEBUG" "RÃ©sultats des vÃ©rifications alternatives: IP locale=${local_ip_check}, Hostname=${hostname_check}, Sudo=${sudo_check}"

        if [[ "${local_ip_check}" == "true" || "${hostname_check}" == "true" || "${ansible_host}" == "localhost" || "${ansible_host}" == "127.0.0.1" ]]; then
            IS_LOCAL_EXECUTION=true
            log "INFO" "DÃ©tection alternative d'exÃ©cution locale rÃ©ussie: le script est exÃ©cutÃ© directement sur le VPS cible"
            log "INFO" "Les commandes SSH seront remplacÃ©es par des commandes locales"
        else
            # Demander Ã  l'utilisateur si le script est exÃ©cutÃ© localement
            log "WARNING" "Impossible de dÃ©terminer automatiquement si le script est exÃ©cutÃ© localement sur le VPS cible"
            log "INFO" "ÃŠtes-vous en train d'exÃ©cuter ce script directement sur le VPS cible? (o/n)"
            read -r user_response

            if [[ "${user_response}" =~ ^[oO][uU]?[iI]?$ || "${user_response}" =~ ^[yY][eE]?[sS]?$ ]]; then
                IS_LOCAL_EXECUTION=true
                log "INFO" "Configuration manuelle pour exÃ©cution locale: le script est exÃ©cutÃ© directement sur le VPS cible"
                log "INFO" "Les commandes SSH seront remplacÃ©es par des commandes locales"
            else
                IS_LOCAL_EXECUTION=false
                log "INFO" "Configuration manuelle pour exÃ©cution distante: le script est exÃ©cutÃ© depuis une machine diffÃ©rente du VPS cible"
            fi
        fi
    fi

    return 0
}

# Fonction pour ouvrir les ports requis sur le VPS
function open_required_ports() {
    local target_host="${ansible_host}"
    local target_port="${ansible_port}"
    local ports_to_open=("$@")
    local timeout=10
    local success=true

    log "INFO" "Tentative d'ouverture des ports requis sur ${target_host}..."

    # VÃ©rification que nous avons des ports Ã  ouvrir
    if [[ ${#ports_to_open[@]} -eq 0 ]]; then
        log "WARNING" "Aucun port Ã  ouvrir spÃ©cifiÃ©"
        return 0
    fi

    # VÃ©rification si exÃ©cution locale ou distante
    if [[ "${IS_LOCAL_EXECUTION}" == "true" ]]; then
        log "INFO" "ExÃ©cution locale dÃ©tectÃ©e, utilisation de commandes locales pour ouvrir les ports"

        # VÃ©rification que UFW est installÃ© et actif
        if ! command -v ufw &>/dev/null || ! systemctl is-active --quiet ufw; then
            log "WARNING" "UFW n'est pas installÃ© ou n'est pas actif sur le VPS"
            log "INFO" "Tentative d'installation et d'activation de UFW..."

            # Installation de UFW si nÃ©cessaire
            log "INFO" "Installation de UFW (commande interactive, veuillez entrer votre mot de passe si demandÃ©)..."
            if ! sudo apt-get update && sudo apt-get install -y ufw; then
                log "ERROR" "Impossible d'installer UFW"
                return 1
            fi

            # Activation de UFW
            log "INFO" "Activation de UFW (commande interactive, veuillez entrer votre mot de passe si demandÃ©)..."
            if ! sudo ufw --force enable; then
                log "ERROR" "Impossible d'activer UFW"
                return 1
            fi

            # VÃ©rification que UFW est bien actif
            log "INFO" "VÃ©rification que UFW est bien actif..."
            if ! sudo ufw status | grep -q "Status: active"; then
                log "WARNING" "UFW n'est pas actif malgrÃ© la tentative d'activation, nouvelle tentative..."
                # DeuxiÃ¨me tentative avec une approche diffÃ©rente
                if ! (echo 'y' | sudo ufw --force enable && sudo systemctl restart ufw); then
                    log "ERROR" "Impossible d'activer UFW malgrÃ© plusieurs tentatives"
                    return 1
                fi

                # VÃ©rification finale
                if ! sudo ufw status | grep -q "Status: active"; then
                    log "ERROR" "Impossible d'activer UFW malgrÃ© plusieurs tentatives"
                    return 1
                else
                    log "SUCCESS" "UFW est maintenant actif aprÃ¨s la deuxiÃ¨me tentative"
                fi
            else
                log "SUCCESS" "UFW est bien actif"
            fi

            log "SUCCESS" "UFW installÃ© et activÃ© avec succÃ¨s"
        fi
    else
        # VÃ©rification de la connexion SSH
        if ! ssh -o StrictHostKeyChecking=no -o ConnectTimeout=5 -p "${target_port}" "${ansible_user}@${target_host}" "echo 'Test de connexion'" &>/dev/null; then
            log "ERROR" "Impossible de se connecter au VPS via SSH pour ouvrir les ports"
            return 1
        fi

        # VÃ©rification que UFW est installÃ© et actif
        if ! ssh -o StrictHostKeyChecking=no -o ConnectTimeout=5 -p "${target_port}" "${ansible_user}@${target_host}" "command -v ufw &>/dev/null && systemctl is-active --quiet ufw" &>/dev/null; then
            log "WARNING" "UFW n'est pas installÃ© ou n'est pas actif sur le VPS"
            log "INFO" "Tentative d'installation et d'activation de UFW..."

            # Installation de UFW si nÃ©cessaire
            log "INFO" "Installation de UFW sur le VPS (commande interactive, veuillez entrer votre mot de passe si demandÃ©)..."
            local ssh_cmd="ssh -t -o StrictHostKeyChecking=no -o ConnectTimeout=${timeout} -p \"${target_port}\" \"${ansible_user}@${target_host}\" \"sudo apt-get update && sudo apt-get install -y ufw\""
            log "DEBUG" "ExÃ©cution de la commande avec eval: ${ssh_cmd}"
            eval "${ssh_cmd}"
            if [ $? -ne 0 ]; then
                log "ERROR" "Impossible d'installer UFW sur le VPS"
                return 1
            fi

            # Activation de UFW
            log "INFO" "Activation de UFW sur le VPS (commande interactive, veuillez entrer votre mot de passe si demandÃ©)..."
            local ssh_cmd="ssh -t -o StrictHostKeyChecking=no -o ConnectTimeout=${timeout} -p \"${target_port}\" \"${ansible_user}@${target_host}\" \"sudo ufw --force enable\""
            log "DEBUG" "ExÃ©cution de la commande avec eval: ${ssh_cmd}"
            eval "${ssh_cmd}"
            if [ $? -ne 0 ]; then
                log "ERROR" "Impossible d'activer UFW sur le VPS"
                return 1
            fi

            # VÃ©rification que UFW est bien actif
            log "INFO" "VÃ©rification que UFW est bien actif..."
            local ufw_status_cmd="ssh -o StrictHostKeyChecking=no -o ConnectTimeout=${timeout} -p \"${target_port}\" \"${ansible_user}@${target_host}\" \"sudo ufw status | grep -q 'Status: active' && echo 'active' || echo 'inactive'\""
            local ufw_status=$(eval "${ufw_status_cmd}" 2>/dev/null)

            if [[ "${ufw_status}" != "active" ]]; then
                log "WARNING" "UFW n'est pas actif malgrÃ© la tentative d'activation, nouvelle tentative..."
                # DeuxiÃ¨me tentative avec une approche diffÃ©rente
                local ssh_cmd="ssh -t -o StrictHostKeyChecking=no -o ConnectTimeout=${timeout} -p \"${target_port}\" \"${ansible_user}@${target_host}\" \"echo 'y' | sudo ufw --force enable && sudo systemctl restart ufw\""
                log "DEBUG" "ExÃ©cution de la commande avec eval: ${ssh_cmd}"
                eval "${ssh_cmd}"

                # VÃ©rification finale
                local ufw_status_cmd="ssh -o StrictHostKeyChecking=no -o ConnectTimeout=${timeout} -p \"${target_port}\" \"${ansible_user}@${target_host}\" \"sudo ufw status | grep -q 'Status: active' && echo 'active' || echo 'inactive'\""
                local ufw_status=$(eval "${ufw_status_cmd}" 2>/dev/null)

                if [[ "${ufw_status}" != "active" ]]; then
                    log "ERROR" "Impossible d'activer UFW sur le VPS malgrÃ© plusieurs tentatives"
                    return 1
                else
                    log "SUCCESS" "UFW est maintenant actif aprÃ¨s la deuxiÃ¨me tentative"
                fi
            else
                log "SUCCESS" "UFW est bien actif"
            fi

            log "SUCCESS" "UFW installÃ© et activÃ© avec succÃ¨s"
        fi
    fi

    # Ouverture des ports
    log "INFO" "Ouverture des ports: ${ports_to_open[*]}"

    for port in "${ports_to_open[@]}"; do
        log "INFO" "Ouverture du port ${port}..."

        if [[ "${IS_LOCAL_EXECUTION}" == "true" ]]; then
            # ExÃ©cution locale

            # VÃ©rification si le port est dÃ©jÃ  ouvert
            log "INFO" "VÃ©rification si le port ${port} est dÃ©jÃ  ouvert (commande interactive, veuillez entrer votre mot de passe si demandÃ©)..."
            if sudo ufw status | grep -E "^${port}/(tcp|udp)" &>/dev/null; then
                log "INFO" "Le port ${port} est dÃ©jÃ  ouvert dans UFW"
                continue
            fi

            # Validation du port
            if ! [[ "${port}" =~ ^[0-9]+$ ]] || [ "${port}" -lt 1 ] || [ "${port}" -gt 65535 ]; then
                log "WARNING" "Port invalide: ${port}. Les ports doivent Ãªtre des nombres entre 1 et 65535."
                success=false
                continue
            fi

            # Ouverture du port TCP
            log "INFO" "Ouverture du port ${port}/tcp (commande interactive, veuillez entrer votre mot de passe si demandÃ©)..."
            if ! sudo ufw allow "${port}/tcp"; then
                log "ERROR" "Impossible d'ouvrir le port ${port}/tcp"
                success=false
                continue
            fi

            # Ouverture du port UDP
            log "INFO" "Ouverture du port ${port}/udp (commande interactive, veuillez entrer votre mot de passe si demandÃ©)..."
            if ! sudo ufw allow "${port}/udp"; then
                log "WARNING" "Impossible d'ouvrir le port ${port}/udp"
                # Ne pas Ã©chouer pour UDP, car certains services n'utilisent que TCP
            fi
        else
            # ExÃ©cution distante

            # VÃ©rification si le port est dÃ©jÃ  ouvert
            log "INFO" "VÃ©rification si le port ${port} est dÃ©jÃ  ouvert (commande interactive, veuillez entrer votre mot de passe si demandÃ©)..."
            local ssh_cmd="ssh -tt -o StrictHostKeyChecking=no -o ConnectTimeout=5 -p \"${target_port}\" \"${ansible_user}@${target_host}\" \"sudo ufw status | grep -E \\\"^${port}/(tcp|udp)\\\"\""
            log "DEBUG" "ExÃ©cution de la commande avec eval: ${ssh_cmd}"
            eval "${ssh_cmd}"
            if [ $? -eq 0 ]; then
                log "INFO" "Le port ${port} est dÃ©jÃ  ouvert dans UFW"
                continue
            fi

            # Validation du port
            if ! [[ "${port}" =~ ^[0-9]+$ ]] || [ "${port}" -lt 1 ] || [ "${port}" -gt 65535 ]; then
                log "WARNING" "Port invalide: ${port}. Les ports doivent Ãªtre des nombres entre 1 et 65535."
                success=false
                continue
            fi

            # Ouverture du port TCP
            log "INFO" "Ouverture du port ${port}/tcp sur le VPS (commande interactive, veuillez entrer votre mot de passe si demandÃ©)..."
            local ssh_cmd="ssh -tt -o StrictHostKeyChecking=no -o ConnectTimeout=${timeout} -p \"${target_port}\" \"${ansible_user}@${target_host}\" \"sudo ufw allow \\\"${port}/tcp\\\"\""
            log "DEBUG" "ExÃ©cution de la commande avec eval: ${ssh_cmd}"
            eval "${ssh_cmd}"
            if [ $? -ne 0 ]; then
                log "ERROR" "Impossible d'ouvrir le port ${port}/tcp sur le VPS"
                success=false
                continue
            fi

            # Ouverture du port UDP
            log "INFO" "Ouverture du port ${port}/udp sur le VPS (commande interactive, veuillez entrer votre mot de passe si demandÃ©)..."
            local ssh_cmd="ssh -tt -o StrictHostKeyChecking=no -o ConnectTimeout=${timeout} -p \"${target_port}\" \"${ansible_user}@${target_host}\" \"sudo ufw allow \\\"${port}/udp\\\"\""
            log "DEBUG" "ExÃ©cution de la commande avec eval: ${ssh_cmd}"
            eval "${ssh_cmd}"
            if [ $? -ne 0 ]; then
                log "WARNING" "Impossible d'ouvrir le port ${port}/udp sur le VPS"
                # Ne pas Ã©chouer pour UDP, car certains services n'utilisent que TCP
            fi
        fi

        log "SUCCESS" "Port ${port} ouvert avec succÃ¨s"
    done

    # Rechargement des rÃ¨gles UFW
    log "INFO" "Rechargement des rÃ¨gles UFW (commande interactive, veuillez entrer votre mot de passe si demandÃ©)..."

    if [[ "${IS_LOCAL_EXECUTION}" == "true" ]]; then
        # ExÃ©cution locale
        if ! sudo ufw reload; then
            log "WARNING" "Impossible de recharger les rÃ¨gles UFW"
            # Ne pas Ã©chouer pour le rechargement, car les rÃ¨gles sont dÃ©jÃ  appliquÃ©es
        fi
    else
        # ExÃ©cution distante
        local ssh_cmd="ssh -tt -o StrictHostKeyChecking=no -o ConnectTimeout=${timeout} -p \"${target_port}\" \"${ansible_user}@${target_host}\" \"sudo ufw reload\""
        log "DEBUG" "ExÃ©cution de la commande avec eval: ${ssh_cmd}"
        eval "${ssh_cmd}"
        if [ $? -ne 0 ]; then
            log "WARNING" "Impossible de recharger les rÃ¨gles UFW"
            # Ne pas Ã©chouer pour le rechargement, car les rÃ¨gles sont dÃ©jÃ  appliquÃ©es
        fi
    fi

    # VÃ©rification que les ports sont bien ouverts
    log "INFO" "VÃ©rification que les ports sont bien ouverts..."
    local failed_ports=()

    for port in "${ports_to_open[@]}"; do
        log "INFO" "VÃ©rification que le port ${port} est bien ouvert (commande interactive, veuillez entrer votre mot de passe si demandÃ©)..."

        if [[ "${IS_LOCAL_EXECUTION}" == "true" ]]; then
            # ExÃ©cution locale
            if ! sudo ufw status | grep -E "^${port}/(tcp|udp)" &>/dev/null; then
                log "WARNING" "Le port ${port} ne semble pas Ãªtre correctement ouvert dans UFW"
                failed_ports+=("${port}")
                success=false
            fi
        else
            # ExÃ©cution distante
            local ssh_cmd="ssh -tt -o StrictHostKeyChecking=no -o ConnectTimeout=5 -p \"${target_port}\" \"${ansible_user}@${target_host}\" \"sudo ufw status | grep -E \\\"^${port}/(tcp|udp)\\\"\""
            log "DEBUG" "ExÃ©cution de la commande avec eval: ${ssh_cmd}"
            eval "${ssh_cmd}"
            if [ $? -ne 0 ]; then
                log "WARNING" "Le port ${port} ne semble pas Ãªtre correctement ouvert dans UFW"
                failed_ports+=("${port}")
                success=false
            fi
        fi
    done

    if [[ ${#failed_ports[@]} -gt 0 ]]; then
        log "WARNING" "Les ports suivants n'ont pas pu Ãªtre ouverts: ${failed_ports[*]}"
    fi

    # Affichage du statut UFW
    log "INFO" "RÃ©cupÃ©ration du statut UFW (commande interactive, veuillez entrer votre mot de passe si demandÃ©)..."
    local ufw_status=""

    if [[ "${IS_LOCAL_EXECUTION}" == "true" ]]; then
        # ExÃ©cution locale
        ufw_status=$(sudo ufw status || echo "Impossible de rÃ©cupÃ©rer le statut UFW")
    else
        # ExÃ©cution distante
        local ssh_cmd="ssh -tt -o StrictHostKeyChecking=no -o ConnectTimeout=5 -p \"${target_port}\" \"${ansible_user}@${target_host}\" \"sudo ufw status\""
        log "DEBUG" "ExÃ©cution de la commande avec eval: ${ssh_cmd}"
        ufw_status=$(eval "${ssh_cmd}" || echo "Impossible de rÃ©cupÃ©rer le statut UFW")
    fi

    log "INFO" "Statut UFW actuel:"
    echo "${ufw_status}"

    # VÃ©rification finale que UFW est bien actif
    if ! echo "${ufw_status}" | grep -q "Status: active"; then
        log "WARNING" "UFW n'est pas actif aprÃ¨s toutes les opÃ©rations, tentative finale d'activation..."

        if [[ "${IS_LOCAL_EXECUTION}" == "true" ]]; then
            # ExÃ©cution locale
            if ! (echo 'y' | sudo ufw --force enable && sudo systemctl restart ufw); then
                log "ERROR" "Impossible d'activer UFW malgrÃ© plusieurs tentatives"
            else
                log "SUCCESS" "UFW est maintenant actif aprÃ¨s la tentative finale"
                # Affichage du statut mis Ã  jour
                ufw_status=$(sudo ufw status || echo "Impossible de rÃ©cupÃ©rer le statut UFW")
                log "INFO" "Statut UFW mis Ã  jour:"
                echo "${ufw_status}"
            fi
        else
            # ExÃ©cution distante
            local ssh_cmd="ssh -t -o StrictHostKeyChecking=no -o ConnectTimeout=${timeout} -p \"${target_port}\" \"${ansible_user}@${target_host}\" \"echo 'y' | sudo ufw --force enable && sudo systemctl restart ufw\""
            log "DEBUG" "ExÃ©cution de la commande avec eval: ${ssh_cmd}"
            eval "${ssh_cmd}"

            # Affichage du statut mis Ã  jour
            local ssh_cmd="ssh -tt -o StrictHostKeyChecking=no -o ConnectTimeout=5 -p \"${target_port}\" \"${ansible_user}@${target_host}\" \"sudo ufw status\""
            ufw_status=$(eval "${ssh_cmd}" || echo "Impossible de rÃ©cupÃ©rer le statut UFW")
            log "INFO" "Statut UFW mis Ã  jour:"
            echo "${ufw_status}"

            if ! echo "${ufw_status}" | grep -q "Status: active"; then
                log "ERROR" "Impossible d'activer UFW malgrÃ© plusieurs tentatives"
            else
                log "SUCCESS" "UFW est maintenant actif aprÃ¨s la tentative finale"
            fi
        fi
    fi

    if [[ "${success}" == "true" ]]; then
        log "SUCCESS" "Tous les ports ont Ã©tÃ© ouverts avec succÃ¨s"
        return 0
    else
        log "WARNING" "Certains ports n'ont pas pu Ãªtre ouverts"
        return 1
    fi
}

# Fonction pour vÃ©rifier la connectivitÃ© rÃ©seau de maniÃ¨re approfondie
function check_network() {
    local target_host="${ansible_host}"
    local target_port="${ansible_port}"
    local retry_count=3
    local timeout=5
    local success=false

    # Si le script est exÃ©cutÃ© sur le VPS cible, pas besoin de vÃ©rifier la connectivitÃ© rÃ©seau
    if [[ "${IS_LOCAL_EXECUTION}" == "true" ]]; then
        log "INFO" "ExÃ©cution locale dÃ©tectÃ©e, vÃ©rification de la connectivitÃ© rÃ©seau ignorÃ©e"
        return 0
    fi

    log "INFO" "VÃ©rification approfondie de la connectivitÃ© rÃ©seau vers ${target_host}..."

    if [[ -z "${target_host}" ]]; then
        log "ERROR" "Impossible de dÃ©terminer l'adresse du VPS"
        return 1
    fi

    # VÃ©rification de la rÃ©solution DNS
    log "INFO" "VÃ©rification de la rÃ©solution DNS pour ${target_host}..."
    if [[ "${target_host}" =~ ^[0-9]+\.[0-9]+\.[0-9]+\.[0-9]+$ ]]; then
        log "INFO" "L'adresse ${target_host} est une adresse IP, pas besoin de rÃ©solution DNS"
    else
        # Tentative de rÃ©solution DNS
        local resolved_ip=""
        for ((i=1; i<=retry_count; i++)); do
            resolved_ip=$(dig +short "${target_host}" 2>/dev/null || host "${target_host}" 2>/dev/null | grep "has address" | awk '{print $4}' || nslookup "${target_host}" 2>/dev/null | grep "Address:" | tail -n1 | awk '{print $2}')

            if [[ -n "${resolved_ip}" ]]; then
                log "INFO" "RÃ©solution DNS rÃ©ussie: ${target_host} -> ${resolved_ip}"
                success=true
                break
            else
                log "WARNING" "Tentative ${i}/${retry_count}: Ã‰chec de la rÃ©solution DNS pour ${target_host}"
                sleep 2
            fi
        done

        if [[ "${success}" != "true" ]]; then
            log "ERROR" "Impossible de rÃ©soudre l'adresse DNS pour ${target_host}"
            log "ERROR" "VÃ©rifiez votre connexion Internet et la configuration DNS"

            # VÃ©rification des serveurs DNS
            log "INFO" "VÃ©rification des serveurs DNS..."
            local dns_servers=$(cat /etc/resolv.conf | grep "nameserver" | awk '{print $2}')

            if [[ -z "${dns_servers}" ]]; then
                log "ERROR" "Aucun serveur DNS configurÃ©"
            else
                log "INFO" "Serveurs DNS configurÃ©s: ${dns_servers}"

                # Test de connectivitÃ© vers les serveurs DNS
                for dns in ${dns_servers}; do
                    if ping -c 1 -W 5 "${dns}" &>/dev/null; then
                        log "INFO" "Serveur DNS ${dns} accessible"
                    else
                        log "WARNING" "Serveur DNS ${dns} inaccessible"
                    fi
                done
            fi

            # Suggestion de solution
            log "INFO" "Suggestions:"
            log "INFO" "1. VÃ©rifiez votre connexion Internet"
            log "INFO" "2. VÃ©rifiez que le nom d'hÃ´te ${target_host} est correct"
            log "INFO" "3. Essayez d'utiliser une adresse IP directement dans le fichier d'inventaire"

            return 1
        fi
    fi

    # VÃ©rification de la connectivitÃ© ICMP (ping)
    log "INFO" "VÃ©rification de la connectivitÃ© ICMP vers ${target_host}..."
    success=false

    for ((i=1; i<=retry_count; i++)); do
        if ping -c 3 -W ${timeout} "${target_host}" &>/dev/null; then
            log "INFO" "ConnectivitÃ© ICMP vers ${target_host} vÃ©rifiÃ©e avec succÃ¨s"
            success=true
            break
        else
            log "WARNING" "Tentative ${i}/${retry_count}: Ã‰chec de la connectivitÃ© ICMP vers ${target_host}"
            sleep 2
        fi
    done

    if [[ "${success}" != "true" ]]; then
        log "WARNING" "Impossible de joindre le VPS par ICMP (ping) Ã  l'adresse ${target_host}"
        log "WARNING" "Le pare-feu du VPS bloque peut-Ãªtre les pings, tentative de connexion TCP..."
    fi

    # VÃ©rification de la connectivitÃ© TCP (SSH)
    log "INFO" "VÃ©rification de la connectivitÃ© TCP (SSH) vers ${target_host}:${target_port}..."
    success=false

    for ((i=1; i<=retry_count; i++)); do
        if nc -z -w ${timeout} "${target_host}" "${target_port}" &>/dev/null; then
            log "INFO" "ConnectivitÃ© TCP (SSH) vers ${target_host}:${target_port} vÃ©rifiÃ©e avec succÃ¨s"
            success=true
            break
        else
            log "WARNING" "Tentative ${i}/${retry_count}: Ã‰chec de la connectivitÃ© TCP vers ${target_host}:${target_port}"
            sleep 2
        fi
    done

    if [[ "${success}" != "true" ]]; then
        log "ERROR" "Impossible de joindre le VPS par TCP (SSH) Ã  l'adresse ${target_host}:${target_port}"
        log "ERROR" "VÃ©rifiez que le VPS est en ligne et que le port SSH est ouvert"

        # VÃ©rification de la route rÃ©seau
        log "INFO" "VÃ©rification de la route rÃ©seau vers ${target_host}..."
        local traceroute_output=$(traceroute -m 15 "${target_host}" 2>/dev/null || tracepath -m 15 "${target_host}" 2>/dev/null || true)

        if [[ -n "${traceroute_output}" ]]; then
            log "INFO" "Route rÃ©seau vers ${target_host}:"
            echo "${traceroute_output}" | head -10
        else
            log "WARNING" "Impossible de dÃ©terminer la route rÃ©seau vers ${target_host}"
        fi

        # Suggestion de solution
        log "INFO" "Suggestions:"
        log "INFO" "1. VÃ©rifiez que le VPS est en ligne"
        log "INFO" "2. VÃ©rifiez que le port SSH (${target_port}) est ouvert sur le VPS"
        log "INFO" "3. VÃ©rifiez les rÃ¨gles de pare-feu sur le VPS et sur votre rÃ©seau local"

        return 1
    fi

    # VÃ©rification des ports requis
    log "INFO" "VÃ©rification des ports requis sur ${target_host}..."
    local open_ports=()
    local closed_ports=()

    for port in "${REQUIRED_PORTS[@]}"; do
        # Si le port est le port SSH et que nous avons dÃ©jÃ  vÃ©rifiÃ© la connectivitÃ© SSH, le considÃ©rer comme ouvert
        if [[ "${port}" == "${target_port}" ]]; then
            log "INFO" "Port ${port} (SSH) accessible sur ${target_host} (dÃ©jÃ  vÃ©rifiÃ©)"
            open_ports+=("${port}")
            continue
        fi

        # Augmenter le timeout pour les vÃ©rifications de port
        if nc -z -w $((timeout*2)) "${target_host}" "${port}" &>/dev/null; then
            log "INFO" "Port ${port} accessible sur ${target_host}"
            open_ports+=("${port}")
        else
            # DeuxiÃ¨me tentative avec un dÃ©lai
            sleep 1
            if nc -z -w $((timeout*2)) "${target_host}" "${port}" &>/dev/null; then
                log "INFO" "Port ${port} accessible sur ${target_host} (deuxiÃ¨me tentative)"
                open_ports+=("${port}")
            else
                log "WARNING" "Port ${port} non accessible sur ${target_host}"
                closed_ports+=("${port}")
            fi
        fi
        # Ajout d'un petit dÃ©lai entre chaque vÃ©rification de port pour Ã©viter les problÃ¨mes d'affichage
        sleep 0.1
    done

    # RÃ©sumÃ© des ports
    if [[ ${#open_ports[@]} -eq ${#REQUIRED_PORTS[@]} ]]; then
        log "SUCCESS" "Tous les ports requis sont accessibles sur ${target_host}"
    else
        log "WARNING" "Certains ports requis ne sont pas accessibles sur ${target_host}"
        # Utilisation d'IFS pour formater les listes de ports avec des virgules
        local IFS=","
        log "INFO" "Ports ouverts: ${open_ports[*]}"
        # Ajout d'un dÃ©lai pour s'assurer que les messages sont affichÃ©s sÃ©parÃ©ment
        sleep 0.1
        log "WARNING" "Ports fermÃ©s: ${closed_ports[*]}"
        # Restauration de l'IFS par dÃ©faut
        unset IFS

        # Ouverture automatique des ports requis sans demander Ã  l'utilisateur
        log "INFO" "Des ports requis sont fermÃ©s. Ouverture automatique des ports..."

        # DÃ©finir answer comme "o" pour toujours ouvrir les ports automatiquement
        answer="o"

        if [[ "${answer}" =~ ^[Oo]$ ]]; then
            # Tentative d'ouverture des ports fermÃ©s
            log "INFO" "Tentative d'ouverture automatique des ports fermÃ©s..."
            if open_required_ports "${closed_ports[@]}"; then
                log "SUCCESS" "Ports ouverts avec succÃ¨s"

                # VÃ©rification que les ports sont maintenant accessibles
                local still_closed_ports=()
                for port in "${closed_ports[@]}"; do
                    if ! nc -z -w ${timeout} "${target_host}" "${port}" &>/dev/null; then
                        still_closed_ports+=("${port}")
                    else
                        open_ports+=("${port}")
                    fi
                done

                if [[ ${#still_closed_ports[@]} -eq 0 ]]; then
                    log "SUCCESS" "Tous les ports sont maintenant accessibles"
                    closed_ports=()
                else
                    log "WARNING" "Certains ports sont toujours inaccessibles malgrÃ© l'ouverture dans le pare-feu"
                    log "WARNING" "Cela peut Ãªtre dÃ» Ã  un pare-feu externe ou Ã  des services non dÃ©marrÃ©s"
                    closed_ports=("${still_closed_ports[@]}")
                    # Utilisation d'IFS pour formater la liste des ports avec des virgules
                    local IFS=","
                    log "INFO" "Ports toujours fermÃ©s: ${closed_ports[*]}"
                    # Restauration de l'IFS par dÃ©faut
                    unset IFS
                    # Ajout d'un dÃ©lai pour s'assurer que les messages suivants sont affichÃ©s sÃ©parÃ©ment
                    sleep 0.1
                fi
            else
                log "WARNING" "Impossible d'ouvrir automatiquement certains ports"
                log "WARNING" "Vous devrez peut-Ãªtre les ouvrir manuellement"
            fi
        else
            log "INFO" "Ouverture automatique des ports annulÃ©e par l'utilisateur"
        fi

        # VÃ©rification si le port SSH est ouvert (seul port vraiment essentiel)
        if ! nc -z -w ${timeout} "${target_host}" "${target_port}" &>/dev/null; then
            log "ERROR" "Le port SSH (${target_port}) n'est pas accessible, impossible de continuer"
            log "INFO" "Suggestions:"
            log "INFO" "1. VÃ©rifiez les rÃ¨gles de pare-feu sur le VPS"
            log "INFO" "2. VÃ©rifiez que le service SSH est en cours d'exÃ©cution sur le VPS"
            return 1
        else
            log "WARNING" "Certains ports non essentiels ne sont pas accessibles, l'installation peut continuer mais certaines fonctionnalitÃ©s pourraient ne pas fonctionner correctement"
            # Continuer automatiquement si seuls des ports non essentiels sont inaccessibles
            log "INFO" "Continuation automatique de l'installation..."
        fi
    fi

    # VÃ©rification de la latence rÃ©seau
    log "INFO" "VÃ©rification de la latence rÃ©seau vers ${target_host}..."
    local ping_output=$(ping -c 5 -W ${timeout} "${target_host}" 2>/dev/null || echo "Ping failed")
    local avg_latency=$(echo "${ping_output}" | grep "avg" | awk -F'/' '{print $5}')

    if [[ -n "${avg_latency}" ]]; then
        log "INFO" "Latence moyenne vers ${target_host}: ${avg_latency} ms"

        if (( $(echo "${avg_latency} > 300" | bc -l) )); then
            log "WARNING" "Latence Ã©levÃ©e vers ${target_host}, les performances peuvent Ãªtre dÃ©gradÃ©es"
        fi
    else
        log "WARNING" "Impossible de mesurer la latence vers ${target_host}"
    fi

    log "SUCCESS" "VÃ©rification de la connectivitÃ© rÃ©seau terminÃ©e avec succÃ¨s"
    return 0
}

# Fonction pour sauvegarder l'Ã©tat avant modification
function backup_state() {
    local backup_name="${1:-backup-$(date +%Y%m%d-%H%M%S)}"
    local optional="${2:-false}"  # New parameter to make backup optional
    local backup_file="${BACKUP_DIR}/${backup_name}.tar.gz"
    local metadata_file="${BACKUP_DIR}/${backup_name}.json"

    log "INFO" "Sauvegarde de l'Ã©tat actuel dans ${backup_file}..."

    # CrÃ©ation du fichier de mÃ©tadonnÃ©es
    cat > "${metadata_file}" << EOF
{
  "backup_name": "${backup_name}",
  "backup_date": "$(date -Iseconds)",
  "environment": "${environment}",
  "installation_step": "${INSTALLATION_STEP}",
  "ansible_host": "${ansible_host}",
  "ansible_port": "${ansible_port}",
  "ansible_user": "${ansible_user}",
  "script_version": "1.0.0",
  "description": "Sauvegarde automatique avant l'Ã©tape ${INSTALLATION_STEP}"
}
EOF

    # Liste des rÃ©pertoires Ã  sauvegarder
    local backup_dirs=(
        "/etc/rancher"
        "/var/lib/rancher/k3s/server/manifests"
        "/home/${ansible_user}/.kube"
        "/etc/systemd/system/k3s.service"
        "/var/log/lions"
    )

    # Liste des fichiers Ã  exclure
    local exclude_patterns=(
        "*.log"
        "*.tmp"
        "*.bak"
        "*.old"
        "*.swp"
    )

    # Construction de la commande d'exclusion
    local exclude_args=""
    for pattern in "${exclude_patterns[@]}"; do
        exclude_args="${exclude_args} --exclude='${pattern}'"
    done

    # VÃ©rification de l'existence des rÃ©pertoires avant la sauvegarde
    local existing_dirs=()
    for dir in "${backup_dirs[@]}"; do
        log "DEBUG" "VÃ©rification de l'existence du rÃ©pertoire: ${dir}"

        if [[ "${IS_LOCAL_EXECUTION}" == "true" ]]; then
            # ExÃ©cution locale
            if sudo test -d "${dir}" 2>/dev/null; then
                existing_dirs+=("${dir}")
                log "DEBUG" "RÃ©pertoire trouvÃ© pour sauvegarde: ${dir}"
            else
                log "DEBUG" "RÃ©pertoire non trouvÃ© ou erreur d'accÃ¨s, ignorÃ© pour la sauvegarde: ${dir}"
            fi
        else
            # ExÃ©cution distante
            # Utilisation de run_with_timeout_fallback pour Ã©viter que la commande ne se bloque indÃ©finiment
            if run_with_timeout_fallback 10 ssh -o ConnectTimeout=5 -p "${ansible_port}" "${ansible_user}@${ansible_host}" "sudo test -d ${dir}" 2>/dev/null; then
                existing_dirs+=("${dir}")
                log "DEBUG" "RÃ©pertoire trouvÃ© pour sauvegarde: ${dir}"
            else
                log "DEBUG" "RÃ©pertoire non trouvÃ© ou erreur d'accÃ¨s, ignorÃ© pour la sauvegarde: ${dir}"
            fi
        fi
    done

    # Si aucun rÃ©pertoire n'existe, log un avertissement et retourne 0 si optionnel
    if [[ ${#existing_dirs[@]} -eq 0 ]]; then
        log "WARNING" "Aucun rÃ©pertoire Ã  sauvegarder n'existe encore sur le VPS"
        if [[ "${optional}" == "true" ]]; then
            log "INFO" "Sauvegarde ignorÃ©e (optionnelle)"
            rm -f "${metadata_file}"
            return 0
        else
            log "WARNING" "Impossible de crÃ©er une sauvegarde de l'Ã©tat actuel sur le VPS"
            rm -f "${metadata_file}"
            return 1
        fi
    fi

    # Construction de la commande de sauvegarde avec les rÃ©pertoires existants
    local backup_cmd="sudo tar -czf /tmp/${backup_name}.tar.gz ${exclude_args}"
    for dir in "${existing_dirs[@]}"; do
        backup_cmd="${backup_cmd} ${dir}"
    done
    backup_cmd="${backup_cmd} 2>/dev/null || true"

    # ExÃ©cution de la commande de sauvegarde
    if [[ "${IS_LOCAL_EXECUTION}" == "true" ]]; then
        # ExÃ©cution locale
        log "DEBUG" "ExÃ©cution locale de la commande de sauvegarde: ${backup_cmd}"

        # CrÃ©ation du rÃ©pertoire temporaire si nÃ©cessaire
        mkdir -p /tmp

        # ExÃ©cution de la commande de sauvegarde
        if eval "${backup_cmd}"; then
            log "DEBUG" "Commande de sauvegarde exÃ©cutÃ©e avec succÃ¨s, copie du fichier..."

            # Copie du fichier de sauvegarde
            if cp "/tmp/${backup_name}.tar.gz" "${backup_file}" 2>/dev/null; then
                log "DEBUG" "Fichier de sauvegarde copiÃ© avec succÃ¨s, nettoyage du fichier temporaire..."
                # Nettoyage du fichier temporaire
                sudo rm -f "/tmp/${backup_name}.tar.gz" 2>/dev/null || log "WARNING" "Impossible de supprimer le fichier temporaire"
            else
                log "ERROR" "Impossible de copier le fichier de sauvegarde"
                rm -f "${metadata_file}"
                return 1
            fi
        else
            log "ERROR" "Ã‰chec de la commande de sauvegarde"
            rm -f "${metadata_file}"
            return 1
        fi
    else
        # ExÃ©cution distante
        log "DEBUG" "ExÃ©cution de la commande de sauvegarde: ${backup_cmd}"
        if run_with_timeout_fallback 60 ssh -o ConnectTimeout=10 -p "${ansible_port}" "${ansible_user}@${ansible_host}" "${backup_cmd}"; then
            log "DEBUG" "Commande de sauvegarde exÃ©cutÃ©e avec succÃ¨s, rÃ©cupÃ©ration du fichier..."
            # RÃ©cupÃ©ration du fichier de sauvegarde avec timeout
            if run_with_timeout_fallback 60 scp -o ConnectTimeout=10 -P "${ansible_port}" "${ansible_user}@${ansible_host}:/tmp/${backup_name}.tar.gz" "${backup_file}" 2>/dev/null; then
                log "DEBUG" "Fichier de sauvegarde rÃ©cupÃ©rÃ© avec succÃ¨s, nettoyage du fichier temporaire..."
                # Nettoyage du fichier temporaire sur le VPS
                run_with_timeout_fallback 10 ssh -o ConnectTimeout=5 -p "${ansible_port}" "${ansible_user}@${ansible_host}" "sudo rm -f /tmp/${backup_name}.tar.gz" 2>/dev/null || log "WARNING" "Impossible de supprimer le fichier temporaire sur le VPS"
            else
                log "ERROR" "Impossible de rÃ©cupÃ©rer le fichier de sauvegarde"
                rm -f "${metadata_file}"
                return 1
            fi
        else
            log "ERROR" "Ã‰chec de la commande de sauvegarde"
            rm -f "${metadata_file}"
            return 1
        fi
    fi

    # VÃ©rification de la taille du fichier de sauvegarde
    local backup_size=$(du -h "${backup_file}" | awk '{print $1}')

    # Ajout de la taille du fichier aux mÃ©tadonnÃ©es
    local tmp_file=$(mktemp)
    jq ".backup_size = \"${backup_size}\"" "${metadata_file}" > "${tmp_file}" && mv "${tmp_file}" "${metadata_file}"

    log "SUCCESS" "Sauvegarde de l'Ã©tat crÃ©Ã©e: ${backup_file} (${backup_size})"

    # Nettoyage des anciennes sauvegardes (garder les 5 plus rÃ©centes)
    local old_backups=($(ls -t "${BACKUP_DIR}"/*.tar.gz 2>/dev/null | tail -n +6))
    if [[ ${#old_backups[@]} -gt 0 ]]; then
        log "INFO" "Nettoyage des anciennes sauvegardes..."
        for old_backup in "${old_backups[@]}"; do
            local old_name=$(basename "${old_backup}" .tar.gz)
            rm -f "${old_backup}" "${BACKUP_DIR}/${old_name}.json"
            log "INFO" "Sauvegarde supprimÃ©e: ${old_backup}"
        done
    fi

    # Enregistrement du nom de la derniÃ¨re sauvegarde
    echo "${backup_name}" > "${BACKUP_DIR}/.last_backup"

    return 0
}

# Fonction pour restaurer l'Ã©tat Ã  partir d'une sauvegarde
function restore_state() {
    local backup_name="$1"

    # Si aucun nom de sauvegarde n'est fourni, utiliser la derniÃ¨re sauvegarde
    if [[ -z "${backup_name}" && -f "${BACKUP_DIR}/.last_backup" ]]; then
        backup_name=$(cat "${BACKUP_DIR}/.last_backup")
    fi

    # VÃ©rification de l'existence de la sauvegarde
    if [[ -z "${backup_name}" || ! -f "${BACKUP_DIR}/${backup_name}.tar.gz" ]]; then
        log "ERROR" "Sauvegarde non trouvÃ©e: ${backup_name}"
        return 1
    fi

    local backup_file="${BACKUP_DIR}/${backup_name}.tar.gz"
    local metadata_file="${BACKUP_DIR}/${backup_name}.json"

    log "INFO" "Restauration de l'Ã©tat Ã  partir de ${backup_file}..."

    # Lecture des mÃ©tadonnÃ©es
    if [[ -f "${metadata_file}" ]]; then
        local backup_date
        backup_date=$(jq -r '.backup_date' "${metadata_file}")
        local backup_step
        backup_step=$(jq -r '.installation_step' "${metadata_file}")
        local backup_env
        backup_env=$(jq -r '.environment' "${metadata_file}")

        log "INFO" "Sauvegarde du ${backup_date}, Ã©tape: ${backup_step}, environnement: ${backup_env}"

        # VÃ©rification de la compatibilitÃ© de l'environnement
        if [[ "${backup_env}" != "${environment}" ]]; then
            log "WARNING" "L'environnement de la sauvegarde (${backup_env}) ne correspond pas Ã  l'environnement actuel (${environment})"
            log "INFO" "Voulez-vous continuer malgrÃ© tout? (o/N)"
            read -r answer
            if [[ ! "${answer}" =~ ^[Oo]$ ]]; then
                return 1
            fi
        fi
    else
        log "WARNING" "Fichier de mÃ©tadonnÃ©es non trouvÃ©: ${metadata_file}"
        log "INFO" "Voulez-vous continuer malgrÃ© tout? (o/N)"
        read -r answer
        if [[ ! "${answer}" =~ ^[Oo]$ ]]; then
            return 1
        fi
    fi

    # PrÃ©paration pour la restauration
    if [[ "${IS_LOCAL_EXECUTION}" == "true" ]]; then
        # ExÃ©cution locale - pas besoin de copier le fichier
        log "INFO" "ExÃ©cution locale dÃ©tectÃ©e, pas besoin de copier le fichier de sauvegarde"

        # CrÃ©ation d'un lien symbolique vers le fichier de sauvegarde pour simplifier la suite
        if ! ln -sf "${backup_file}" "/tmp/${backup_name}.tar.gz" 2>/dev/null; then
            log "WARNING" "Impossible de crÃ©er un lien symbolique, copie du fichier Ã  la place..."
            if ! cp "${backup_file}" "/tmp/${backup_name}.tar.gz" 2>/dev/null; then
                log "ERROR" "Impossible de copier le fichier de sauvegarde localement"
                return 1
            fi
        fi
    else
        # ExÃ©cution distante - copie du fichier vers le VPS
        log "INFO" "Copie du fichier de sauvegarde vers le VPS..."
        log "DEBUG" "Taille du fichier de sauvegarde: $(du -h "${backup_file}" | awk '{print $1}')"
        if ! run_with_timeout_fallback 60 scp -o ConnectTimeout=10 -P "${ansible_port}" "${backup_file}" "${ansible_user}@${ansible_host}:/tmp/${backup_name}.tar.gz" 2>/dev/null; then
            log "ERROR" "Impossible de copier le fichier de sauvegarde vers le VPS"
            log "DEBUG" "VÃ©rification de l'espace disque disponible sur le VPS..."
            run_with_timeout_fallback 10 ssh -o ConnectTimeout=5 -p "${ansible_port}" "${ansible_user}@${ansible_host}" "df -h /tmp" 2>/dev/null || log "DEBUG" "Impossible de vÃ©rifier l'espace disque sur le VPS"
            return 1
        fi

        # VÃ©rification que le fichier a bien Ã©tÃ© copiÃ©
        log "DEBUG" "VÃ©rification que le fichier a bien Ã©tÃ© copiÃ© sur le VPS..."
        if ! run_with_timeout_fallback 10 ssh -o ConnectTimeout=5 -p "${ansible_port}" "${ansible_user}@${ansible_host}" "ls -la /tmp/${backup_name}.tar.gz" 2>/dev/null; then
            log "ERROR" "Le fichier de sauvegarde n'a pas Ã©tÃ© correctement copiÃ© sur le VPS"
            return 1
        fi
    fi

    # ArrÃªt des services avant restauration
    log "INFO" "ArrÃªt des services avant restauration..."
    if [[ "${IS_LOCAL_EXECUTION}" == "true" ]]; then
        # ExÃ©cution locale
        if ! sudo systemctl stop k3s 2>/dev/null; then
            log "WARNING" "Impossible d'arrÃªter le service K3s, tentative de restauration quand mÃªme"
        fi
    else
        # ExÃ©cution distante
        if ! run_with_timeout_fallback 30 ssh -o ConnectTimeout=10 -p "${ansible_port}" "${ansible_user}@${ansible_host}" "sudo systemctl stop k3s || true" 2>/dev/null; then
            log "WARNING" "Impossible d'arrÃªter le service K3s, tentative de restauration quand mÃªme"
        fi
    fi

    # Restauration des fichiers
    log "INFO" "Restauration des fichiers..."
    if [[ "${IS_LOCAL_EXECUTION}" == "true" ]]; then
        # ExÃ©cution locale
        if ! sudo tar -xzf "/tmp/${backup_name}.tar.gz" -C / 2>/dev/null; then
            log "ERROR" "Ã‰chec de la restauration des fichiers"
            log "DEBUG" "VÃ©rification du contenu de l'archive..."
            sudo tar -tvf "/tmp/${backup_name}.tar.gz" | head -10 2>/dev/null || log "DEBUG" "Impossible de lister le contenu de l'archive"

            # RedÃ©marrage des services en cas d'Ã©chec
            log "INFO" "Tentative de redÃ©marrage des services aprÃ¨s Ã©chec..."
            sudo systemctl start k3s 2>/dev/null || true

            return 1
        fi
    else
        # ExÃ©cution distante
        if ! run_with_timeout_fallback 60 ssh -o ConnectTimeout=30 -p "${ansible_port}" "${ansible_user}@${ansible_host}" "sudo tar -xzf /tmp/${backup_name}.tar.gz -C / 2>/dev/null"; then
            log "ERROR" "Ã‰chec de la restauration des fichiers"
            log "DEBUG" "VÃ©rification du contenu de l'archive..."
            run_with_timeout_fallback 10 ssh -o ConnectTimeout=5 -p "${ansible_port}" "${ansible_user}@${ansible_host}" "sudo tar -tvf /tmp/${backup_name}.tar.gz | head -10" 2>/dev/null || log "DEBUG" "Impossible de lister le contenu de l'archive"

            # RedÃ©marrage des services en cas d'Ã©chec
            log "INFO" "Tentative de redÃ©marrage des services aprÃ¨s Ã©chec..."
            run_with_timeout_fallback 30 ssh -o ConnectTimeout=10 -p "${ansible_port}" "${ansible_user}@${ansible_host}" "sudo systemctl start k3s || true" 2>/dev/null

            return 1
        fi
    fi

    # Nettoyage du fichier temporaire
    if [[ "${IS_LOCAL_EXECUTION}" == "true" ]]; then
        # ExÃ©cution locale
        log "DEBUG" "Nettoyage du fichier temporaire local..."
        sudo rm -f "/tmp/${backup_name}.tar.gz" 2>/dev/null || log "WARNING" "Impossible de supprimer le fichier temporaire local"
    else
        # ExÃ©cution distante
        log "DEBUG" "Nettoyage du fichier temporaire sur le VPS..."
        run_with_timeout_fallback 10 ssh -o ConnectTimeout=5 -p "${ansible_port}" "${ansible_user}@${ansible_host}" "sudo rm -f /tmp/${backup_name}.tar.gz" 2>/dev/null || log "WARNING" "Impossible de supprimer le fichier temporaire sur le VPS"
    fi

    # RedÃ©marrage des services
    log "INFO" "RedÃ©marrage des services..."
    if [[ "${IS_LOCAL_EXECUTION}" == "true" ]]; then
        # ExÃ©cution locale
        if ! sudo systemctl daemon-reload && sudo systemctl start k3s; then
            log "WARNING" "Ã‰chec du redÃ©marrage des services"
            log "DEBUG" "VÃ©rification de l'Ã©tat du service K3s..."
            sudo systemctl status k3s 2>/dev/null || log "DEBUG" "Impossible de vÃ©rifier l'Ã©tat du service K3s"
            log "WARNING" "Vous devrez peut-Ãªtre redÃ©marrer manuellement le systÃ¨me"
            return 1
        fi
    else
        # ExÃ©cution distante
        if ! run_with_timeout_fallback 60 ssh -o ConnectTimeout=30 -p "${ansible_port}" "${ansible_user}@${ansible_host}" "sudo systemctl daemon-reload && sudo systemctl start k3s"; then
            log "WARNING" "Ã‰chec du redÃ©marrage des services"
            log "DEBUG" "VÃ©rification de l'Ã©tat du service K3s..."
            run_with_timeout_fallback 10 ssh -o ConnectTimeout=5 -p "${ansible_port}" "${ansible_user}@${ansible_host}" "sudo systemctl status k3s" 2>/dev/null || log "DEBUG" "Impossible de vÃ©rifier l'Ã©tat du service K3s"
            log "WARNING" "Vous devrez peut-Ãªtre redÃ©marrer manuellement le VPS"
            return 1
        fi
    fi

    # Attente que K3s soit prÃªt
    log "INFO" "Attente que K3s soit prÃªt..."
    local k3s_timeout=120  # Augmentation du timeout Ã  2 minutes
    local start_time
    start_time=$(date +%s)
    local k3s_ready=false
    local check_count=0

    while [[ "${k3s_ready}" == "false" ]]; do
        local current_time
        current_time=$(date +%s)
        local elapsed_time
        elapsed_time=$((current_time - start_time))

        if [[ ${elapsed_time} -gt ${k3s_timeout} ]]; then
            log "WARNING" "Timeout atteint en attendant que K3s soit prÃªt"
            log "DEBUG" "VÃ©rification des logs de K3s..."

            if [[ "${IS_LOCAL_EXECUTION}" == "true" ]]; then
                # ExÃ©cution locale
                sudo journalctl -u k3s --no-pager -n 20 2>/dev/null || log "DEBUG" "Impossible de rÃ©cupÃ©rer les logs de K3s"
            else
                # ExÃ©cution distante
                run_with_timeout_fallback 10 ssh -o ConnectTimeout=5 -p "${ansible_port}" "${ansible_user}@${ansible_host}" "sudo journalctl -u k3s --no-pager -n 20" 2>/dev/null || log "DEBUG" "Impossible de rÃ©cupÃ©rer les logs de K3s"
            fi

            break
        fi

        # Toutes les 3 tentatives, afficher plus d'informations de diagnostic
        if [[ $((check_count % 3)) -eq 0 ]]; then
            log "DEBUG" "VÃ©rification de l'Ã©tat du service K3s (tentative ${check_count})..."

            if [[ "${IS_LOCAL_EXECUTION}" == "true" ]]; then
                # ExÃ©cution locale
                sudo systemctl is-active k3s 2>/dev/null || log "DEBUG" "Service K3s non actif"
            else
                # ExÃ©cution distante
                run_with_timeout_fallback 10 ssh -o ConnectTimeout=5 -p "${ansible_port}" "${ansible_user}@${ansible_host}" "sudo systemctl is-active k3s" 2>/dev/null || log "DEBUG" "Service K3s non actif"
            fi
        fi

        if [[ "${IS_LOCAL_EXECUTION}" == "true" ]]; then
            # ExÃ©cution locale
            if sudo kubectl get nodes 2>/dev/null; then
                k3s_ready=true
                log "SUCCESS" "K3s est prÃªt"
                # Afficher les nÅ“uds pour confirmation
                log "DEBUG" "NÅ“uds K3s disponibles:"
                sudo kubectl get nodes 2>/dev/null || log "DEBUG" "Impossible de lister les nÅ“uds K3s"
            else
                log "INFO" "En attente que K3s soit prÃªt... (${elapsed_time}s)"
                check_count=$((check_count + 1))
                sleep 5
            fi
        else
            # ExÃ©cution distante
            if run_with_timeout_fallback 15 ssh -o ConnectTimeout=5 -p "${ansible_port}" "${ansible_user}@${ansible_host}" "sudo kubectl get nodes" 2>/dev/null; then
                k3s_ready=true
                log "SUCCESS" "K3s est prÃªt"
                # Afficher les nÅ“uds pour confirmation
                log "DEBUG" "NÅ“uds K3s disponibles:"
                run_with_timeout_fallback 10 ssh -o ConnectTimeout=5 -p "${ansible_port}" "${ansible_user}@${ansible_host}" "sudo kubectl get nodes" 2>/dev/null || log "DEBUG" "Impossible de lister les nÅ“uds K3s"
            else
                log "INFO" "En attente que K3s soit prÃªt... (${elapsed_time}s)"
                check_count=$((check_count + 1))
                sleep 5
            fi
        fi
    done

    log "SUCCESS" "Restauration terminÃ©e avec succÃ¨s"

    # Mise Ã  jour de l'Ã©tat actuel
    if [[ -f "${metadata_file}" ]]; then
        local backup_step
        backup_step=$(jq -r '.installation_step' "${metadata_file}")
        INSTALLATION_STEP="${backup_step}"
        echo "${INSTALLATION_STEP}" > "${STATE_FILE}"
        log "INFO" "Ã‰tat actuel mis Ã  jour: ${INSTALLATION_STEP}"
    fi

    return 0
}

# Fonction pour exÃ©cuter une commande avec timeout
function run_with_timeout() {
    local cmd_str="$1"
    local timeout="${2:-${TIMEOUT_SECONDS}}"
    local cmd_type="${3:-generic}"
    local max_retries=3
    local retry_count=0
    local backoff_time=5
    local interactive=false

    # VÃ©rifier si la commande est interactive (nÃ©cessite une entrÃ©e utilisateur)
    # Ne pas considÃ©rer --ask-become-pass comme interactif si on est en exÃ©cution locale
    if [[ ("${cmd_str}" == *"--ask-become-pass"* && "${IS_LOCAL_EXECUTION}" != "true") || "${cmd_str}" == *"--ask-pass"* || "${cmd_str}" == *"-K"* || "${cmd_str}" == *"-k"* ]]; then
        interactive=true
        log "INFO" "Commande interactive dÃ©tectÃ©e, l'entrÃ©e utilisateur sera requise"
    fi

    log "INFO" "ExÃ©cution de la commande avec timeout ${timeout}s: ${cmd_str}"
    LAST_COMMAND="${cmd_str}"

    # DÃ©finition du type de commande pour la gestion des erreurs
    COMMAND_NAME="${cmd_type}"

    # Sauvegarde de l'Ã©tat avant l'exÃ©cution pour permettre une reprise
    echo "${INSTALLATION_STEP}" > "${STATE_FILE}"

    # Fonction pour vÃ©rifier si l'erreur est liÃ©e au rÃ©seau
    function is_network_error() {
        local output="$1"
        local exit_code="$2"

        # Codes d'erreur typiques des problÃ¨mes rÃ©seau
        if [[ ${exit_code} -eq 124 || ${exit_code} -eq 255 ]]; then
            return 0
        fi

        # Messages d'erreur typiques des problÃ¨mes rÃ©seau
        if echo "${output}" | grep -q -E "Connection refused|Connection timed out|Network is unreachable|Unable to connect|Connection reset by peer|Temporary failure in name resolution|Could not resolve host|Network error"; then
            return 0
        fi

        return 1
    }

    while true; do
        # VÃ©rification de la connectivitÃ© avant l'exÃ©cution
        if [[ "${cmd_type}" == "ansible_playbook" || "${cmd_type}" == "ssh" ]]; then
            if ! ping -c 1 -W 5 "${ansible_host}" &>/dev/null; then
                if [[ ${retry_count} -lt ${max_retries} ]]; then
                    retry_count=$((retry_count + 1))
                    log "WARNING" "ConnectivitÃ© rÃ©seau perdue avec le VPS (${ansible_host}). Tentative ${retry_count}/${max_retries} dans ${backoff_time} secondes..."
                    sleep ${backoff_time}
                    backoff_time=$((backoff_time * 2))  # Backoff exponentiel
                    continue
                else
                    log "ERROR" "ConnectivitÃ© rÃ©seau perdue avec le VPS (${ansible_host})"
                    log "ERROR" "Impossible d'exÃ©cuter la commande sans connectivitÃ© rÃ©seau aprÃ¨s ${max_retries} tentatives"
                    return 1
                fi
            fi
        fi

        # ExÃ©cution de la commande avec timeout
        log "DEBUG" "DÃ©but de l'exÃ©cution de la commande..."

        local exit_code=0
        local command_output=""

        # DÃ©tection du systÃ¨me d'exploitation pour adapter la commande
        local os_name=""
        os_name=$(uname -s)

        # Traitement spÃ©cial pour Windows/WSL pour toutes les commandes interactives
        if [[ "${os_name}" == *"MINGW"* || "${os_name}" == *"MSYS"* || "${os_name}" == *"CYGWIN"* || "${os_name}" == *"Windows"* || "${os_name}" == *"Linux"*"microsoft"* ]]; then
            # Ne pas dÃ©finir ANSIBLE_BECOME_ASK_PASS si on est en exÃ©cution locale
            if [[ "${IS_LOCAL_EXECUTION}" != "true" ]]; then
                log "INFO" "SystÃ¨me Windows/WSL dÃ©tectÃ©, dÃ©finition de la variable d'environnement ANSIBLE_BECOME_ASK_PASS"
                export ANSIBLE_BECOME_ASK_PASS=True
            else
                log "INFO" "SystÃ¨me Windows/WSL dÃ©tectÃ© en exÃ©cution locale, ANSIBLE_BECOME_ASK_PASS non dÃ©fini"
            fi
        fi

        if [[ "${interactive}" == "true" ]]; then
            # Pour les commandes interactives, exÃ©cuter avec un timeout mais permettre l'entrÃ©e utilisateur
            log "INFO" "ExÃ©cution de la commande interactive, veuillez rÃ©pondre aux invites si nÃ©cessaire..."

            # ExÃ©cution directe de la commande avec eval comme dans deploy.sh
            log "DEBUG" "ExÃ©cution de la commande avec eval"
            eval "${cmd_str}"
            exit_code=$?
        else
            # Pour les commandes non interactives, capturer la sortie
            local output_file
            output_file=$(mktemp)
            timeout "${timeout}" bash -c "${cmd_str}" > "${output_file}" 2>&1
            exit_code=$?
            command_output=$(cat "${output_file}")
            rm -f "${output_file}"
        fi

        # Journalisation de la sortie si en mode debug et si la commande n'Ã©tait pas interactive
        if [[ "${debug_mode}" == "true" && -n "${command_output}" ]]; then
            log "DEBUG" "Sortie de la commande:"
            echo "${command_output}" | while IFS= read -r line; do
                log "DEBUG" "  ${line}"
            done
        fi

        # VÃ©rification si l'erreur est liÃ©e au rÃ©seau et si on doit rÃ©essayer
        if [[ ${exit_code} -ne 0 ]]; then
            # Pour les commandes interactives, on ne peut pas analyser la sortie
            if [[ "${interactive}" == "true" ]]; then
                # Si c'est une erreur de timeout, on considÃ¨re que c'est une erreur rÃ©seau
                if [[ ${exit_code} -eq 124 || ${exit_code} -eq 255 ]]; then
                    if [[ ${retry_count} -lt ${max_retries} ]]; then
                        retry_count=$((retry_count + 1))
                        log "WARNING" "Erreur possible de rÃ©seau pour la commande interactive (code ${exit_code}). Tentative ${retry_count}/${max_retries} dans ${backoff_time} secondes..."
                        sleep ${backoff_time}
                        backoff_time=$((backoff_time * 2))  # Backoff exponentiel
                        continue
                    fi
                fi
            # Pour les commandes non interactives, on peut analyser la sortie
            elif is_network_error "${command_output}" ${exit_code}; then
                if [[ ${retry_count} -lt ${max_retries} ]]; then
                    retry_count=$((retry_count + 1))
                    log "WARNING" "Erreur rÃ©seau dÃ©tectÃ©e (code ${exit_code}). Tentative ${retry_count}/${max_retries} dans ${backoff_time} secondes..."
                    sleep ${backoff_time}
                    backoff_time=$((backoff_time * 2))  # Backoff exponentiel
                    continue
                fi
            fi
        fi

        # Analyse du code de retour
        if [[ ${exit_code} -eq 124 ]]; then
            log "ERROR" "La commande a dÃ©passÃ© le dÃ©lai d'attente (${timeout}s)"

            # Tentative de diagnostic pour les timeouts
            case "${cmd_type}" in
                "ansible_playbook")
                    log "INFO" "VÃ©rification de la connectivitÃ© SSH..."
                    if ssh -o BatchMode=yes -o ConnectTimeout=5 -p "${ansible_port}" "${ansible_user}@${ansible_host}" "echo 'Test de connexion'" &>/dev/null; then
                        log "INFO" "La connexion SSH fonctionne, le problÃ¨me pourrait Ãªtre liÃ© Ã  Ansible ou Ã  une opÃ©ration longue"
                    else
                        log "ERROR" "La connexion SSH ne fonctionne pas, vÃ©rifiez les paramÃ¨tres de connexion"
                    fi
                    ;;
                "kubectl_apply")
                    log "INFO" "VÃ©rification de l'accÃ¨s Ã  l'API Kubernetes..."
                    if kubectl cluster-info &>/dev/null; then
                        log "INFO" "L'accÃ¨s Ã  l'API Kubernetes fonctionne, le problÃ¨me pourrait Ãªtre liÃ© Ã  une opÃ©ration longue"
                    else
                        log "ERROR" "L'accÃ¨s Ã  l'API Kubernetes ne fonctionne pas, vÃ©rifiez la configuration de kubectl"
                    fi
                    ;;
            esac

            return 1
        elif [[ ${exit_code} -ne 0 ]]; then
            log "ERROR" "La commande a Ã©chouÃ© avec le code ${exit_code}"

            # Analyse de la sortie pour des erreurs connues (seulement pour les commandes non interactives)
            if [[ "${interactive}" == "false" && -n "${command_output}" ]]; then
                if echo "${command_output}" | grep -q "Connection refused"; then
                    log "ERROR" "Connexion refusÃ©e - vÃ©rifiez que le service est en cours d'exÃ©cution et accessible"
                elif echo "${command_output}" | grep -q "Permission denied"; then
                    log "ERROR" "Permission refusÃ©e - vÃ©rifiez les droits d'accÃ¨s"
                elif echo "${command_output}" | grep -q "No space left on device"; then
                    log "ERROR" "Plus d'espace disque disponible - libÃ©rez de l'espace et rÃ©essayez"
                elif echo "${command_output}" | grep -q "Unable to connect to the server"; then
                    log "ERROR" "Impossible de se connecter au serveur Kubernetes - vÃ©rifiez que K3s est en cours d'exÃ©cution"
                fi
            elif [[ "${interactive}" == "true" ]]; then
                log "INFO" "La commande interactive a Ã©chouÃ©. VÃ©rifiez les erreurs affichÃ©es ci-dessus."

                # Suggestions spÃ©cifiques pour les commandes interactives
                if [[ "${LAST_COMMAND}" == *"ansible-playbook"* && "${LAST_COMMAND}" == *"--ask-become-pass"* ]]; then
                    log "INFO" "Suggestions pour les erreurs Ansible avec --ask-become-pass:"
                    log "INFO" "1. VÃ©rifiez que vous avez entrÃ© le bon mot de passe sudo"
                    log "INFO" "2. VÃ©rifiez que l'utilisateur a les droits sudo sur le VPS"
                    log "INFO" "3. VÃ©rifiez la configuration de sudoers sur le VPS"
                fi
            fi

            return ${exit_code}
        fi

        # Si on arrive ici, c'est que la commande a rÃ©ussi
        if [[ ${retry_count} -gt 0 ]]; then
            log "SUCCESS" "Commande exÃ©cutÃ©e avec succÃ¨s aprÃ¨s ${retry_count} tentatives"
        else
            if [[ "${interactive}" == "true" ]]; then
                log "SUCCESS" "Commande interactive exÃ©cutÃ©e avec succÃ¨s"
            else
                log "DEBUG" "Commande exÃ©cutÃ©e avec succÃ¨s"
            fi
        fi
        return 0
    done
}

# Fonction d'affichage de l'aide
function afficher_aide() {
    cat << EOF
Script d'Installation de l'Infrastructure LIONS sur VPS

Ce script orchestre l'installation complÃ¨te de l'infrastructure LIONS sur un VPS.

Usage:
    $0 [options]

Options:
    -e, --environment <env>   Environnement cible (production, staging, development)
                             Par dÃ©faut: development
    -i, --inventory <file>    Fichier d'inventaire Ansible spÃ©cifique
                             Par dÃ©faut: inventories/development/hosts.yml
    -s, --skip-init           Ignorer l'initialisation du VPS (si dÃ©jÃ  effectuÃ©e)
    -d, --debug               Active le mode debug
    -h, --help                Affiche cette aide

Exemples:
    $0
    $0 --environment staging
    $0 --skip-init --debug
EOF
}

# Fonction de vÃ©rification des prÃ©requis
function verifier_prerequis() {
    log "INFO" "VÃ©rification des prÃ©requis..."
    INSTALLATION_STEP="prerequis"
    LAST_COMMAND="verifier_prerequis"
    COMMAND_NAME="verifier_prerequis"

    # VÃ©rification du verrouillage
    if [[ -f "${LOCK_FILE}" ]]; then
        log "WARNING" "Une autre instance du script semble Ãªtre en cours d'exÃ©cution"

        # VÃ©rification de l'Ã¢ge du fichier de verrouillage
        local lock_file_age
        lock_file_age=$(( $(date +%s) - $(stat -c %Y "${LOCK_FILE}" 2>/dev/null || echo $(date +%s)) ))

        # VÃ©rification de l'uptime du systÃ¨me
        local uptime_seconds
        uptime_seconds=$(cat /proc/uptime 2>/dev/null | awk '{print int($1)}' || echo 999999)

        # VÃ©rification des processus en cours d'exÃ©cution
        local script_name
        script_name=$(basename "$0")
        local script_count
        script_count=$(ps aux | grep -v grep | grep -c "${script_name}" || echo 1)

        # Si le systÃ¨me a redÃ©marrÃ© aprÃ¨s la crÃ©ation du fichier de verrouillage
        # ou si le fichier de verrouillage existe depuis plus d'une heure
        # ou si aucun autre processus du script n'est en cours d'exÃ©cution
        if [[ ${uptime_seconds} -lt ${lock_file_age} || ${lock_file_age} -gt 3600 || ${script_count} -le 1 ]]; then
            log "INFO" "Le systÃ¨me a redÃ©marrÃ© ou le fichier de verrouillage est obsolÃ¨te (Ã¢ge: ${lock_file_age}s, uptime: ${uptime_seconds}s) ou aucune autre instance n'est en cours d'exÃ©cution"
            log "INFO" "Suppression automatique du fichier de verrouillage obsolÃ¨te"
            # Tentative de suppression sans sudo d'abord
            if ! rm -f "${LOCK_FILE}" 2>/dev/null; then
                log "WARNING" "Impossible de supprimer le fichier de verrouillage sans sudo, tentative avec sudo..."
                # Si Ã§a Ã©choue, essayer avec secure_sudo
                if secure_sudo rm -f "${LOCK_FILE}"; then
                    log "SUCCESS" "Fichier de verrouillage obsolÃ¨te supprimÃ© avec succÃ¨s (sudo)"
                else
                    log "WARNING" "Impossible de supprimer le fichier de verrouillage obsolÃ¨te, mÃªme avec sudo"
                fi
            fi
        else
            log "WARNING" "Si ce n'est pas le cas, tentative de suppression du fichier de verrouillage avec sudo"
            log "INFO" "ExÃ©cution de la commande: sudo rm -f ${LOCK_FILE}"
            # Utilisation de secure_sudo pour supprimer le fichier, ce qui demandera le mot de passe
            if secure_sudo rm -f "${LOCK_FILE}"; then
                log "SUCCESS" "Fichier de verrouillage supprimÃ© avec succÃ¨s"
            else
                log "ERROR" "Impossible de supprimer le fichier de verrouillage, mÃªme avec sudo"
                log "ERROR" "Veuillez le supprimer manuellement: sudo rm -f ${LOCK_FILE}"
                exit 1
            fi
        fi
    fi

    # CrÃ©ation du fichier de verrouillage
    touch "${LOCK_FILE}"

    # VÃ©rification de la version du systÃ¨me d'exploitation
    log "INFO" "VÃ©rification du systÃ¨me d'exploitation..."
    local os_name
    os_name=$(uname -s)
    local os_version
    os_version=$(uname -r)

    if [[ "${os_name}" != "Linux" && "${os_name}" != "Darwin" ]]; then
        log "WARNING" "SystÃ¨me d'exploitation non testÃ©: ${os_name} ${os_version}"
        log "WARNING" "Ce script est conÃ§u pour fonctionner sur Linux ou macOS"
        log "INFO" "Voulez-vous continuer malgrÃ© tout? (o/N)"
        read -r answer
        if [[ ! "${answer}" =~ ^[Oo]$ ]]; then
            cleanup
            exit 1
        fi
    else
        log "INFO" "SystÃ¨me d'exploitation: ${os_name} ${os_version}"
    fi

    # DÃ©tection de WSL2 et avertissement sur les problÃ¨mes de compatibilitÃ© avec K3s
    if [[ "${os_version}" == *"WSL"* || "${os_version}" == *"Microsoft"* || "${os_version}" == *"microsoft"* ]]; then
        log "WARNING" "Environnement WSL2 dÃ©tectÃ©: ${os_version}"
        log "WARNING" "âš ï¸ ATTENTION: K3s peut rencontrer des problÃ¨mes de compatibilitÃ© dans WSL2 âš ï¸"
        log "WARNING" "ProblÃ¨mes connus:"
        log "WARNING" "  - Erreurs de dÃ©marrage du ContainerManager"
        log "WARNING" "  - ProblÃ¨mes avec les cgroups"
        log "WARNING" "  - Connexions refusÃ©es Ã  l'API Kubernetes"
        log "WARNING" "  - Service K3s qui ne dÃ©marre jamais complÃ¨tement"
        log "INFO" "Recommandations:"
        log "INFO" "  1. ExÃ©cutez ce script directement sur le VPS cible plutÃ´t que via WSL2"
        log "INFO" "  2. Connectez-vous au VPS via SSH: ssh ${ansible_user}@${ansible_host} -p ${ansible_port}"
        log "INFO" "  3. Clonez le dÃ©pÃ´t sur le VPS: git clone https://github.com/votre-repo/lions-infrastructure-automated-depl.git"
        log "INFO" "  4. ExÃ©cutez le script d'installation sur le VPS: cd lions-infrastructure-automated-depl/lions-infrastructure/scripts && ./install.sh"
        log "INFO" "Voulez-vous continuer malgrÃ© ces avertissements? (o/N)"
        read -r answer
        if [[ ! "${answer}" =~ ^[Oo]$ ]]; then
            log "INFO" "Installation annulÃ©e. ExÃ©cutez le script directement sur le VPS pour de meilleurs rÃ©sultats."
            cleanup
            exit 1
        fi
        log "WARNING" "Continuation de l'installation dans WSL2 malgrÃ© les risques de problÃ¨mes..."
    fi

    # VÃ©rification de l'espace disque
    if ! check_disk_space; then
        log "ERROR" "VÃ©rification de l'espace disque Ã©chouÃ©e"
        cleanup
        exit 1
    fi

    # VÃ©rification de la mÃ©moire disponible
    log "INFO" "VÃ©rification de la mÃ©moire disponible..."
    local available_memory=0
    if [[ "${os_name}" == "Linux" ]]; then
        available_memory=$(free -m | awk '/^Mem:/ {print $7}')
    elif [[ "${os_name}" == "Darwin" ]]; then
        available_memory=$(vm_stat | awk '/Pages free/ {free=$3} /Pages inactive/ {inactive=$3} END {print (free+inactive)*4096/1048576}' | cut -d. -f1)
    fi

    if [[ ${available_memory} -lt 1024 ]]; then
        log "WARNING" "MÃ©moire disponible limitÃ©e: ${available_memory}MB (recommandÃ©: 1024MB minimum)"
        log "WARNING" "Des problÃ¨mes de performance peuvent survenir pendant l'installation"
        log "INFO" "Voulez-vous continuer malgrÃ© tout? (o/N)"
        read -r answer
        if [[ ! "${answer}" =~ ^[Oo]$ ]]; then
            cleanup
            exit 1
        fi
    else
        log "INFO" "MÃ©moire disponible: ${available_memory}MB (minimum recommandÃ©: 1024MB)"
    fi

    # VÃ©rification des commandes requises avec versions minimales
    log "INFO" "VÃ©rification des commandes requises..."
    local required_commands=(
        "ansible-playbook:2.13.13"
        "ssh:7.0"
        "scp:7.0"
        "kubectl:1.20.0"
        "helm:3.5.0"
        "timeout:0"
        "nc:0"
        "ping:0"
        "jq:0"
    )
    local missing_commands=()
    local outdated_commands=()

    for cmd_with_version in "${required_commands[@]}"; do
        local cmd="${cmd_with_version%%:*}"
        local min_version="${cmd_with_version#*:}"

        if ! command_exists "${cmd}"; then
            missing_commands+=("${cmd}")
            continue
        fi

        # VÃ©rification des versions pour les commandes critiques
        if [[ "${min_version}" != "0" ]]; then
            local current_version=""
            case "${cmd}" in
                "ansible-playbook")
                    # Add timeout to prevent hanging and handle errors better
                    current_version=$(timeout 5 ansible-playbook --version 2>/dev/null | head -n1 | awk '{print $2}' | cut -d[ -f1 || echo "${min_version}")
                    ;;
                "kubectl")
                    # Add timeout to prevent hanging and handle errors better
                    current_version=$(timeout 5 kubectl version --client --short 2>/dev/null | awk '{print $3}' | sed 's/v//' || echo "${min_version}")
                    ;;
                "helm")
                    # Add timeout to prevent hanging and handle errors better
                    current_version=$(timeout 5 helm version --short 2>/dev/null | sed 's/v//' | cut -d+ -f1 || echo "${min_version}")
                    ;;
                *)
                    # Pour les autres commandes, on ne vÃ©rifie pas la version
                    current_version="${min_version}"
                    ;;
            esac

            if [[ -n "${current_version}" && "${current_version}" != "${min_version}" ]]; then
                if ! version_greater_equal "${current_version}" "${min_version}"; then
                    outdated_commands+=("${cmd} (actuelle: ${current_version}, requise: ${min_version})")
                fi
            fi
        fi
    done

    if [[ ${#missing_commands[@]} -gt 0 ]]; then
        log "WARNING" "Commandes requises non trouvÃ©es: ${missing_commands[*]}"
        log "INFO" "Tentative d'installation automatique des commandes manquantes..."

        if install_missing_commands "${missing_commands[@]}"; then
            log "SUCCESS" "Installation des commandes manquantes rÃ©ussie"
            # VÃ©rifier Ã  nouveau les commandes
            missing_commands=()
            for cmd_with_version in "${required_commands[@]}"; do
                local cmd="${cmd_with_version%%:*}"
                if ! command_exists "${cmd}"; then
                    missing_commands+=("${cmd}")
                fi
            done

            if [[ ${#missing_commands[@]} -gt 0 ]]; then
                log "ERROR" "Certaines commandes n'ont pas pu Ãªtre installÃ©es: ${missing_commands[*]}"
                log "ERROR" "Veuillez installer ces commandes manuellement avant de continuer"
                cleanup
                exit 1
            fi
        else
            log "ERROR" "Ã‰chec de l'installation automatique des commandes manquantes"
            log "ERROR" "Veuillez installer ces commandes manuellement avant de continuer"
            cleanup
            exit 1
        fi
    fi

    if [[ ${#outdated_commands[@]} -gt 0 ]]; then
        log "WARNING" "Commandes avec versions obsolÃ¨tes: ${outdated_commands[*]}"
        log "INFO" "Voulez-vous tenter de mettre Ã  jour ces commandes automatiquement? (o/N)"
        read -r update_answer
        if [[ "${update_answer}" =~ ^[Oo]$ ]]; then
            log "INFO" "Tentative de mise Ã  jour des commandes obsolÃ¨tes..."
            if update_outdated_commands "${outdated_commands[@]}"; then
                log "SUCCESS" "Mise Ã  jour des commandes obsolÃ¨tes rÃ©ussie"
                # VÃ©rifier Ã  nouveau les commandes
                outdated_commands=()
                for cmd_with_version in "${required_commands[@]}"; do
                    local cmd="${cmd_with_version%%:*}"
                    local min_version="${cmd_with_version#*:}"

                    if command_exists "${cmd}" && [[ "${min_version}" != "0" ]]; then
                        local current_version=""
                        case "${cmd}" in
                            "ansible-playbook")
                                current_version=$(timeout 5 ansible-playbook --version 2>/dev/null | head -n1 | awk '{print $2}' | cut -d[ -f1 || echo "${min_version}")
                                ;;
                            "kubectl")
                                current_version=$(timeout 5 kubectl version --client --short 2>/dev/null | awk '{print $3}' | sed 's/v//' || echo "${min_version}")
                                ;;
                            "helm")
                                current_version=$(timeout 5 helm version --short 2>/dev/null | sed 's/v//' | cut -d+ -f1 || echo "${min_version}")
                                ;;
                            *)
                                current_version="${min_version}"
                                ;;
                        esac

                        if [[ -n "${current_version}" && "${current_version}" != "${min_version}" ]]; then
                            if ! version_greater_equal "${current_version}" "${min_version}"; then
                                outdated_commands+=("${cmd} (actuelle: ${current_version}, requise: ${min_version})")
                            fi
                        fi
                    fi
                done

                if [[ ${#outdated_commands[@]} -gt 0 ]]; then
                    log "WARNING" "Certaines commandes sont toujours obsolÃ¨tes aprÃ¨s la mise Ã  jour: ${outdated_commands[*]}"
                    log "WARNING" "Il est recommandÃ© de mettre Ã  jour ces commandes manuellement avant de continuer"
                    log "INFO" "Voulez-vous continuer malgrÃ© tout? (o/N)"
                    read -r continue_answer
                    if [[ ! "${continue_answer}" =~ ^[Oo]$ ]]; then
                        cleanup
                        exit 1
                    fi
                fi
            else
                log "WARNING" "Ã‰chec de la mise Ã  jour automatique des commandes obsolÃ¨tes"
                log "WARNING" "Il est recommandÃ© de mettre Ã  jour ces commandes manuellement avant de continuer"
                log "INFO" "Voulez-vous continuer malgrÃ© tout? (o/N)"
                read -r continue_answer
                if [[ ! "${continue_answer}" =~ ^[Oo]$ ]]; then
                    cleanup
                    exit 1
                fi
            fi
        else
            log "WARNING" "Il est recommandÃ© de mettre Ã  jour ces commandes avant de continuer"
            log "INFO" "Voulez-vous continuer malgrÃ© tout? (o/N)"
            read -r continue_answer
            if [[ ! "${continue_answer}" =~ ^[Oo]$ ]]; then
                cleanup
                exit 1
            fi
        fi
    fi

    # VÃ©rification et installation des collections Ansible requises
    if ! check_ansible_collections; then
        log "ERROR" "Ã‰chec de la vÃ©rification ou de l'installation des collections Ansible requises"
        log "ERROR" "Assurez-vous que les collections nÃ©cessaires sont installÃ©es avant de continuer"
        log "INFO" "Vous pouvez les installer manuellement avec: ansible-galaxy collection install community.kubernetes"
        cleanup
        exit 1
    fi

    # VÃ©rification et installation des dÃ©pendances Python requises
    if ! check_python_dependencies; then
        log "ERROR" "Ã‰chec de la vÃ©rification ou de l'installation des dÃ©pendances Python requises"
        log "ERROR" "Assurez-vous que les modules Python nÃ©cessaires sont installÃ©s avant de continuer"
        log "INFO" "Vous pouvez les installer manuellement avec: pip install kubernetes openshift"
        cleanup
        exit 1
    fi

    # VÃ©rification et installation des plugins Helm requis
    if ! check_helm_plugins; then
        log "ERROR" "Ã‰chec de la vÃ©rification ou de l'installation des plugins Helm requis"
        log "ERROR" "Assurez-vous que le plugin helm-diff est installÃ© avant de continuer"
        log "INFO" "Vous pouvez l'installer manuellement avec: helm plugin install https://github.com/databus23/helm-diff --version v3.4.1"
        cleanup
        exit 1
    fi

    # VÃ©rification des fichiers Ansible
    log "INFO" "VÃ©rification des fichiers Ansible..."

    # Adaptation des chemins pour Windows si nÃ©cessaire
    local inventory_dir="${ANSIBLE_DIR}/inventories/${environment}"
    local is_windows=false

    if [[ "${os_name}" == *"MINGW"* || "${os_name}" == *"MSYS"* || "${os_name}" == *"CYGWIN"* || "${os_name}" == *"Windows"* || "${os_name}" == *"WSL"* ]]; then
        is_windows=true
        log "DEBUG" "Environnement Windows/WSL dÃ©tectÃ©, adaptation des chemins"

        # Convertir les chemins pour Windows
        if [[ "${inventory_dir}" == *"/"* && "${inventory_dir}" != *"\\"* ]]; then
            local inventory_dir_win=$(echo "${inventory_dir}" | tr '/' '\\')
            log "DEBUG" "Chemin d'inventaire adaptÃ© pour Windows: ${inventory_dir_win}"

            # VÃ©rifier si le chemin converti existe
            if [[ -d "${inventory_dir_win}" ]]; then
                log "DEBUG" "Utilisation du chemin Windows pour le rÃ©pertoire d'inventaire"
                inventory_dir="${inventory_dir_win}"
            else
                log "DEBUG" "Le chemin Windows n'existe pas, vÃ©rification du chemin original"
            fi
        fi
    fi

    if [[ ! -d "${inventory_dir}" ]]; then
        log "INFO" "Le rÃ©pertoire d'inventaire pour l'environnement ${environment} n'existe pas: ${inventory_dir}"
        log "INFO" "CrÃ©ation du rÃ©pertoire d'inventaire..."

        # CrÃ©ation du rÃ©pertoire avec gestion d'erreur amÃ©liorÃ©e
        if ! mkdir -p "${inventory_dir}" 2>/dev/null; then
            # Tentative avec le chemin original si le chemin Windows Ã©choue
            if [[ "${is_windows}" == "true" && "${inventory_dir}" == *"\\"* ]]; then
                local inventory_dir_unix=$(echo "${inventory_dir}" | tr '\\' '/')
                log "DEBUG" "Tentative avec le chemin Unix: ${inventory_dir_unix}"
                if ! mkdir -p "${inventory_dir_unix}" 2>/dev/null; then
                    log "ERROR" "Impossible de crÃ©er le rÃ©pertoire d'inventaire: ${inventory_dir}"
                    cleanup
                    exit 1
                else
                    inventory_dir="${inventory_dir_unix}"
                fi
            else
                log "ERROR" "Impossible de crÃ©er le rÃ©pertoire d'inventaire: ${inventory_dir}"
                cleanup
                exit 1
            fi
        fi
        log "SUCCESS" "RÃ©pertoire d'inventaire crÃ©Ã©: ${inventory_dir}"
    fi

    # Adaptation des chemins pour Windows si nÃ©cessaire
    local inventory_file_path="${ANSIBLE_DIR}/${inventory_file}"
    if [[ "${os_name}" == *"MINGW"* || "${os_name}" == *"MSYS"* || "${os_name}" == *"CYGWIN"* || "${os_name}" == *"Windows"* ]]; then
        # Convertir les chemins pour Windows
        if [[ "${inventory_file_path}" == *"/"* && "${inventory_file_path}" != *"\\"* ]]; then
            local inventory_file_path_win=$(echo "${inventory_file_path}" | tr '/' '\\')
            log "DEBUG" "Chemin du fichier d'inventaire adaptÃ© pour Windows: ${inventory_file_path_win}"

            # VÃ©rifier si le chemin converti existe
            if [[ -f "${inventory_file_path_win}" ]]; then
                log "DEBUG" "Utilisation du chemin Windows pour le fichier d'inventaire"
                inventory_file_path="${inventory_file_path_win}"
            else
                log "DEBUG" "Le chemin Windows n'existe pas, vÃ©rification du chemin original"
            fi
        fi
    fi

    if [[ ! -f "${inventory_file_path}" ]]; then
        log "WARNING" "Le fichier d'inventaire n'existe pas: ${inventory_file_path}"
        log "INFO" "CrÃ©ation d'un fichier d'inventaire par dÃ©faut..."

        # CrÃ©er le rÃ©pertoire parent si nÃ©cessaire
        mkdir -p "$(dirname "${inventory_file_path}")" || {
            log "ERROR" "Impossible de crÃ©er le rÃ©pertoire parent pour le fichier d'inventaire"
            cleanup
            exit 1
        }

        # CrÃ©er un fichier d'inventaire par dÃ©faut
        cat > "${inventory_file_path}" << EOF
---
all:
  children:
    vps:
      hosts:
        contabo-vps:
          ansible_host: ${LIONS_VPS_HOST:-176.57.150.2}
          ansible_port: ${LIONS_VPS_PORT:-225}
          ansible_user: ${LIONS_VPS_USER:-root}
          ansible_python_interpreter: /usr/bin/python3
          ansible_connection: local
    kubernetes:
      hosts:
        contabo-vps:
          ansible_host: ${LIONS_VPS_HOST:-176.57.150.2}
          ansible_port: ${LIONS_VPS_PORT:-225}
          ansible_connection: local
    databases:
      hosts:
        contabo-vps:
          ansible_host: ${LIONS_VPS_HOST:-176.57.150.2}
          ansible_port: ${LIONS_VPS_PORT:-225}
          ansible_connection: local
    monitoring:
      hosts:
        contabo-vps:
          ansible_host: ${LIONS_VPS_HOST:-176.57.150.2}
          ansible_port: ${LIONS_VPS_PORT:-225}
          ansible_connection: local
  vars:
    ansible_user: ${LIONS_VPS_USER:-lionsdevadmin}
    ansible_become: yes
    environment: ${environment}
    domain_name: ${LIONS_DOMAIN:-dev.lions.dev}
EOF

        log "SUCCESS" "Fichier d'inventaire crÃ©Ã©: ${inventory_file_path}"
    fi

    # VÃ©rification des playbooks avec adaptation des chemins pour Windows
    local playbooks=(
        "init-vps.yml"
        "install-k3s.yml"
    )

    for playbook in "${playbooks[@]}"; do
        local playbook_path="${ANSIBLE_DIR}/playbooks/${playbook}"

        # Adaptation des chemins pour Windows si nÃ©cessaire
        if [[ "${os_name}" == *"MINGW"* || "${os_name}" == *"MSYS"* || "${os_name}" == *"CYGWIN"* || "${os_name}" == *"Windows"* ]]; then
            # Convertir les chemins pour Windows
            if [[ "${playbook_path}" == *"/"* && "${playbook_path}" != *"\\"* ]]; then
                local playbook_path_win=$(echo "${playbook_path}" | tr '/' '\\')
                log "DEBUG" "Chemin du playbook adaptÃ© pour Windows: ${playbook_path_win}"

                # VÃ©rifier si le chemin converti existe
                if [[ -f "${playbook_path_win}" ]]; then
                    log "DEBUG" "Utilisation du chemin Windows pour le playbook"
                    playbook_path="${playbook_path_win}"
                else
                    log "DEBUG" "Le chemin Windows n'existe pas, vÃ©rification du chemin original"
                fi
            fi
        fi

        if [[ ! -f "${playbook_path}" ]]; then
            log "ERROR" "Le playbook ${playbook} n'existe pas: ${playbook_path}"
            log "ERROR" "Veuillez vÃ©rifier que tous les playbooks nÃ©cessaires sont prÃ©sents dans le rÃ©pertoire ${ANSIBLE_DIR}/playbooks/"
            cleanup
            exit 1
        fi
    done

    # VÃ©rification des fichiers Kubernetes
    log "INFO" "VÃ©rification des fichiers Kubernetes..."

    # Adaptation des chemins pour Windows si nÃ©cessaire
    local k8s_overlay_dir="${PROJECT_ROOT}/kubernetes/overlays/${environment}"
    local is_windows=false

    if [[ "${os_name}" == *"MINGW"* || "${os_name}" == *"MSYS"* || "${os_name}" == *"CYGWIN"* || "${os_name}" == *"Windows"* || "${os_name}" == *"WSL"* ]]; then
        is_windows=true
        log "DEBUG" "Environnement Windows/WSL dÃ©tectÃ©, adaptation des chemins Kubernetes"

        # Convertir les chemins pour Windows
        if [[ "${k8s_overlay_dir}" == *"/"* && "${k8s_overlay_dir}" != *"\\"* ]]; then
            local k8s_overlay_dir_win=$(echo "${k8s_overlay_dir}" | tr '/' '\\')
            log "DEBUG" "Chemin d'overlay Kubernetes adaptÃ© pour Windows: ${k8s_overlay_dir_win}"

            # VÃ©rifier si le chemin converti existe
            if [[ -d "${k8s_overlay_dir_win}" ]]; then
                log "DEBUG" "Utilisation du chemin Windows pour le rÃ©pertoire d'overlay Kubernetes"
                k8s_overlay_dir="${k8s_overlay_dir_win}"
            else
                log "DEBUG" "Le chemin Windows n'existe pas, vÃ©rification du chemin original"
            fi
        fi
    fi

    if [[ ! -d "${k8s_overlay_dir}" ]]; then
        log "INFO" "Le rÃ©pertoire d'overlay Kubernetes pour l'environnement ${environment} n'existe pas: ${k8s_overlay_dir}"
        log "INFO" "CrÃ©ation du rÃ©pertoire d'overlay Kubernetes..."

        # CrÃ©ation du rÃ©pertoire avec gestion d'erreur amÃ©liorÃ©e
        if ! mkdir -p "${k8s_overlay_dir}" 2>/dev/null; then
            # Tentative avec le chemin original si le chemin Windows Ã©choue
            if [[ "${is_windows}" == "true" && "${k8s_overlay_dir}" == *"\\"* ]]; then
                local k8s_overlay_dir_unix=$(echo "${k8s_overlay_dir}" | tr '\\' '/')
                log "DEBUG" "Tentative avec le chemin Unix: ${k8s_overlay_dir_unix}"
                if ! mkdir -p "${k8s_overlay_dir_unix}" 2>/dev/null; then
                    log "ERROR" "Impossible de crÃ©er le rÃ©pertoire d'overlay Kubernetes: ${k8s_overlay_dir}"
                    cleanup
                    exit 1
                else
                    k8s_overlay_dir="${k8s_overlay_dir_unix}"
                fi
            else
                log "ERROR" "Impossible de crÃ©er le rÃ©pertoire d'overlay Kubernetes: ${k8s_overlay_dir}"
                cleanup
                exit 1
            fi
        fi
        log "SUCCESS" "RÃ©pertoire d'overlay Kubernetes crÃ©Ã©: ${k8s_overlay_dir}"
    fi

    # VÃ©rification du fichier kustomization.yaml
    local kustomization_file="${k8s_overlay_dir}/kustomization.yaml"

    if [[ "${is_windows}" == "true" ]]; then
        # Convertir les chemins pour Windows
        if [[ "${kustomization_file}" == *"/"* && "${kustomization_file}" != *"\\"* ]]; then
            local kustomization_file_win=$(echo "${kustomization_file}" | tr '/' '\\')
            log "DEBUG" "Chemin du fichier kustomization.yaml adaptÃ© pour Windows: ${kustomization_file_win}"

            # VÃ©rifier si le chemin converti existe
            if [[ -f "${kustomization_file_win}" ]]; then
                log "DEBUG" "Utilisation du chemin Windows pour le fichier kustomization.yaml"
                kustomization_file="${kustomization_file_win}"
            else
                log "DEBUG" "Le chemin Windows n'existe pas, vÃ©rification du chemin original"
            fi
        fi
    fi

    if [[ ! -f "${kustomization_file}" ]]; then
        log "INFO" "Le fichier kustomization.yaml pour l'environnement ${environment} n'existe pas: ${kustomization_file}"
        log "INFO" "CrÃ©ation d'un fichier kustomization.yaml par dÃ©faut..."

        # Tentative de crÃ©ation du fichier avec gestion d'erreur amÃ©liorÃ©e
        if ! cat > "${kustomization_file}" << EOF 2>/dev/null; then
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

resources:
  - ../../base/namespaces
  - ../../base/storage-classes
  - ../../base/network-policies
  - ../../base/resource-quotas

namespace: default

patches:
  - path: patches/namespace-env.yaml
    target:
      kind: Namespace

configMapGenerator:
  - name: environment-config
    namespace: default
    literals:
      - ENVIRONMENT=${environment}
      - DOMAIN=${LIONS_DOMAIN:-dev.lions.dev}
EOF
            # Tentative avec le chemin alternatif si le chemin Windows Ã©choue
            if [[ "${is_windows}" == "true" && "${kustomization_file}" == *"\\"* ]]; then
                local kustomization_file_unix=$(echo "${kustomization_file}" | tr '\\' '/')
                log "DEBUG" "Tentative avec le chemin Unix pour kustomization.yaml: ${kustomization_file_unix}"
                if ! cat > "${kustomization_file_unix}" << EOF 2>/dev/null; then
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

resources:
  - ../../base/namespaces
  - ../../base/storage-classes
  - ../../base/network-policies
  - ../../base/resource-quotas

namespace: default

patches:
  - path: patches/namespace-env.yaml
    target:
      kind: Namespace

configMapGenerator:
  - name: environment-config
    namespace: default
    literals:
      - ENVIRONMENT=${environment}
      - DOMAIN=${LIONS_DOMAIN:-dev.lions.dev}
EOF
                    log "ERROR" "Impossible de crÃ©er le fichier kustomization.yaml: ${kustomization_file}"
                    cleanup
                    exit 1
                else
                    kustomization_file="${kustomization_file_unix}"
                fi
            else
                log "ERROR" "Impossible de crÃ©er le fichier kustomization.yaml: ${kustomization_file}"
                cleanup
                exit 1
            fi
        fi

        # CrÃ©er le rÃ©pertoire patches avec gestion d'erreur amÃ©liorÃ©e
        local patches_dir="${k8s_overlay_dir}/patches"
        if [[ "${is_windows}" == "true" && "${k8s_overlay_dir}" == *"\\"* ]]; then
            patches_dir="${k8s_overlay_dir}\\patches"
        fi

        if ! mkdir -p "${patches_dir}" 2>/dev/null; then
            # Tentative avec le chemin alternatif
            if [[ "${is_windows}" == "true" ]]; then
                local patches_dir_alt
                if [[ "${patches_dir}" == *"\\"* ]]; then
                    patches_dir_alt=$(echo "${patches_dir}" | tr '\\' '/')
                else
                    patches_dir_alt=$(echo "${patches_dir}" | tr '/' '\\')
                fi
                log "DEBUG" "Tentative avec le chemin alternatif pour patches: ${patches_dir_alt}"
                if ! mkdir -p "${patches_dir_alt}" 2>/dev/null; then
                    log "ERROR" "Impossible de crÃ©er le rÃ©pertoire patches: ${patches_dir}"
                    cleanup
                    exit 1
                else
                    patches_dir="${patches_dir_alt}"
                fi
            else
                log "ERROR" "Impossible de crÃ©er le rÃ©pertoire patches: ${patches_dir}"
                cleanup
                exit 1
            fi
        fi

        # CrÃ©er un fichier patch par dÃ©faut pour les namespaces
        local namespace_patch="${patches_dir}/namespace-env.yaml"
        if ! cat > "${namespace_patch}" << EOF 2>/dev/null; then
apiVersion: v1
kind: Namespace
metadata:
  name: default
  labels:
    environment: ${environment}
EOF
            # Tentative avec le chemin alternatif
            if [[ "${is_windows}" == "true" ]]; then
                local namespace_patch_alt
                if [[ "${namespace_patch}" == *"\\"* ]]; then
                    namespace_patch_alt=$(echo "${namespace_patch}" | tr '\\' '/')
                else
                    namespace_patch_alt=$(echo "${namespace_patch}" | tr '/' '\\')
                fi
                log "DEBUG" "Tentative avec le chemin alternatif pour namespace-env.yaml: ${namespace_patch_alt}"
                if ! cat > "${namespace_patch_alt}" << EOF 2>/dev/null; then
apiVersion: v1
kind: Namespace
metadata:
  name: default
  labels:
    environment: ${environment}
EOF
                    log "ERROR" "Impossible de crÃ©er le fichier namespace-env.yaml: ${namespace_patch}"
                    cleanup
                    exit 1
                fi
            else
                log "ERROR" "Impossible de crÃ©er le fichier namespace-env.yaml: ${namespace_patch}"
                cleanup
                exit 1
            fi
        fi

        log "SUCCESS" "Fichier kustomization.yaml crÃ©Ã©: ${kustomization_file}"
    fi

    # Extraction des informations de connexion
    log "INFO" "Extraction des informations de connexion..."

    # Utilisation de la fonction robuste d'extraction d'informations d'inventaire
    if ! extraire_informations_inventaire; then
        log "ERROR" "Ã‰chec de l'extraction des informations de connexion"
        cleanup
        exit 1
    fi

    if [[ -z "${ansible_host}" || -z "${ansible_port}" || -z "${ansible_user}" ]]; then
        log "ERROR" "Impossible d'extraire les informations de connexion du fichier d'inventaire"
        log "ERROR" "VÃ©rifiez que le fichier d'inventaire contient les variables ansible_host, ansible_port et ansible_user"
        cleanup
        exit 1
    fi

    log "INFO" "Informations de connexion: ${ansible_user}@${ansible_host}:${ansible_port}"

    # VÃ©rification de la connectivitÃ© rÃ©seau
    log "INFO" "VÃ©rification de la connectivitÃ© rÃ©seau..."
    if ! check_network; then
        log "ERROR" "VÃ©rification de la connectivitÃ© rÃ©seau Ã©chouÃ©e"
        cleanup
        exit 1
    fi

    # VÃ©rification de la connexion SSH (uniquement si exÃ©cution distante)
    if [[ "${IS_LOCAL_EXECUTION}" == "true" ]]; then
        log "INFO" "ExÃ©cution locale dÃ©tectÃ©e, vÃ©rification de la connexion SSH ignorÃ©e"
    else
        log "INFO" "VÃ©rification de la connexion SSH..."

        # Utilisation de la fonction robust_ssh pour la vÃ©rification de connexion
        if robust_ssh "${ansible_host}" "${ansible_port}" "${ansible_user}" "echo 'Connexion SSH rÃ©ussie'" "" "true"; then
            log "SUCCESS" "Connexion SSH rÃ©ussie"
        else
            log "ERROR" "Impossible de se connecter au VPS via SSH (${ansible_user}@${ansible_host}:${ansible_port})"
            log "ERROR" "VÃ©rifiez vos clÃ©s SSH et les paramÃ¨tres de connexion"

            # VÃ©rification des clÃ©s SSH (plus complÃ¨te)
            local ssh_keys_found=false
            local key_types=("id_rsa" "id_ed25519" "id_ecdsa" "id_dsa")

            for key_type in "${key_types[@]}"; do
                if [[ -f ~/.ssh/${key_type} ]]; then
                    ssh_keys_found=true
                    log "INFO" "ClÃ© SSH trouvÃ©e: ~/.ssh/${key_type}"
                fi
            done

            if [[ "${ssh_keys_found}" == "false" ]]; then
                log "ERROR" "Aucune clÃ© SSH trouvÃ©e dans ~/.ssh/"

                # DÃ©tection de WSL pour des instructions spÃ©cifiques
                if [[ "$(uname -r)" == *"WSL"* || "$(uname -r)" == *"Microsoft"* ]]; then
                    log "INFO" "Environnement WSL dÃ©tectÃ©. Vous pouvez:"
                    log "INFO" "1. GÃ©nÃ©rer une nouvelle clÃ© SSH: ssh-keygen -t ed25519"
                    log "INFO" "2. Copier vos clÃ©s Windows: cp /mnt/c/Users/$USER/.ssh/id_rsa* ~/.ssh/"
                    log "INFO" "3. Assurez-vous que les permissions sont correctes: chmod 600 ~/.ssh/id_rsa"
                else
                    log "ERROR" "GÃ©nÃ©rez une paire de clÃ©s avec: ssh-keygen -t ed25519"
                fi
            fi

            # VÃ©rification du fichier known_hosts
            if ! grep -q "${ansible_host}" ~/.ssh/known_hosts 2>/dev/null; then
                log "WARNING" "L'hÃ´te ${ansible_host} n'est pas dans le fichier known_hosts"
                log "WARNING" "Essayez d'abord de vous connecter manuellement: ssh -p ${ansible_port} ${ansible_user}@${ansible_host}"
                log "INFO" "Ou ajoutez l'hÃ´te automatiquement: ssh-keyscan -p ${ansible_port} -H ${ansible_host} >> ~/.ssh/known_hosts"
            fi

            # VÃ©rification si l'utilisateur peut se connecter manuellement
            log "INFO" "Pouvez-vous vous connecter manuellement avec: ssh -p ${ansible_port} ${ansible_user}@${ansible_host} ?"
            log "INFO" "Si oui, vÃ©rifiez les permissions de vos clÃ©s SSH: chmod 600 ~/.ssh/id_*"

            cleanup
            exit 1
        fi
    fi

    # VÃ©rification des ressources du VPS
    log "INFO" "VÃ©rification des ressources du VPS..."
    local vps_cpu_cores
    local vps_memory_total
    local vps_disk_free
    local cmd_output

    if [[ "${IS_LOCAL_EXECUTION}" == "true" ]]; then
        # Utilisation de commandes locales pour obtenir les ressources
        vps_cpu_cores=$(nproc --all 2>/dev/null || echo "0")
        vps_memory_total=$(free -m | awk '/^Mem:/ {print $2}' 2>/dev/null || echo "0")
        vps_disk_free=$(df -m / | awk 'NR==2 {print $4}' 2>/dev/null || echo "0")
    else
        # Utilisation de commandes SSH directes avec capture complÃ¨te de la sortie pour le dÃ©bogage
        log "DEBUG" "RÃ©cupÃ©ration des informations CPU..."
        cmd_output=$(ssh -o BatchMode=yes -o ConnectTimeout=10 -p "${ansible_port}" "${ansible_user}@${ansible_host}" "cat /proc/cpuinfo | grep -c processor" 2>/dev/null || echo "Erreur")
        log "DEBUG" "Sortie de la commande CPU: ${cmd_output}"
        if [[ "${cmd_output}" == "Erreur" || ! "${cmd_output}" =~ ^[0-9]+$ ]]; then
            log "DEBUG" "Tentative alternative pour CPU avec nproc..."
            cmd_output=$(ssh -o BatchMode=yes -o ConnectTimeout=10 -p "${ansible_port}" "${ansible_user}@${ansible_host}" "nproc --all" 2>/dev/null || echo "Erreur")
            log "DEBUG" "Sortie de la commande nproc: ${cmd_output}"
        fi
        if [[ "${cmd_output}" == "Erreur" || ! "${cmd_output}" =~ ^[0-9]+$ ]]; then
            vps_cpu_cores="0"
        else
            vps_cpu_cores="${cmd_output}"
        fi

        log "DEBUG" "RÃ©cupÃ©ration des informations mÃ©moire..."
        cmd_output=$(ssh -o BatchMode=yes -o ConnectTimeout=10 -p "${ansible_port}" "${ansible_user}@${ansible_host}" "free -m | grep Mem | awk '{print \$2}'" 2>/dev/null || echo "Erreur")
        log "DEBUG" "Sortie de la commande mÃ©moire: ${cmd_output}"
        if [[ "${cmd_output}" == "Erreur" || ! "${cmd_output}" =~ ^[0-9]+$ ]]; then
            log "DEBUG" "Tentative alternative pour la mÃ©moire..."
            cmd_output=$(ssh -o BatchMode=yes -o ConnectTimeout=10 -p "${ansible_port}" "${ansible_user}@${ansible_host}" "cat /proc/meminfo | grep MemTotal | awk '{print \$2/1024}'" 2>/dev/null || echo "Erreur")
            log "DEBUG" "Sortie de la commande meminfo: ${cmd_output}"
        fi
        if [[ "${cmd_output}" == "Erreur" || ! "${cmd_output}" =~ ^[0-9.]+$ ]]; then
            vps_memory_total="0"
        else
            # Arrondir Ã  l'entier le plus proche si c'est un nombre dÃ©cimal
            vps_memory_total=$(printf "%.0f" "${cmd_output}" 2>/dev/null || echo "${cmd_output}" | cut -d. -f1)
        fi

        log "DEBUG" "RÃ©cupÃ©ration des informations disque..."
        cmd_output=$(ssh -o BatchMode=yes -o ConnectTimeout=10 -p "${ansible_port}" "${ansible_user}@${ansible_host}" "df -m / | grep -v Filesystem | head -1 | awk '{print \$4}'" 2>/dev/null || echo "Erreur")
        log "DEBUG" "Sortie de la commande disque: ${cmd_output}"
        if [[ "${cmd_output}" == "Erreur" || ! "${cmd_output}" =~ ^[0-9]+$ ]]; then
            log "DEBUG" "Tentative alternative pour le disque..."
            cmd_output=$(ssh -o BatchMode=yes -o ConnectTimeout=10 -p "${ansible_port}" "${ansible_user}@${ansible_host}" "df -k / | grep -v Filesystem | head -1 | awk '{print \$4/1024}'" 2>/dev/null || echo "Erreur")
            log "DEBUG" "Sortie de la commande df -k: ${cmd_output}"
        fi
        if [[ "${cmd_output}" == "Erreur" || ! "${cmd_output}" =~ ^[0-9.]+$ ]]; then
            vps_disk_free="0"
        else
            # Arrondir Ã  l'entier le plus proche si c'est un nombre dÃ©cimal
            vps_disk_free=$(printf "%.0f" "${cmd_output}" 2>/dev/null || echo "${cmd_output}" | cut -d. -f1)
        fi
    fi

    # Log des valeurs brutes pour le dÃ©bogage
    log "DEBUG" "Valeurs brutes aprÃ¨s rÃ©cupÃ©ration: CPU=${vps_cpu_cores}, RAM=${vps_memory_total}, Disk=${vps_disk_free}"

    # Nettoyage des valeurs pour s'assurer qu'elles sont des nombres entiers valides
    if [[ ! "${vps_cpu_cores}" =~ ^[0-9]+$ ]]; then
        log "DEBUG" "Nettoyage de la valeur CPU non numÃ©rique: ${vps_cpu_cores}"
        vps_cpu_cores=$(echo "${vps_cpu_cores}" | tr -cd '0-9' || echo "0")
    fi

    if [[ ! "${vps_memory_total}" =~ ^[0-9]+$ ]]; then
        log "DEBUG" "Nettoyage de la valeur RAM non numÃ©rique: ${vps_memory_total}"
        vps_memory_total=$(echo "${vps_memory_total}" | tr -cd '0-9' || echo "0")
    fi

    if [[ ! "${vps_disk_free}" =~ ^[0-9]+$ ]]; then
        log "DEBUG" "Nettoyage de la valeur disque non numÃ©rique: ${vps_disk_free}"
        vps_disk_free=$(echo "${vps_disk_free}" | tr -cd '0-9' || echo "0")
    fi

    # Log des valeurs nettoyÃ©es pour le dÃ©bogage
    log "DEBUG" "Valeurs aprÃ¨s nettoyage: CPU=${vps_cpu_cores}, RAM=${vps_memory_total}, Disk=${vps_disk_free}"

    # VÃ©rification que les valeurs sont des nombres valides et non nuls
    if [[ -z "${vps_cpu_cores}" || "${vps_cpu_cores}" == "0" ]]; then
        log "WARNING" "Impossible de dÃ©terminer le nombre de cÅ“urs CPU du VPS"
        vps_cpu_cores=0
    fi

    if [[ -z "${vps_memory_total}" || "${vps_memory_total}" == "0" ]]; then
        log "WARNING" "Impossible de dÃ©terminer la mÃ©moire totale du VPS"
        vps_memory_total=0
    fi

    if [[ -z "${vps_disk_free}" || "${vps_disk_free}" == "0" ]]; then
        log "WARNING" "Impossible de dÃ©terminer l'espace disque libre du VPS"
        vps_disk_free=0
    fi

    # Affichage des ressources aprÃ¨s nettoyage des valeurs
    log "INFO" "Ressources du VPS (valeurs finales): ${vps_cpu_cores} cÅ“urs CPU, ${vps_memory_total}MB RAM, ${vps_disk_free}MB espace disque libre"

    # Log supplÃ©mentaire pour le dÃ©bogage des valeurs finales
    log "DEBUG" "Valeurs finales pour comparaison: CPU=${vps_cpu_cores}, RAM=${vps_memory_total}, Disk=${vps_disk_free}"

    if [[ ${vps_cpu_cores} -lt 2 && ${vps_cpu_cores} -ne 0 ]]; then
        log "WARNING" "Le VPS a moins de 2 cÅ“urs CPU (${vps_cpu_cores}), ce qui peut affecter les performances"
    fi

    if [[ ${vps_memory_total} -lt 4096 && ${vps_memory_total} -ne 0 ]]; then
        log "WARNING" "Le VPS a moins de 4GB de RAM (${vps_memory_total}MB), ce qui peut affecter les performances"
    fi

    if [[ ${vps_disk_free} -lt 20000 && ${vps_disk_free} -ne 0 ]]; then
        log "WARNING" "Le VPS a moins de 20GB d'espace disque libre (${vps_disk_free}MB), ce qui peut Ãªtre insuffisant"
    fi

    log "SUCCESS" "Tous les prÃ©requis sont satisfaits"

    # VÃ©rification de l'Ã©tat prÃ©cÃ©dent
    if [[ -f "${STATE_FILE}" ]]; then
        local previous_step
        previous_step=$(cat "${STATE_FILE}")
        log "INFO" "Ã‰tat prÃ©cÃ©dent dÃ©tectÃ©: ${previous_step}"
        log "INFO" "Voulez-vous reprendre Ã  partir de cette Ã©tape? (o/N)"

        read -r answer
        if [[ "${answer}" =~ ^[Oo]$ ]]; then
            log "INFO" "Reprise Ã  partir de l'Ã©tape: ${previous_step}"

            case "${previous_step}" in
                "init_vps")
                    initialiser_vps
                    installer_k3s
                    deployer_infrastructure_base
                    deployer_monitoring
                    verifier_installation
                    ;;
                "install_k3s")
                    installer_k3s
                    deployer_infrastructure_base
                    deployer_monitoring
                    verifier_installation
                    ;;
                "deploy_infra")
                    deployer_infrastructure_base
                    deployer_monitoring
                    verifier_installation
                    ;;
                "deploy_monitoring")
                    deployer_monitoring
                    verifier_installation
                    ;;
                "verify")
                    verifier_installation
                    ;;
                *)
                    log "WARNING" "Ã‰tape inconnue: ${previous_step}, reprise depuis le dÃ©but"
                    ;;
            esac

            # Nettoyage et sortie
            cleanup
            exit 0
        else
            log "INFO" "DÃ©marrage d'une nouvelle installation"
            rm -f "${STATE_FILE}"
        fi
    fi
}

# Fonction pour comparer des versions
function version_greater_equal() {
    local version1=$1
    local version2=$2

    # Normalisation des versions pour la comparaison
    local v1_parts=(${version1//./ })
    local v2_parts=(${version2//./ })

    # ComplÃ©ter les tableaux avec des zÃ©ros si nÃ©cessaire
    local max_length=$(( ${#v1_parts[@]} > ${#v2_parts[@]} ? ${#v1_parts[@]} : ${#v2_parts[@]} ))
    for ((i=${#v1_parts[@]}; i<max_length; i++)); do
        v1_parts[i]=0
    done
    for ((i=${#v2_parts[@]}; i<max_length; i++)); do
        v2_parts[i]=0
    done

    # Comparaison des parties de version
    for ((i=0; i<max_length; i++)); do
        if [[ ${v1_parts[i]} -gt ${v2_parts[i]} ]]; then
            return 0  # version1 > version2
        elif [[ ${v1_parts[i]} -lt ${v2_parts[i]} ]]; then
            return 1  # version1 < version2
        fi
    done

    return 0  # version1 == version2
}

# Fonction d'initialisation du VPS
function initialiser_vps() {
    log "INFO" "Initialisation du VPS..."
    INSTALLATION_STEP="init_vps"

    # Sauvegarde de l'Ã©tat actuel
    echo "${INSTALLATION_STEP}" > "${STATE_FILE}"

    # Sauvegarde de l'Ã©tat du VPS avant modification (optionnelle)
    backup_state "pre-init-vps" "true"

    # Construction de la commande Ansible
    # Utilisation de chemins absolus pour Ã©viter les problÃ¨mes de rÃ©solution de chemin
    local inventory_path="${LIONS_ANSIBLE_DIR:-${ANSIBLE_DIR}}/${inventory_file}"
    local playbook_path="${LIONS_ANSIBLE_DIR:-${ANSIBLE_DIR}}/playbooks/${LIONS_INIT_VPS_PLAYBOOK:-init-vps.yml}"

    # VÃ©rification que les fichiers existent
    if [[ ! -f "${inventory_path}" ]]; then
        log "ERROR" "Fichier d'inventaire non trouvÃ©: ${inventory_path}"
        log "ERROR" "VÃ©rifiez que le chemin est correct et que le fichier existe"
        return 1
    fi

    if [[ ! -f "${playbook_path}" ]]; then
        log "ERROR" "Fichier de playbook non trouvÃ©: ${playbook_path}"
        log "ERROR" "VÃ©rifiez que le chemin est correct et que le fichier existe"
        return 1
    fi

    # DÃ©tection du systÃ¨me d'exploitation pour le formatage des chemins
    local os_name
    os_name=$(uname -s)
    if [[ "${os_name}" == *"MINGW"* || "${os_name}" == *"MSYS"* || "${os_name}" == *"CYGWIN"* || "${os_name}" == *"Windows"* ]]; then
        # Windows: convertir les chemins Unix en chemins Windows
        log "DEBUG" "SystÃ¨me Windows dÃ©tectÃ©, conversion des chemins"

        # VÃ©rifier si les chemins contiennent dÃ©jÃ  des backslashes
        if [[ "${inventory_path}" != *"\\"* ]]; then
            # Remplacer les slashes par des backslashes pour Windows
            inventory_path=$(echo "${inventory_path}" | tr '/' '\\')
            log "DEBUG" "Chemin d'inventaire converti: ${inventory_path}"
        fi

        if [[ "${playbook_path}" != *"\\"* ]]; then
            # Remplacer les slashes par des backslashes pour Windows
            playbook_path=$(echo "${playbook_path}" | tr '/' '\\')
            log "DEBUG" "Chemin de playbook converti: ${playbook_path}"
        fi

        # VÃ©rifier si les chemins existent aprÃ¨s conversion
        if [[ ! -f "${inventory_path}" ]]; then
            log "WARNING" "Le chemin d'inventaire converti n'existe pas: ${inventory_path}"
            log "DEBUG" "Tentative d'utilisation du chemin original"
            inventory_path="${LIONS_ANSIBLE_DIR:-${ANSIBLE_DIR}}/${inventory_file}"
        fi

        if [[ ! -f "${playbook_path}" ]]; then
            log "WARNING" "Le chemin de playbook converti n'existe pas: ${playbook_path}"
            log "DEBUG" "Tentative d'utilisation du chemin original"
            playbook_path="${LIONS_ANSIBLE_DIR:-${ANSIBLE_DIR}}/playbooks/${LIONS_INIT_VPS_PLAYBOOK:-init-vps.yml}"
        fi
    fi

    # Construction de la commande Ansible avec options configurables
    local ansible_cmd="ansible-playbook -i \"${inventory_path}\" \"${playbook_path}\""

    # Ajout des options supplÃ©mentaires configurables
    if [[ "${LIONS_ANSIBLE_ASK_BECOME_PASS:-true}" == "true" && "${IS_LOCAL_EXECUTION}" != "true" ]]; then
        ansible_cmd="${ansible_cmd} --ask-become-pass"
    fi

    if [[ "${LIONS_ANSIBLE_FORKS:-}" != "" ]]; then
        ansible_cmd="${ansible_cmd} --forks=${LIONS_ANSIBLE_FORKS}"
    fi

    if [[ "${LIONS_ANSIBLE_EXTRA_VARS:-}" != "" ]]; then
        ansible_cmd="${ansible_cmd} --extra-vars \"${LIONS_ANSIBLE_EXTRA_VARS}\""
    fi

    if [[ "${debug_mode}" == "true" || "${LIONS_ANSIBLE_VERBOSE:-false}" == "true" ]]; then
        ansible_cmd="${ansible_cmd} -vvv"
    fi

    log "INFO" "ExÃ©cution de la commande: ${ansible_cmd}"
    LAST_COMMAND="${ansible_cmd}"

    # ExÃ©cution de la commande avec timeout configurable
    local timeout="${LIONS_ANSIBLE_TIMEOUT:-${TIMEOUT_SECONDS}}"
    if run_with_timeout "${ansible_cmd}" "${timeout}" "ansible_playbook"; then
        log "SUCCESS" "Initialisation du VPS terminÃ©e avec succÃ¨s"

        # VÃ©rification de l'Ã©tat du VPS aprÃ¨s initialisation
        local ssh_timeout="${LIONS_SSH_CONNECT_TIMEOUT:-5}"
        local ssh_port="${LIONS_VPS_PORT:-${ansible_port}}"
        local ssh_user="${LIONS_VPS_USER:-${ansible_user}}"
        local ssh_host="${LIONS_VPS_HOST:-${ansible_host}}"

        # Liste des services Ã  vÃ©rifier (configurable)
        local services_to_check="${LIONS_INIT_SERVICES_CHECK:-sshd fail2ban ufw}"
        local services_check_cmd="sudo systemctl is-active --quiet ${services_to_check// / && sudo systemctl is-active --quiet }"

        if ! ssh -o BatchMode=yes -o ConnectTimeout=${ssh_timeout} -p "${ssh_port}" "${ssh_user}@${ssh_host}" "${services_check_cmd}" &>/dev/null; then
            log "WARNING" "Certains services essentiels ne sont pas actifs aprÃ¨s l'initialisation"
            log "WARNING" "VÃ©rifiez manuellement l'Ã©tat des services sur le VPS"
        else
            log "INFO" "Services essentiels actifs et fonctionnels"
        fi
    else
        log "ERROR" "Ã‰chec de l'initialisation du VPS"

        # VÃ©rification des erreurs courantes
        local ssh_timeout="${LIONS_SSH_CONNECT_TIMEOUT:-5}"
        local ssh_port="${LIONS_VPS_PORT:-${ansible_port}}"
        local ssh_user="${LIONS_VPS_USER:-${ansible_user}}"
        local ssh_host="${LIONS_VPS_HOST:-${ansible_host}}"
        local ansible_log_path="${LIONS_ANSIBLE_LOG_PATH:-/var/log/ansible.log}"

        if ssh -o BatchMode=yes -o ConnectTimeout=${ssh_timeout} -p "${ssh_port}" "${ssh_user}@${ssh_host}" "sudo grep -i 'failed=' ${ansible_log_path} 2>/dev/null | tail -10" &>/dev/null; then
            log "INFO" "DerniÃ¨res erreurs Ansible sur le VPS:"
            ssh -o ConnectTimeout=${ssh_timeout} -p "${ssh_port}" "${ssh_user}@${ssh_host}" "sudo grep -i 'failed=' ${ansible_log_path} 2>/dev/null | tail -10" 2>/dev/null || true
        fi

        # Tentative de diagnostic
        log "INFO" "Tentative de diagnostic des problÃ¨mes..."

        # VÃ©rification des droits sudo
        if ! ssh -o BatchMode=yes -o ConnectTimeout=${ssh_timeout} -p "${ssh_port}" "${ssh_user}@${ssh_host}" "sudo -n true" &>/dev/null; then
            log "ERROR" "L'utilisateur ${ssh_user} n'a pas les droits sudo sans mot de passe"
            log "ERROR" "Assurez-vous que l'utilisateur est configurÃ© correctement dans le fichier sudoers"
        fi

        # VÃ©rification de l'espace disque sur le VPS
        local disk_info
        disk_info=$(ssh -o ConnectTimeout=${ssh_timeout} -p "${ssh_port}" "${ssh_user}@${ssh_host}" "sudo df -h /" 2>/dev/null || echo "Impossible de vÃ©rifier l'espace disque")
        log "INFO" "Espace disque sur le VPS:"
        echo "${disk_info}"

        # VÃ©rification de la mÃ©moire disponible
        local memory_info
        memory_info=$(ssh -o ConnectTimeout=${ssh_timeout} -p "${ssh_port}" "${ssh_user}@${ssh_host}" "free -h" 2>/dev/null || echo "Impossible de vÃ©rifier la mÃ©moire")
        log "INFO" "MÃ©moire disponible sur le VPS:"
        echo "${memory_info}"

        cleanup
        exit 1
    fi
}

# Fonction d'installation de K3s
function check_k3s_logs() {
    log "INFO" "VÃ©rification des journaux du service K3s..."

    # VÃ©rifier si on peut accÃ©der au VPS
    if ! ssh -o BatchMode=yes -o ConnectTimeout=5 -p "${ansible_port}" "${ansible_user}@${ansible_host}" "echo 'Connexion rÃ©ussie'" &>/dev/null; then
        log "ERROR" "Impossible de se connecter au VPS pour vÃ©rifier les journaux K3s"
        return 1
    fi

    # Afficher les 20 derniÃ¨res lignes des journaux K3s
    local k3s_logs
    k3s_logs=$(ssh -o ConnectTimeout=5 -p "${ansible_port}" "${ansible_user}@${ansible_host}" "sudo journalctl -u k3s -n 20 --no-pager" 2>/dev/null || echo "Impossible de rÃ©cupÃ©rer les journaux")
    log "INFO" "DerniÃ¨res lignes des journaux K3s:"
    echo "${k3s_logs}"

    # Rechercher des erreurs spÃ©cifiques
    local error_logs
    error_logs=$(ssh -o ConnectTimeout=5 -p "${ansible_port}" "${ansible_user}@${ansible_host}" "sudo journalctl -u k3s -n 100 | grep -i 'error\|failed\|fatal'" 2>/dev/null || echo "")

    if [[ -n "${error_logs}" ]]; then
        log "WARNING" "Des erreurs ont Ã©tÃ© dÃ©tectÃ©es dans les journaux K3s"
        log "WARNING" "Voici les erreurs dÃ©tectÃ©es:"
        echo "${error_logs}"
        return 1
    else
        log "SUCCESS" "Aucune erreur majeure dÃ©tectÃ©e dans les journaux K3s"
        return 0
    fi
}

function check_k3s_system_resources() {
    log "INFO" "VÃ©rification des ressources systÃ¨me pour K3s..."

    # VÃ©rifier si on peut accÃ©der au VPS
    if ! ssh -o BatchMode=yes -o ConnectTimeout=5 -p "${ansible_port}" "${ansible_user}@${ansible_host}" "echo 'Connexion rÃ©ussie'" &>/dev/null; then
        log "ERROR" "Impossible de se connecter au VPS pour vÃ©rifier les ressources systÃ¨me"
        return 1
    fi

    # VÃ©rifier l'espace disque
    local disk_usage
    disk_usage=$(ssh -o ConnectTimeout=5 -p "${ansible_port}" "${ansible_user}@${ansible_host}" "df -h / | awk 'NR==2 {print \$5}' | sed 's/%//'" 2>/dev/null || echo "Erreur")

    if [[ "${disk_usage}" == "Erreur" ]]; then
        log "ERROR" "Impossible de vÃ©rifier l'espace disque"
    elif [[ "${disk_usage}" -gt 90 ]]; then
        log "ERROR" "Espace disque critique: ${disk_usage}%"
    elif [[ "${disk_usage}" -gt 80 ]]; then
        log "WARNING" "Espace disque faible: ${disk_usage}%"
    else
        log "SUCCESS" "Espace disque suffisant: ${disk_usage}%"
    fi

    # VÃ©rifier la mÃ©moire
    local free_mem
    free_mem=$(ssh -o ConnectTimeout=5 -p "${ansible_port}" "${ansible_user}@${ansible_host}" "free -m | awk 'NR==2 {print \$4}'" 2>/dev/null || echo "Erreur")

    if [[ "${free_mem}" == "Erreur" ]]; then
        log "ERROR" "Impossible de vÃ©rifier la mÃ©moire disponible"
    elif [[ "${free_mem}" -lt 512 ]]; then
        log "ERROR" "MÃ©moire disponible critique: ${free_mem} MB"
    elif [[ "${free_mem}" -lt 1024 ]]; then
        log "WARNING" "MÃ©moire disponible faible: ${free_mem} MB"
    else
        log "SUCCESS" "MÃ©moire disponible suffisante: ${free_mem} MB"
    fi

    # VÃ©rifier la charge CPU
    local load_avg
    load_avg=$(ssh -o ConnectTimeout=5 -p "${ansible_port}" "${ansible_user}@${ansible_host}" "cat /proc/loadavg | awk '{print \$1}'" 2>/dev/null || echo "Erreur")

    if [[ "${load_avg}" == "Erreur" ]]; then
        log "ERROR" "Impossible de vÃ©rifier la charge CPU"
    elif (( $(echo "${load_avg} > 2.0" | bc -l) )); then
        log "WARNING" "Charge CPU Ã©levÃ©e: ${load_avg}"
    else
        log "SUCCESS" "Charge CPU normale: ${load_avg}"
    fi

    return 0
}

function restart_k3s_service() {
    log "INFO" "RedÃ©marrage du service K3s..."

    # VÃ©rifier si on peut accÃ©der au VPS
    if ! ssh -o BatchMode=yes -o ConnectTimeout=5 -p "${ansible_port}" "${ansible_user}@${ansible_host}" "echo 'Connexion rÃ©ussie'" &>/dev/null; then
        log "ERROR" "Impossible de se connecter au VPS pour redÃ©marrer K3s"
        return 1
    fi

    # VÃ©rifier l'Ã©tat actuel du service K3s
    local k3s_status
    k3s_status=$(ssh -o ConnectTimeout=5 -p "${ansible_port}" "${ansible_user}@${ansible_host}" "sudo systemctl status k3s" 2>/dev/null || echo "Impossible de rÃ©cupÃ©rer l'Ã©tat du service")
    log "DEBUG" "Ã‰tat actuel du service K3s avant redÃ©marrage:"
    echo "${k3s_status}" | head -10

    # VÃ©rifier les ressources systÃ¨me avant le redÃ©marrage
    check_k3s_system_resources

    # VÃ©rifier et corriger les drapeaux dÃ©prÃ©ciÃ©s avant le redÃ©marrage
    log "INFO" "VÃ©rification et correction des drapeaux dÃ©prÃ©ciÃ©s avant le redÃ©marrage..."
    fix_k3s_deprecated_flags

    # Recharger le daemon systemd aprÃ¨s correction des drapeaux dÃ©prÃ©ciÃ©s
    ssh -o ConnectTimeout=5 -p "${ansible_port}" "${ansible_user}@${ansible_host}" "sudo systemctl daemon-reload" &>/dev/null

    # RedÃ©marrer le service K3s avec capture des erreurs
    local restart_output
    restart_output=$(ssh -o ConnectTimeout=10 -p "${ansible_port}" "${ansible_user}@${ansible_host}" "sudo systemctl restart k3s 2>&1" || echo "Ã‰chec du redÃ©marrage")

    if [[ "${restart_output}" == *"failed"* || "${restart_output}" == *"Ã‰chec"* ]]; then
        log "ERROR" "Ã‰chec du redÃ©marrage du service K3s"
        log "ERROR" "Message d'erreur: ${restart_output}"

        # RÃ©cupÃ©rer les journaux du service pour diagnostiquer le problÃ¨me
        local journal_output
        journal_output=$(ssh -o ConnectTimeout=5 -p "${ansible_port}" "${ansible_user}@${ansible_host}" "sudo journalctl -u k3s -n 20 --no-pager" 2>/dev/null || echo "Impossible de rÃ©cupÃ©rer les journaux")
        log "DEBUG" "Journaux rÃ©cents du service K3s:"
        echo "${journal_output}"

        # VÃ©rifier les problÃ¨mes courants
        if [[ "${journal_output}" == *"port is already allocated"* ]]; then
            log "WARNING" "Un port requis par K3s est dÃ©jÃ  utilisÃ©"
            log "INFO" "Tentative de libÃ©ration des ports..."
            ssh -o ConnectTimeout=5 -p "${ansible_port}" "${ansible_user}@${ansible_host}" "sudo netstat -tulpn | grep 6443" 2>/dev/null
        elif [[ "${journal_output}" == *"insufficient memory"* || "${journal_output}" == *"cannot allocate memory"* ]]; then
            log "WARNING" "MÃ©moire insuffisante pour dÃ©marrer K3s"
            log "INFO" "VÃ©rification de la mÃ©moire disponible..."
            ssh -o ConnectTimeout=5 -p "${ansible_port}" "${ansible_user}@${ansible_host}" "free -m" 2>/dev/null
        elif [[ "${journal_output}" == *"permission denied"* ]]; then
            log "WARNING" "ProblÃ¨me de permissions dÃ©tectÃ©"
            log "INFO" "Tentative de correction des permissions..."
            ssh -o ConnectTimeout=5 -p "${ansible_port}" "${ansible_user}@${ansible_host}" "sudo chmod -R 755 /var/lib/rancher/k3s 2>/dev/null || true" &>/dev/null
            ssh -o ConnectTimeout=5 -p "${ansible_port}" "${ansible_user}@${ansible_host}" "sudo chmod 600 /etc/rancher/k3s/k3s.yaml 2>/dev/null || true" &>/dev/null
            # Nouvelle tentative aprÃ¨s correction des permissions
            log "INFO" "Nouvelle tentative de redÃ©marrage aprÃ¨s correction des permissions..."
            ssh -o ConnectTimeout=5 -p "${ansible_port}" "${ansible_user}@${ansible_host}" "sudo systemctl daemon-reload && sudo systemctl restart k3s" &>/dev/null
        fi

        return 1
    fi

    # Attendre que le service dÃ©marre
    log "INFO" "Attente du dÃ©marrage du service K3s..."
    local max_wait=30
    local waited=0
    local is_active=false

    while [[ "${waited}" -lt "${max_wait}" && "${is_active}" == "false" ]]; do
        if ssh -o ConnectTimeout=5 -p "${ansible_port}" "${ansible_user}@${ansible_host}" "sudo systemctl is-active --quiet k3s" &>/dev/null; then
            is_active=true
        else
            sleep 2
            waited=$((waited + 2))
            log "INFO" "En attente du dÃ©marrage de K3s... (${waited}/${max_wait}s)"
        fi
    done

    # VÃ©rifier si le service est actif aprÃ¨s le redÃ©marrage
    if [[ "${is_active}" == "false" ]]; then
        log "ERROR" "Le service K3s n'est pas actif aprÃ¨s le redÃ©marrage (timeout aprÃ¨s ${max_wait}s)"

        # RÃ©cupÃ©rer l'Ã©tat actuel du service pour diagnostiquer le problÃ¨me
        local current_status
        current_status=$(ssh -o ConnectTimeout=5 -p "${ansible_port}" "${ansible_user}@${ansible_host}" "sudo systemctl status k3s" 2>/dev/null || echo "Impossible de rÃ©cupÃ©rer l'Ã©tat du service")
        log "DEBUG" "Ã‰tat actuel du service K3s aprÃ¨s tentative de redÃ©marrage:"
        echo "${current_status}" | head -10

        return 1
    else
        log "SUCCESS" "Le service K3s a Ã©tÃ© redÃ©marrÃ© avec succÃ¨s"

        # VÃ©rifier que les composants essentiels sont en cours d'exÃ©cution
        log "INFO" "VÃ©rification des composants K3s..."
        local pods_status
        pods_status=$(ssh -o ConnectTimeout=5 -p "${ansible_port}" "${ansible_user}@${ansible_host}" "sudo kubectl get pods -n kube-system" 2>/dev/null || echo "Impossible de vÃ©rifier les pods")
        log "DEBUG" "Ã‰tat des pods systÃ¨me:"
        echo "${pods_status}" | head -10

        return 0
    fi
}

function fix_k3s_deprecated_flags() {
    log "INFO" "VÃ©rification et correction des drapeaux dÃ©prÃ©ciÃ©s dans la configuration K3s..."

    # VÃ©rifier si on est en exÃ©cution locale
    if [[ "${IS_LOCAL_EXECUTION}" == "true" ]]; then
        log "INFO" "ExÃ©cution locale dÃ©tectÃ©e, utilisation des commandes locales"
    else
        # VÃ©rifier si on peut accÃ©der au VPS
        if ! ssh -o BatchMode=yes -o ConnectTimeout=5 -p "${ansible_port}" "${ansible_user}@${ansible_host}" "echo 'Connexion rÃ©ussie'" &>/dev/null; then
            log "ERROR" "Impossible de se connecter au VPS pour corriger les drapeaux dÃ©prÃ©ciÃ©s"
            return 1
        fi
    fi

    # VÃ©rifier l'existence du fichier de service K3s
    local service_exists
    if [[ "${IS_LOCAL_EXECUTION}" == "true" ]]; then
        # ExÃ©cution locale
        service_exists=$(test -f /etc/systemd/system/k3s.service && echo 'true' || echo 'false')
    else
        # ExÃ©cution distante
        service_exists=$(ssh -o ConnectTimeout=5 -p "${ansible_port}" "${ansible_user}@${ansible_host}" "test -f /etc/systemd/system/k3s.service && echo 'true' || echo 'false'" 2>/dev/null)
    fi

    if [[ "${service_exists}" == "true" ]]; then
        log "INFO" "Fichier de service K3s trouvÃ©, vÃ©rification des drapeaux dÃ©prÃ©ciÃ©s..."

        # VÃ©rifier si le fichier contient des drapeaux dÃ©prÃ©ciÃ©s
        local contains_deprecated
        if [[ "${IS_LOCAL_EXECUTION}" == "true" ]]; then
            # ExÃ©cution locale
            contains_deprecated=$(grep -q -- '--no-deploy' /etc/systemd/system/k3s.service && echo 'true' || echo 'false')
        else
            # ExÃ©cution distante
            contains_deprecated=$(ssh -o ConnectTimeout=5 -p "${ansible_port}" "${ansible_user}@${ansible_host}" "grep -q -- '--no-deploy' /etc/systemd/system/k3s.service && echo 'true' || echo 'false'" 2>/dev/null)
        fi

        if [[ "${contains_deprecated}" == "true" ]]; then
            log "WARNING" "Drapeaux dÃ©prÃ©ciÃ©s trouvÃ©s dans le fichier de service K3s"
            log "INFO" "Remplacement des drapeaux dÃ©prÃ©ciÃ©s..."

            # Remplacer les drapeaux dÃ©prÃ©ciÃ©s (plusieurs formats possibles)
            if [[ "${IS_LOCAL_EXECUTION}" == "true" ]]; then
                # ExÃ©cution locale
                sudo sed -i 's/--no-deploy /--disable=/g; s/--no-deploy=/--disable=/g; s/--no-deploy\([[:space:]]\+\)\([[:alnum:]]\+\)/--disable=\2/g' /etc/systemd/system/k3s.service &>/dev/null

                # Recharger le daemon systemd
                sudo systemctl daemon-reload &>/dev/null
            else
                # ExÃ©cution distante
                ssh -o ConnectTimeout=5 -p "${ansible_port}" "${ansible_user}@${ansible_host}" "sudo sed -i 's/--no-deploy /--disable=/g; s/--no-deploy=/--disable=/g; s/--no-deploy\([[:space:]]\+\)\([[:alnum:]]\+\)/--disable=\2/g' /etc/systemd/system/k3s.service" &>/dev/null

                # Recharger le daemon systemd
                ssh -o ConnectTimeout=5 -p "${ansible_port}" "${ansible_user}@${ansible_host}" "sudo systemctl daemon-reload" &>/dev/null
            fi

            log "SUCCESS" "Drapeaux dÃ©prÃ©ciÃ©s remplacÃ©s avec succÃ¨s"
            return 0
        else
            log "INFO" "Aucun drapeau dÃ©prÃ©ciÃ© trouvÃ© dans le fichier de service K3s"
        fi
    else
        log "WARNING" "Fichier de service K3s non trouvÃ© (/etc/systemd/system/k3s.service)"

        # VÃ©rifier s'il existe dans un autre emplacement
        local alt_service_exists
        if [[ "${IS_LOCAL_EXECUTION}" == "true" ]]; then
            # ExÃ©cution locale
            alt_service_exists=$(find /etc/systemd/system -name 'k3s*.service' | wc -l)
        else
            # ExÃ©cution distante
            alt_service_exists=$(ssh -o ConnectTimeout=5 -p "${ansible_port}" "${ansible_user}@${ansible_host}" "find /etc/systemd/system -name 'k3s*.service' | wc -l" 2>/dev/null)
        fi

        if [[ "${alt_service_exists}" -gt 0 ]]; then
            log "INFO" "Fichiers de service K3s alternatifs trouvÃ©s, vÃ©rification des drapeaux dÃ©prÃ©ciÃ©s..."

            # Obtenir la liste des fichiers de service K3s
            local service_files
            if [[ "${IS_LOCAL_EXECUTION}" == "true" ]]; then
                # ExÃ©cution locale
                service_files=$(find /etc/systemd/system -name 'k3s*.service')
            else
                # ExÃ©cution distante
                service_files=$(ssh -o ConnectTimeout=5 -p "${ansible_port}" "${ansible_user}@${ansible_host}" "find /etc/systemd/system -name 'k3s*.service'" 2>/dev/null)
            fi

            # Pour chaque fichier, vÃ©rifier et remplacer les drapeaux dÃ©prÃ©ciÃ©s
            echo "${service_files}" | while read -r service_file; do
                log "INFO" "VÃ©rification du fichier ${service_file}..."

                local file_contains_deprecated
                if [[ "${IS_LOCAL_EXECUTION}" == "true" ]]; then
                    # ExÃ©cution locale
                    file_contains_deprecated=$(grep -q -- '--no-deploy' "${service_file}" && echo 'true' || echo 'false')
                else
                    # ExÃ©cution distante
                    file_contains_deprecated=$(ssh -o ConnectTimeout=5 -p "${ansible_port}" "${ansible_user}@${ansible_host}" "grep -q -- '--no-deploy' \"${service_file}\" && echo 'true' || echo 'false'" 2>/dev/null)
                fi

                if [[ "${file_contains_deprecated}" == "true" ]]; then
                    log "WARNING" "Drapeaux dÃ©prÃ©ciÃ©s trouvÃ©s dans ${service_file}"
                    log "INFO" "Remplacement des drapeaux dÃ©prÃ©ciÃ©s..."

                    # Remplacer les drapeaux dÃ©prÃ©ciÃ©s (plusieurs formats possibles)
                    if [[ "${IS_LOCAL_EXECUTION}" == "true" ]]; then
                        # ExÃ©cution locale
                        sudo sed -i 's/--no-deploy /--disable=/g; s/--no-deploy=/--disable=/g; s/--no-deploy\([[:space:]]\+\)\([[:alnum:]]\+\)/--disable=\2/g' "${service_file}" &>/dev/null
                    else
                        # ExÃ©cution distante
                        ssh -o ConnectTimeout=5 -p "${ansible_port}" "${ansible_user}@${ansible_host}" "sudo sed -i 's/--no-deploy /--disable=/g; s/--no-deploy=/--disable=/g; s/--no-deploy\([[:space:]]\+\)\([[:alnum:]]\+\)/--disable=\2/g' \"${service_file}\"" &>/dev/null
                    fi

                    log "SUCCESS" "Drapeaux dÃ©prÃ©ciÃ©s remplacÃ©s avec succÃ¨s dans ${service_file}"
                else
                    log "INFO" "Aucun drapeau dÃ©prÃ©ciÃ© trouvÃ© dans ${service_file}"
                fi
            done

            # Recharger le daemon systemd
            if [[ "${IS_LOCAL_EXECUTION}" == "true" ]]; then
                # ExÃ©cution locale
                sudo systemctl daemon-reload &>/dev/null
            else
                # ExÃ©cution distante
                ssh -o ConnectTimeout=5 -p "${ansible_port}" "${ansible_user}@${ansible_host}" "sudo systemctl daemon-reload" &>/dev/null
            fi

            log "SUCCESS" "VÃ©rification et correction des drapeaux dÃ©prÃ©ciÃ©s terminÃ©es"
            return 0
        else
            log "WARNING" "Aucun fichier de service K3s trouvÃ© sur le systÃ¨me"
        fi
    fi

    return 0
}

function repair_k3s() {
    log "INFO" "Tentative de rÃ©paration de l'installation K3s..."

    # VÃ©rifier si on peut accÃ©der au VPS
    if ! ssh -o BatchMode=yes -o ConnectTimeout=5 -p "${ansible_port}" "${ansible_user}@${ansible_host}" "echo 'Connexion rÃ©ussie'" &>/dev/null; then
        log "ERROR" "Impossible de se connecter au VPS pour rÃ©parer K3s"
        return 1
    fi

    # VÃ©rifier les fichiers de configuration
    local config_exists
    config_exists=$(ssh -o ConnectTimeout=5 -p "${ansible_port}" "${ansible_user}@${ansible_host}" "test -f /etc/rancher/k3s/k3s.yaml && echo 'true' || echo 'false'" 2>/dev/null)

    if [[ "${config_exists}" == "false" ]]; then
        log "ERROR" "Fichier de configuration K3s manquant"
    fi

    # VÃ©rifier les permissions des rÃ©pertoires
    log "INFO" "VÃ©rification et correction des permissions des rÃ©pertoires K3s..."
    ssh -o ConnectTimeout=5 -p "${ansible_port}" "${ansible_user}@${ansible_host}" "sudo chmod 755 /var/lib/rancher/k3s 2>/dev/null || true" &>/dev/null
    ssh -o ConnectTimeout=5 -p "${ansible_port}" "${ansible_user}@${ansible_host}" "sudo chmod 600 /etc/rancher/k3s/k3s.yaml 2>/dev/null || true" &>/dev/null

    # VÃ©rifier et corriger les problÃ¨mes de rÃ©seau
    log "INFO" "VÃ©rification de la configuration rÃ©seau..."
    local cni_exists
    cni_exists=$(ssh -o ConnectTimeout=5 -p "${ansible_port}" "${ansible_user}@${ansible_host}" "ip addr show | grep -q 'cni0' && echo 'true' || echo 'false'" 2>/dev/null)

    if [[ "${cni_exists}" == "false" ]]; then
        log "WARNING" "Interface CNI non dÃ©tectÃ©e"
    fi

    # Corriger les drapeaux dÃ©prÃ©ciÃ©s dans la configuration K3s
    fix_k3s_deprecated_flags

    # RedÃ©marrer le service aprÃ¨s les rÃ©parations
    restart_k3s_service
    return $?
}

function reinstall_k3s() {
    log "WARNING" "La rÃ©installation de K3s est une opÃ©ration destructive"
    log "WARNING" "Toutes les donnÃ©es Kubernetes seront perdues"
    log "WARNING" "Assurez-vous d'avoir des sauvegardes avant de continuer"

    # Demander confirmation
    local confirm
    read -p "ÃŠtes-vous sÃ»r de vouloir rÃ©installer K3s? (oui/NON): " confirm
    if [[ "${confirm}" != "oui" ]]; then
        log "INFO" "RÃ©installation annulÃ©e"
        return 1
    fi

    # VÃ©rifier si on peut accÃ©der au VPS
    if ! ssh -o BatchMode=yes -o ConnectTimeout=5 -p "${ansible_port}" "${ansible_user}@${ansible_host}" "echo 'Connexion rÃ©ussie'" &>/dev/null; then
        log "ERROR" "Impossible de se connecter au VPS pour rÃ©installer K3s"
        return 1
    fi

    log "INFO" "DÃ©sinstallation de K3s..."
    if ! ssh -o ConnectTimeout=5 -p "${ansible_port}" "${ansible_user}@${ansible_host}" "sudo /usr/local/bin/k3s-uninstall.sh" &>/dev/null; then
        log "ERROR" "Ã‰chec de la dÃ©sinstallation de K3s"
        return 1
    fi

    log "INFO" "RÃ©installation de K3s..."
    # Utiliser le playbook Ansible pour rÃ©installer K3s
    local inventory_path="${ANSIBLE_DIR}/${inventory_file}"
    local playbook_path="${ANSIBLE_DIR}/playbooks/install-k3s.yml"

    # VÃ©rification que les fichiers existent
    if [[ ! -f "${inventory_path}" ]]; then
        log "ERROR" "Fichier d'inventaire non trouvÃ©: ${inventory_path}"
        return 1
    fi

    if [[ ! -f "${playbook_path}" ]]; then
        log "ERROR" "Fichier de playbook non trouvÃ©: ${playbook_path}"
        return 1
    fi

    local ansible_cmd="ansible-playbook -i \"${inventory_path}\" \"${playbook_path}\""

    # Ajouter l'option --ask-become-pass seulement si l'exÃ©cution n'est pas locale
    if [[ "${IS_LOCAL_EXECUTION}" != "true" ]]; then
        ansible_cmd="${ansible_cmd} --ask-become-pass"
    fi

    if [[ "${debug_mode}" == "true" ]]; then
        ansible_cmd="${ansible_cmd} -vvv"
    fi

    log "INFO" "ExÃ©cution de la commande: ${ansible_cmd}"
    LAST_COMMAND="${ansible_cmd}"

    # ExÃ©cution de la commande avec timeout
    if ! run_with_timeout "${ansible_cmd}" 3600 "ansible_playbook"; then
        log "ERROR" "Ã‰chec de la rÃ©installation de K3s"
        return 1
    fi

    # Correction des drapeaux dÃ©prÃ©ciÃ©s aprÃ¨s la rÃ©installation
    log "INFO" "VÃ©rification et correction des drapeaux dÃ©prÃ©ciÃ©s aprÃ¨s la rÃ©installation..."
    fix_k3s_deprecated_flags

    # RedÃ©marrage du service K3s aprÃ¨s correction des drapeaux dÃ©prÃ©ciÃ©s
    log "INFO" "RedÃ©marrage du service K3s aprÃ¨s correction des drapeaux dÃ©prÃ©ciÃ©s..."
    if [[ "${IS_LOCAL_EXECUTION}" == "true" ]]; then
        # ExÃ©cution locale
        sudo systemctl daemon-reload && sudo systemctl restart k3s &>/dev/null
    else
        # ExÃ©cution distante
        ssh -o ConnectTimeout=5 -p "${ansible_port}" "${ansible_user}@${ansible_host}" "sudo systemctl daemon-reload && sudo systemctl restart k3s" &>/dev/null
    fi

    # Attente que le service K3s soit prÃªt
    log "INFO" "Attente que le service K3s soit prÃªt..."
    sleep 10

    # VÃ©rifier si le service est actif aprÃ¨s la rÃ©installation
    if ! ssh -o ConnectTimeout=5 -p "${ansible_port}" "${ansible_user}@${ansible_host}" "sudo systemctl is-active --quiet k3s" &>/dev/null; then
        log "ERROR" "Le service K3s n'est pas actif aprÃ¨s la rÃ©installation"
        return 1
    else
        log "SUCCESS" "K3s a Ã©tÃ© rÃ©installÃ© avec succÃ¨s"

        # Configuration de kubectl pour l'utilisateur courant
        log "INFO" "Configuration de kubectl pour l'utilisateur courant..."
        mkdir -p "${HOME}/.kube"

        if ! scp -P "${ansible_port}" "${ansible_user}@${ansible_host}:/etc/rancher/k3s/k3s.yaml" "${HOME}/.kube/config" &>/dev/null; then
            log "WARNING" "Impossible de rÃ©cupÃ©rer le fichier kubeconfig"
            log "WARNING" "Vous devrez configurer kubectl manuellement"
        else
            # Remplacer localhost par l'adresse IP du VPS
            sed -i "s/127.0.0.1/${ansible_host}/g" "${HOME}/.kube/config"
            log "SUCCESS" "kubectl configurÃ© avec succÃ¨s"
        fi

        return 0
    fi
}

function check_fix_k3s() {
    log "INFO" "VÃ©rification et rÃ©paration du service K3s..."

    # VÃ©rifier si on peut accÃ©der au VPS
    if ! ssh -o BatchMode=yes -o ConnectTimeout=5 -p "${ansible_port}" "${ansible_user}@${ansible_host}" "echo 'Connexion rÃ©ussie'" &>/dev/null; then
        log "ERROR" "Impossible de se connecter au VPS pour vÃ©rifier K3s"
        return 1
    fi

    # VÃ©rifier l'Ã©tat du service K3s
    local k3s_active
    k3s_active=$(ssh -o ConnectTimeout=5 -p "${ansible_port}" "${ansible_user}@${ansible_host}" "sudo systemctl is-active k3s" 2>/dev/null || echo "unknown")

    # VÃ©rifier et corriger les drapeaux dÃ©prÃ©ciÃ©s, mÃªme si le service est actif
    log "INFO" "VÃ©rification des drapeaux dÃ©prÃ©ciÃ©s dans la configuration K3s..."
    fix_k3s_deprecated_flags

    if [[ "${k3s_active}" == "active" ]]; then
        log "SUCCESS" "Le service K3s est actif et en cours d'exÃ©cution"
        return 0
    else
        log "WARNING" "Le service K3s n'est pas actif (Ã©tat: ${k3s_active})"

        # VÃ©rifier les journaux et les ressources
        check_k3s_logs
        check_k3s_system_resources

        # Demander Ã  l'utilisateur quelle action entreprendre
        log "INFO" "Que souhaitez-vous faire?"
        echo "1. RedÃ©marrer le service K3s"
        echo "2. Tenter de rÃ©parer l'installation K3s"
        echo "3. RÃ©installer K3s (destructif)"
        echo "4. Quitter sans action"

        local choice
        read -p "Votre choix (1-4): " choice

        case $choice in
            1)
                restart_k3s_service
                ;;
            2)
                repair_k3s
                ;;
            3)
                reinstall_k3s
                ;;
            4)
                log "INFO" "Aucune action entreprise"
                return 1
                ;;
            *)
                log "ERROR" "Choix invalide"
                return 1
                ;;
        esac

        # VÃ©rification finale
        k3s_active=$(ssh -o ConnectTimeout=5 -p "${ansible_port}" "${ansible_user}@${ansible_host}" "sudo systemctl is-active k3s" 2>/dev/null || echo "unknown")

        if [[ "${k3s_active}" == "active" ]]; then
            log "SUCCESS" "Le service K3s est maintenant actif et en cours d'exÃ©cution"
            return 0
        else
            log "ERROR" "Le service K3s prÃ©sente toujours des problÃ¨mes (Ã©tat: ${k3s_active})"
            log "WARNING" "Consultez les journaux systÃ¨me pour plus d'informations"
            log "WARNING" "Vous pouvez Ã©galement essayer une rÃ©installation complÃ¨te"
            return 1
        fi
    fi
}

function installer_vault() {
    log "INFO" "Installation de HashiCorp Vault..."
    INSTALLATION_STEP="install_vault"

    # VÃ©rification si Vault est activÃ©
    if [[ "${LIONS_VAULT_ENABLED:-false}" != "true" ]]; then
        log "INFO" "Installation de Vault ignorÃ©e (LIONS_VAULT_ENABLED n'est pas dÃ©fini Ã  'true')"
        return 0
    fi

    # Sauvegarde de l'Ã©tat actuel
    echo "${INSTALLATION_STEP}" > "${STATE_FILE}"

    # Sauvegarde de l'Ã©tat du VPS avant modification (optionnelle)
    backup_state "pre-install-vault" "true"

    # Construction de la commande Ansible
    # Utilisation de chemins absolus pour Ã©viter les problÃ¨mes de rÃ©solution de chemin
    local inventory_path="${ANSIBLE_DIR}/${inventory_file}"
    local playbook_path="${ANSIBLE_DIR}/playbooks/install-vault.yml"

    # VÃ©rification que les fichiers existent
    if [[ ! -f "${inventory_path}" ]]; then
        log "ERROR" "Fichier d'inventaire non trouvÃ©: ${inventory_path}"
        log "ERROR" "VÃ©rifiez que le chemin est correct et que le fichier existe"
        return 1
    fi

    if [[ ! -f "${playbook_path}" ]]; then
        log "ERROR" "Fichier de playbook non trouvÃ©: ${playbook_path}"
        log "ERROR" "VÃ©rifiez que le chemin est correct et que le fichier existe"
        return 1
    fi

    # DÃ©tection du systÃ¨me d'exploitation pour le formatage des chemins
    local os_name
    os_name=$(uname -s)
    if [[ "${os_name}" == *"MINGW"* || "${os_name}" == *"MSYS"* || "${os_name}" == *"CYGWIN"* || "${os_name}" == *"Windows"* ]]; then
        # Windows: convertir les chemins Unix en chemins Windows
        log "DEBUG" "SystÃ¨me Windows dÃ©tectÃ©, conversion des chemins"

        # VÃ©rifier si les chemins contiennent dÃ©jÃ  des backslashes
        if [[ "${inventory_path}" != *"\\"* ]]; then
            # Remplacer les slashes par des backslashes pour Windows
            inventory_path=$(echo "${inventory_path}" | tr '/' '\\')
            log "DEBUG" "Chemin d'inventaire converti: ${inventory_path}"
        fi

        if [[ "${playbook_path}" != *"\\"* ]]; then
            # Remplacer les slashes par des backslashes pour Windows
            playbook_path=$(echo "${playbook_path}" | tr '/' '\\')
            log "DEBUG" "Chemin de playbook converti: ${playbook_path}"
        fi

        # VÃ©rifier si les chemins existent aprÃ¨s conversion
        if [[ ! -f "${inventory_path}" ]]; then
            log "WARNING" "Le chemin d'inventaire converti n'existe pas: ${inventory_path}"
            log "DEBUG" "Tentative d'utilisation du chemin original"
            inventory_path="${ANSIBLE_DIR}/${inventory_file}"
        fi

        if [[ ! -f "${playbook_path}" ]]; then
            log "WARNING" "Le chemin de playbook converti n'existe pas: ${playbook_path}"
            log "DEBUG" "Tentative d'utilisation du chemin original"
            playbook_path="${ANSIBLE_DIR}/playbooks/install-vault.yml"
        fi
    fi

    local ansible_cmd="ansible-playbook -i \"${inventory_path}\" \"${playbook_path}\""

    # Ajouter l'option --ask-become-pass seulement si l'exÃ©cution n'est pas locale
    if [[ "${IS_LOCAL_EXECUTION}" != "true" ]]; then
        ansible_cmd="${ansible_cmd} --ask-become-pass"
    fi

    if [[ "${debug_mode}" == "true" ]]; then
        ansible_cmd="${ansible_cmd} -vvv"
    fi

    log "INFO" "ExÃ©cution de la commande: ${ansible_cmd}"
    LAST_COMMAND="${ansible_cmd}"

    # ExÃ©cution de la commande avec timeout
    if run_with_timeout "${ansible_cmd}" 1800 "ansible_playbook"; then  # Timeout de 30 minutes pour l'installation de Vault
        log "SUCCESS" "Installation de HashiCorp Vault terminÃ©e avec succÃ¨s"

        # VÃ©rification de l'installation de Vault
        local vault_active=false
        if [[ "${IS_LOCAL_EXECUTION}" == "true" ]]; then
            # ExÃ©cution locale
            sudo systemctl is-active --quiet vault &>/dev/null && vault_active=true
        else
            # ExÃ©cution distante
            ssh -o BatchMode=yes -o ConnectTimeout=5 -p "${ansible_port}" "${ansible_user}@${ansible_host}" "sudo systemctl is-active --quiet vault" &>/dev/null && vault_active=true
        fi

        if [[ "${vault_active}" != "true" ]]; then
            log "WARNING" "Le service Vault ne semble pas Ãªtre actif aprÃ¨s l'installation"
            log "WARNING" "VÃ©rifiez manuellement l'Ã©tat du service sur le VPS"
            return 1
        else
            log "INFO" "Service Vault actif et fonctionnel"
            return 0
        fi
    else
        log "ERROR" "Ã‰chec de l'installation de HashiCorp Vault"
        log "ERROR" "DerniÃ¨re erreur: ${LAST_ERROR}"
        return 1
    fi
}

function installer_k3s() {
    log "INFO" "Installation de K3s..."
    INSTALLATION_STEP="install_k3s"

    # VÃ©rification de Vault si activÃ©
    if [[ "${LIONS_VAULT_ENABLED:-false}" == "true" ]]; then
        log "INFO" "Vault est activÃ©, vÃ©rification de son Ã©tat avant l'installation de K3s..."

        # VÃ©rification que Vault est installÃ© et accessible
        local vault_accessible=false
        local vault_initialized=false
        local vault_unsealed=false
        local vault_api_addr="${LIONS_VAULT_ADDR:-https://127.0.0.1:8200}"

        # VÃ©rification que le service Vault est actif
        if [[ "${IS_LOCAL_EXECUTION}" == "true" ]]; then
            # ExÃ©cution locale
            sudo systemctl is-active --quiet vault &>/dev/null && vault_accessible=true
        else
            # ExÃ©cution distante
            ssh -o BatchMode=yes -o ConnectTimeout=5 -p "${ansible_port}" "${ansible_user}@${ansible_host}" "sudo systemctl is-active --quiet vault" &>/dev/null && vault_accessible=true
        fi

        if [[ "${vault_accessible}" != "true" ]]; then
            log "WARNING" "Le service Vault n'est pas actif ou accessible"
            log "WARNING" "L'installation de K3s pourrait Ã©chouer si elle dÃ©pend de secrets stockÃ©s dans Vault"

            # Demander Ã  l'utilisateur s'il souhaite continuer
            read -p "Souhaitez-vous continuer l'installation de K3s sans Vault actif? (O/n): " continue_response
            if [[ "${continue_response}" =~ ^[Nn] ]]; then
                log "INFO" "Installation de K3s annulÃ©e par l'utilisateur"
                return 1
            fi

            log "INFO" "Continuation de l'installation sans Vault actif"
        else
            # VÃ©rification que Vault est initialisÃ© et dÃ©verrouillÃ©
            local vault_status_output
            if [[ "${IS_LOCAL_EXECUTION}" == "true" ]]; then
                # ExÃ©cution locale
                vault_status_output=$(VAULT_ADDR="${vault_api_addr}" VAULT_SKIP_VERIFY=true vault status -format=json 2>/dev/null)
            else
                # ExÃ©cution distante
                vault_status_output=$(ssh -o BatchMode=yes -o ConnectTimeout=5 -p "${ansible_port}" "${ansible_user}@${ansible_host}" "VAULT_ADDR='${vault_api_addr}' VAULT_SKIP_VERIFY=true vault status -format=json" 2>/dev/null)
            fi

            if [[ -n "${vault_status_output}" ]]; then
                vault_initialized=$(echo "${vault_status_output}" | jq -r '.initialized')
                vault_unsealed=$(echo "${vault_status_output}" | jq -r '.sealed')
                vault_unsealed=$([ "${vault_unsealed}" == "false" ] && echo "true" || echo "false")

                if [[ "${vault_initialized}" != "true" ]]; then
                    log "WARNING" "Vault n'est pas initialisÃ©"
                    log "WARNING" "L'installation de K3s pourrait Ã©chouer si elle dÃ©pend de secrets stockÃ©s dans Vault"
                elif [[ "${vault_unsealed}" != "true" ]]; then
                    log "WARNING" "Vault est scellÃ© (sealed) et inaccessible"
                    log "WARNING" "L'installation de K3s pourrait Ã©chouer si elle dÃ©pend de secrets stockÃ©s dans Vault"
                else
                    log "INFO" "Vault est correctement initialisÃ©, dÃ©verrouillÃ© et accessible"
                fi
            else
                log "WARNING" "Impossible d'obtenir le statut de Vault"
                log "WARNING" "L'installation de K3s pourrait Ã©chouer si elle dÃ©pend de secrets stockÃ©s dans Vault"
            fi
        fi
    fi

    # Sauvegarde de l'Ã©tat actuel
    echo "${INSTALLATION_STEP}" > "${STATE_FILE}"

    # Sauvegarde de l'Ã©tat du VPS avant modification (optionnelle)
    backup_state "pre-install-k3s" "true"

    # Construction de la commande Ansible
    # Utilisation de chemins absolus pour Ã©viter les problÃ¨mes de rÃ©solution de chemin
    local inventory_path="${ANSIBLE_DIR}/${inventory_file}"
    local playbook_path="${ANSIBLE_DIR}/playbooks/install-k3s.yml"

    # VÃ©rification que les fichiers existent
    if [[ ! -f "${inventory_path}" ]]; then
        log "ERROR" "Fichier d'inventaire non trouvÃ©: ${inventory_path}"
        log "ERROR" "VÃ©rifiez que le chemin est correct et que le fichier existe"
        return 1
    fi

    if [[ ! -f "${playbook_path}" ]]; then
        log "ERROR" "Fichier de playbook non trouvÃ©: ${playbook_path}"
        log "ERROR" "VÃ©rifiez que le chemin est correct et que le fichier existe"
        return 1
    fi

    # DÃ©tection du systÃ¨me d'exploitation pour le formatage des chemins
    local os_name
    os_name=$(uname -s)
    if [[ "${os_name}" == *"MINGW"* || "${os_name}" == *"MSYS"* || "${os_name}" == *"CYGWIN"* || "${os_name}" == *"Windows"* ]]; then
        # Windows: convertir les chemins Unix en chemins Windows
        log "DEBUG" "SystÃ¨me Windows dÃ©tectÃ©, conversion des chemins"

        # VÃ©rifier si les chemins contiennent dÃ©jÃ  des backslashes
        if [[ "${inventory_path}" != *"\\"* ]]; then
            # Remplacer les slashes par des backslashes pour Windows
            inventory_path=$(echo "${inventory_path}" | tr '/' '\\')
            log "DEBUG" "Chemin d'inventaire converti: ${inventory_path}"
        fi

        if [[ "${playbook_path}" != *"\\"* ]]; then
            # Remplacer les slashes par des backslashes pour Windows
            playbook_path=$(echo "${playbook_path}" | tr '/' '\\')
            log "DEBUG" "Chemin de playbook converti: ${playbook_path}"
        fi

        # VÃ©rifier si les chemins existent aprÃ¨s conversion
        if [[ ! -f "${inventory_path}" ]]; then
            log "WARNING" "Le chemin d'inventaire converti n'existe pas: ${inventory_path}"
            log "DEBUG" "Tentative d'utilisation du chemin original"
            inventory_path="${ANSIBLE_DIR}/${inventory_file}"
        fi

        if [[ ! -f "${playbook_path}" ]]; then
            log "WARNING" "Le chemin de playbook converti n'existe pas: ${playbook_path}"
            log "DEBUG" "Tentative d'utilisation du chemin original"
            playbook_path="${ANSIBLE_DIR}/playbooks/install-k3s.yml"
        fi
    fi

    local ansible_cmd="ansible-playbook -i \"${inventory_path}\" \"${playbook_path}\""

    # Ajouter l'option --ask-become-pass seulement si l'exÃ©cution n'est pas locale
    if [[ "${IS_LOCAL_EXECUTION}" != "true" ]]; then
        ansible_cmd="${ansible_cmd} --ask-become-pass"
    fi

    if [[ "${debug_mode}" == "true" ]]; then
        ansible_cmd="${ansible_cmd} -vvv"
    fi

    log "INFO" "ExÃ©cution de la commande: ${ansible_cmd}"
    LAST_COMMAND="${ansible_cmd}"

    # ExÃ©cution de la commande avec timeout
    if run_with_timeout "${ansible_cmd}" 3600 "ansible_playbook"; then  # Timeout plus long (1h) pour l'installation de K3s
        log "SUCCESS" "Installation de K3s terminÃ©e avec succÃ¨s"

        # Correction des drapeaux dÃ©prÃ©ciÃ©s dans la configuration K3s
        log "INFO" "VÃ©rification et correction des drapeaux dÃ©prÃ©ciÃ©s dans la configuration K3s..."
        fix_k3s_deprecated_flags

        # RedÃ©marrage du service K3s aprÃ¨s correction des drapeaux dÃ©prÃ©ciÃ©s
        log "INFO" "RedÃ©marrage du service K3s aprÃ¨s correction des drapeaux dÃ©prÃ©ciÃ©s..."
        if [[ "${IS_LOCAL_EXECUTION}" == "true" ]]; then
            # ExÃ©cution locale
            sudo systemctl daemon-reload && sudo systemctl restart k3s &>/dev/null
        else
            # ExÃ©cution distante
            ssh -o ConnectTimeout=5 -p "${ansible_port}" "${ansible_user}@${ansible_host}" "sudo systemctl daemon-reload && sudo systemctl restart k3s" &>/dev/null
        fi

        # Attente que le service K3s soit prÃªt
        log "INFO" "Attente que le service K3s soit prÃªt..."
        sleep 10

        # VÃ©rification de l'installation de K3s
        local k3s_active=false
        if [[ "${IS_LOCAL_EXECUTION}" == "true" ]]; then
            # ExÃ©cution locale
            sudo systemctl is-active --quiet k3s &>/dev/null && k3s_active=true
        else
            # ExÃ©cution distante
            ssh -o BatchMode=yes -o ConnectTimeout=5 -p "${ansible_port}" "${ansible_user}@${ansible_host}" "sudo systemctl is-active --quiet k3s" &>/dev/null && k3s_active=true
        fi

        if [[ "${k3s_active}" != "true" ]]; then
            log "WARNING" "Le service K3s ne semble pas Ãªtre actif aprÃ¨s l'installation"
            log "WARNING" "VÃ©rifiez manuellement l'Ã©tat du service sur le VPS"
        else
            log "INFO" "Service K3s actif et fonctionnel"

            # VÃ©rification des pods systÃ¨me
            local pods_status
            if [[ "${IS_LOCAL_EXECUTION}" == "true" ]]; then
                # ExÃ©cution locale
                pods_status=$(sudo kubectl get pods -n kube-system -o wide 2>/dev/null || echo "Impossible de vÃ©rifier les pods")
            else
                # ExÃ©cution distante
                pods_status=$(ssh -o ConnectTimeout=5 -p "${ansible_port}" "${ansible_user}@${ansible_host}" "sudo kubectl get pods -n kube-system -o wide" 2>/dev/null || echo "Impossible de vÃ©rifier les pods")
            fi
            log "INFO" "Ã‰tat des pods systÃ¨me:"
            echo "${pods_status}"

            # VÃ©rification de l'accÃ¨s au cluster depuis la machine locale
            if ! kubectl cluster-info &>/dev/null; then
                log "WARNING" "Impossible d'accÃ©der au cluster K3s depuis la machine locale"
                log "WARNING" "VÃ©rifiez votre configuration kubectl et le fichier kubeconfig"

                # Tentative de rÃ©cupÃ©ration du fichier kubeconfig
                local kubeconfig_dir="${HOME}/.kube"
                mkdir -p "${kubeconfig_dir}"

                if [[ "${IS_LOCAL_EXECUTION}" == "true" ]]; then
                    # ExÃ©cution locale
                    if [[ -f "/home/${ansible_user}/.kube/config" ]]; then
                        cp "/home/${ansible_user}/.kube/config" "${kubeconfig_dir}/config.k3s" &>/dev/null
                        log "INFO" "Fichier kubeconfig copiÃ© dans ${kubeconfig_dir}/config.k3s"
                        log "INFO" "Utilisez la commande: export KUBECONFIG=${kubeconfig_dir}/config.k3s"
                    else
                        log "ERROR" "Impossible de trouver le fichier kubeconfig local"
                    fi
                else
                    # ExÃ©cution distante
                    if scp -P "${ansible_port}" "${ansible_user}@${ansible_host}:/home/${ansible_user}/.kube/config" "${kubeconfig_dir}/config.k3s" &>/dev/null; then
                        log "INFO" "Fichier kubeconfig rÃ©cupÃ©rÃ© dans ${kubeconfig_dir}/config.k3s"
                        log "INFO" "Utilisez la commande: export KUBECONFIG=${kubeconfig_dir}/config.k3s"
                    else
                        log "ERROR" "Impossible de rÃ©cupÃ©rer le fichier kubeconfig"
                    fi
                fi
            else
                log "INFO" "AccÃ¨s au cluster K3s depuis la machine locale vÃ©rifiÃ© avec succÃ¨s"
            fi
        fi
    else
        log "ERROR" "Ã‰chec de l'installation de K3s"

        # VÃ©rification des erreurs courantes
        if [[ "${IS_LOCAL_EXECUTION}" == "true" ]]; then
            # ExÃ©cution locale
            if sudo grep -i 'failed=' /var/log/ansible.log 2>/dev/null | tail -10 &>/dev/null; then
                log "INFO" "DerniÃ¨res erreurs Ansible sur le VPS:"
                sudo grep -i 'failed=' /var/log/ansible.log 2>/dev/null | tail -10 2>/dev/null || true
            fi
        else
            # ExÃ©cution distante
            if ssh -o BatchMode=yes -o ConnectTimeout=5 -p "${ansible_port}" "${ansible_user}@${ansible_host}" "sudo grep -i 'failed=' /var/log/ansible.log 2>/dev/null | tail -10" &>/dev/null; then
                log "INFO" "DerniÃ¨res erreurs Ansible sur le VPS:"
                ssh -o ConnectTimeout=5 -p "${ansible_port}" "${ansible_user}@${ansible_host}" "sudo grep -i 'failed=' /var/log/ansible.log 2>/dev/null | tail -10" 2>/dev/null || true
            fi
        fi

        # VÃ©rification des logs de K3s
        local k3s_logs
        if [[ "${IS_LOCAL_EXECUTION}" == "true" ]]; then
            # ExÃ©cution locale
            k3s_logs=$(sudo journalctl -u k3s --no-pager -n 50 2>/dev/null || echo "Impossible de rÃ©cupÃ©rer les logs de K3s")
        else
            # ExÃ©cution distante
            k3s_logs=$(ssh -o ConnectTimeout=5 -p "${ansible_port}" "${ansible_user}@${ansible_host}" "sudo journalctl -u k3s --no-pager -n 50" 2>/dev/null || echo "Impossible de rÃ©cupÃ©rer les logs de K3s")
        fi
        log "INFO" "Derniers logs de K3s:"
        echo "${k3s_logs}"

        # VÃ©rification des ports requis pour K3s
        log "INFO" "VÃ©rification des ports requis pour K3s..."
        local k3s_ports=(6443 10250 10251 10252 8472 4789 51820 51821)

        for port in "${k3s_ports[@]}"; do
            if [[ "${IS_LOCAL_EXECUTION}" == "true" ]]; then
                # ExÃ©cution locale
                if ! ss -tuln | grep ":${port}" &>/dev/null; then
                    log "WARNING" "Le port ${port} n'est pas ouvert sur le VPS, ce qui peut causer des problÃ¨mes avec K3s"
                fi
            else
                # ExÃ©cution distante
                if ! ssh -o BatchMode=yes -o ConnectTimeout=5 -p "${ansible_port}" "${ansible_user}@${ansible_host}" "ss -tuln | grep :${port}" &>/dev/null; then
                    log "WARNING" "Le port ${port} n'est pas ouvert sur le VPS, ce qui peut causer des problÃ¨mes avec K3s"
                fi
            fi
        done

        # VÃ©rification des prÃ©requis systÃ¨me pour K3s
        log "INFO" "VÃ©rification des prÃ©requis systÃ¨me pour K3s..."
        local system_info
        if [[ "${IS_LOCAL_EXECUTION}" == "true" ]]; then
            # ExÃ©cution locale
            system_info=$(uname -a && cat /etc/os-release | grep PRETTY_NAME && free -h && df -h / && sysctl -a | grep -E 'vm.max_map_count|net.ipv4.ip_forward' 2>/dev/null || echo "Impossible de rÃ©cupÃ©rer les informations systÃ¨me")
        else
            # ExÃ©cution distante
            system_info=$(ssh -o BatchMode=yes -o ConnectTimeout=5 -p "${ansible_port}" "${ansible_user}@${ansible_host}" "uname -a && cat /etc/os-release | grep PRETTY_NAME && free -h && df -h / && sysctl -a | grep -E 'vm.max_map_count|net.ipv4.ip_forward'" 2>/dev/null || echo "Impossible de rÃ©cupÃ©rer les informations systÃ¨me")
        fi
        log "INFO" "Informations systÃ¨me:"
        echo "${system_info}"

        cleanup
        exit 1
    fi
}

# Fonction de dÃ©ploiement de l'infrastructure de base
function deployer_infrastructure_base() {
    log "INFO" "DÃ©ploiement de l'infrastructure de base..."
    INSTALLATION_STEP="deploy_infra"

    # Sauvegarde de l'Ã©tat actuel
    echo "${INSTALLATION_STEP}" > "${STATE_FILE}"

    # Configuration du KUBECONFIG pour l'exÃ©cution locale
    if [[ "${IS_LOCAL_EXECUTION}" == "true" ]]; then
        log "INFO" "ExÃ©cution locale dÃ©tectÃ©e, configuration du KUBECONFIG pour K3s..."
        if [[ -f "/etc/rancher/k3s/k3s.yaml" ]]; then
            log "INFO" "Fichier kubeconfig K3s trouvÃ© Ã  /etc/rancher/k3s/k3s.yaml"
            export KUBECONFIG="/etc/rancher/k3s/k3s.yaml"
            log "SUCCESS" "KUBECONFIG configurÃ© pour l'exÃ©cution locale: ${KUBECONFIG}"
        else
            log "WARNING" "Fichier kubeconfig K3s non trouvÃ© Ã  /etc/rancher/k3s/k3s.yaml"
            log "INFO" "Recherche d'autres fichiers kubeconfig..."

            # Recherche d'autres emplacements possibles pour le fichier kubeconfig
            local possible_kubeconfig_files=(
                "/root/.kube/config"
                "${HOME}/.kube/config"
                "/var/lib/rancher/k3s/server/cred/admin.kubeconfig"
            )

            local kubeconfig_found=false
            for kubeconfig_file in "${possible_kubeconfig_files[@]}"; do
                if [[ -f "${kubeconfig_file}" ]]; then
                    log "INFO" "Fichier kubeconfig trouvÃ© Ã  ${kubeconfig_file}"
                    export KUBECONFIG="${kubeconfig_file}"
                    kubeconfig_found=true
                    log "SUCCESS" "KUBECONFIG configurÃ© pour l'exÃ©cution locale: ${KUBECONFIG}"
                    break
                fi
            done

            if [[ "${kubeconfig_found}" == "false" ]]; then
                log "WARNING" "Aucun fichier kubeconfig trouvÃ©, utilisation de la configuration par dÃ©faut"
            fi
        fi
    fi

    # Attente que les CRDs de cert-manager soient prÃªts
    log "INFO" "VÃ©rification que les CRDs de cert-manager sont prÃªts..."
    local max_attempts=30
    local attempt=0
    local crds_ready=false
    local connection_error=false

    while [[ "${crds_ready}" == "false" && ${attempt} -lt ${max_attempts} ]]; do
        attempt=$((attempt + 1))
        log "INFO" "Tentative ${attempt}/${max_attempts} de vÃ©rification des CRDs de cert-manager..."

        # RÃ©initialiser la variable connection_error au dÃ©but de chaque itÃ©ration
        connection_error=false

        # VÃ©rification de la connectivitÃ© au cluster Kubernetes
        if ! kubectl cluster-info &>/dev/null; then
            log "WARNING" "Impossible de se connecter au cluster Kubernetes (tentative ${attempt}/${max_attempts})"

            # VÃ©rification du KUBECONFIG
            if [[ -n "${KUBECONFIG}" ]]; then
                log "INFO" "KUBECONFIG actuel: ${KUBECONFIG}"
            else
                log "INFO" "KUBECONFIG non dÃ©fini, utilisation de la configuration par dÃ©faut"
            fi

            # Si exÃ©cution locale, tentative de configuration du KUBECONFIG
            if [[ "${IS_LOCAL_EXECUTION}" == "true" ]]; then
                log "INFO" "Tentative de configuration du KUBECONFIG pour l'exÃ©cution locale..."

                # VÃ©rification de l'existence du fichier kubeconfig K3s
                if [[ -f "/etc/rancher/k3s/k3s.yaml" ]]; then
                    log "INFO" "Utilisation du fichier kubeconfig K3s: /etc/rancher/k3s/k3s.yaml"
                    export KUBECONFIG="/etc/rancher/k3s/k3s.yaml"
                elif [[ -f "${HOME}/.kube/config" ]]; then
                    log "INFO" "Utilisation du fichier kubeconfig utilisateur: ${HOME}/.kube/config"
                    export KUBECONFIG="${HOME}/.kube/config"
                else
                    log "WARNING" "Aucun fichier kubeconfig trouvÃ©"
                    connection_error=true
                fi

                # VÃ©rification de la connectivitÃ© aprÃ¨s configuration
                if kubectl cluster-info &>/dev/null; then
                    log "SUCCESS" "Connexion au cluster Kubernetes Ã©tablie avec le nouveau KUBECONFIG"
                else
                    log "WARNING" "Impossible de se connecter au cluster Kubernetes mÃªme avec le nouveau KUBECONFIG"

                    # Tentative de diagnostic
                    log "INFO" "Diagnostic de la connexion Kubernetes..."
                    log "INFO" "VÃ©rification du service K3s..."
                    systemctl status k3s &>/dev/null
                    if [[ $? -eq 0 ]]; then
                        log "INFO" "Le service K3s est en cours d'exÃ©cution"
                        log "INFO" "VÃ©rification des journaux K3s..."
                        journalctl -u k3s --no-pager -n 20 > /tmp/k3s_logs.txt
                        log "INFO" "Journaux K3s enregistrÃ©s dans /tmp/k3s_logs.txt"
                    else
                        log "WARNING" "Le service K3s n'est pas en cours d'exÃ©cution"
                        log "INFO" "Tentative de dÃ©marrage du service K3s..."
                        systemctl start k3s
                        sleep 10
                        if systemctl status k3s &>/dev/null; then
                            log "SUCCESS" "Service K3s dÃ©marrÃ© avec succÃ¨s"
                        else
                            log "ERROR" "Impossible de dÃ©marrer le service K3s"
                            connection_error=true
                        fi
                    fi

                    # VÃ©rification des ports
                    log "INFO" "VÃ©rification du port 6443..."
                    if netstat -tuln | grep -q ":6443"; then
                        log "INFO" "Le port 6443 est ouvert"
                    else
                        log "WARNING" "Le port 6443 n'est pas ouvert"
                        connection_error=true
                    fi
                fi
            else
                # ExÃ©cution distante
                connection_error=true
            fi

            if [[ "${connection_error}" == "true" ]]; then
                log "INFO" "Les CRDs de cert-manager ne sont pas encore prÃªts, attente de 10 secondes..."
                sleep 10
                continue
            fi
        fi

        # VÃ©rification des CRDs
        if kubectl get crd 2>/dev/null | grep -q "clusterissuers.cert-manager.io" && \
           kubectl get crd 2>/dev/null | grep -q "certificates.cert-manager.io" && \
           kubectl get crd 2>/dev/null | grep -q "issuers.cert-manager.io"; then
            log "SUCCESS" "Les CRDs de cert-manager sont prÃªts"
            crds_ready=true
        else
            log "INFO" "Les CRDs de cert-manager ne sont pas encore prÃªts, attente de 10 secondes..."
            sleep 10
        fi
    done

    if [[ "${crds_ready}" == "false" ]]; then
        log "WARNING" "Les CRDs de cert-manager ne semblent pas Ãªtre installÃ©s aprÃ¨s ${max_attempts} tentatives"
        log "WARNING" "Le dÃ©ploiement des ClusterIssuers pourrait Ã©chouer"
        log "INFO" "Tentative d'installation manuelle des CRDs de cert-manager..."

        # Tentative d'installation manuelle des CRDs
        if run_with_timeout "kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.11.0/cert-manager.crds.yaml" 300; then
            log "SUCCESS" "Installation manuelle des CRDs de cert-manager rÃ©ussie"

            # VÃ©rification que les CRDs sont bien installÃ©s
            log "INFO" "VÃ©rification que les CRDs sont bien installÃ©s..."
            sleep 30  # Attendre que les CRDs soient complÃ¨tement installÃ©s

            if kubectl get crd 2>/dev/null | grep -q "clusterissuers.cert-manager.io" && \
               kubectl get crd 2>/dev/null | grep -q "certificates.cert-manager.io" && \
               kubectl get crd 2>/dev/null | grep -q "issuers.cert-manager.io"; then
                log "SUCCESS" "Les CRDs de cert-manager sont maintenant prÃªts"
                crds_ready=true
            else
                log "WARNING" "Les CRDs de cert-manager ne sont toujours pas prÃªts aprÃ¨s l'installation manuelle"
            fi
        else
            log "WARNING" "Ã‰chec de l'installation manuelle des CRDs de cert-manager"

            # Diagnostic supplÃ©mentaire
            log "INFO" "Diagnostic supplÃ©mentaire..."

            # VÃ©rification de la connectivitÃ©
            if kubectl cluster-info &>/dev/null; then
                log "INFO" "Connexion au cluster Kubernetes Ã©tablie"

                # VÃ©rification des CRDs existants
                log "INFO" "Liste des CRDs existants:"
                kubectl get crd 2>&1 || log "WARNING" "Impossible de lister les CRDs"

                # VÃ©rification des namespaces
                log "INFO" "Liste des namespaces:"
                kubectl get namespaces 2>&1 || log "WARNING" "Impossible de lister les namespaces"

                # VÃ©rification des pods cert-manager
                log "INFO" "VÃ©rification des pods cert-manager:"
                kubectl get pods --all-namespaces | grep cert-manager 2>&1 || log "INFO" "Aucun pod cert-manager trouvÃ©"
            else
                log "ERROR" "Impossible de se connecter au cluster Kubernetes"
                log "ERROR" "VÃ©rifiez que le cluster Kubernetes est en cours d'exÃ©cution et accessible"
            fi

            log "WARNING" "Le dÃ©ploiement des ClusterIssuers pourrait Ã©chouer"
        fi
    fi

    # VÃ©rification de l'accÃ¨s au cluster Kubernetes
    if ! kubectl cluster-info &>/dev/null; then
        log "ERROR" "Impossible d'accÃ©der au cluster Kubernetes"
        log "ERROR" "VÃ©rifiez votre configuration kubectl et le fichier kubeconfig"

        # Tentative de rÃ©cupÃ©ration du fichier kubeconfig
        local kubeconfig_dir="${HOME}/.kube"
        mkdir -p "${kubeconfig_dir}"

        if scp -P "${ansible_port}" "${ansible_user}@${ansible_host}:/home/${ansible_user}/.kube/config" "${kubeconfig_dir}/config.k3s" &>/dev/null; then
            log "INFO" "Fichier kubeconfig rÃ©cupÃ©rÃ© dans ${kubeconfig_dir}/config.k3s"
            log "INFO" "Tentative d'utilisation du nouveau fichier kubeconfig..."

            # Sauvegarde du KUBECONFIG actuel
            local old_kubeconfig="${KUBECONFIG}"
            export KUBECONFIG="${kubeconfig_dir}/config.k3s"

            if ! kubectl cluster-info &>/dev/null; then
                log "ERROR" "Impossible d'accÃ©der au cluster Kubernetes mÃªme avec le nouveau fichier kubeconfig"
                # Restauration du KUBECONFIG
                if [[ -n "${old_kubeconfig}" ]]; then
                    export KUBECONFIG="${old_kubeconfig}"
                else
                    unset KUBECONFIG
                fi
                cleanup
                exit 1
            else
                log "SUCCESS" "AccÃ¨s au cluster Kubernetes rÃ©tabli avec le nouveau fichier kubeconfig"
            fi
        else
            log "ERROR" "Impossible de rÃ©cupÃ©rer le fichier kubeconfig"
            cleanup
            exit 1
        fi
    fi

    # CrÃ©ation du namespace pour l'infrastructure
    log "INFO" "CrÃ©ation du namespace lions-infrastructure..."
    LAST_COMMAND="kubectl create namespace lions-infrastructure --dry-run=client -o yaml | kubectl apply -f -"

    if ! run_with_timeout "kubectl create namespace lions-infrastructure --dry-run=client -o yaml | kubectl apply -f -"; then
        log "ERROR" "Ã‰chec de la crÃ©ation du namespace lions-infrastructure"

        # VÃ©rification si le namespace existe dÃ©jÃ 
        if kubectl get namespace lions-infrastructure &>/dev/null; then
            log "WARNING" "Le namespace lions-infrastructure existe dÃ©jÃ "
        else
            # Tentative de diagnostic
            log "INFO" "Tentative de diagnostic des problÃ¨mes..."
            kubectl get namespaces
            kubectl describe namespace lions-infrastructure 2>/dev/null || true

            cleanup
            exit 1
        fi
    fi

    # DÃ©ploiement des composants de base via kustomize
    log "INFO" "DÃ©ploiement des composants de base via kustomize..."
    LAST_COMMAND="kubectl apply -k \"${PROJECT_ROOT}/kubernetes/overlays/${environment}\""

    # VÃ©rification prÃ©alable de la configuration kustomize
    log "INFO" "VÃ©rification de la configuration kustomize..."
    if ! run_with_timeout "kubectl kustomize \"${PROJECT_ROOT}/kubernetes/overlays/${environment}\" > /dev/null"; then
        log "ERROR" "La configuration kustomize contient des erreurs"

        # Affichage des erreurs de kustomize
        kubectl kustomize "${PROJECT_ROOT}/kubernetes/overlays/${environment}" 2>&1 || true

        # Tentative de diagnostic
        log "INFO" "Tentative de diagnostic des problÃ¨mes de kustomize..."

        # VÃ©rification des fichiers rÃ©fÃ©rencÃ©s
        log "INFO" "VÃ©rification des fichiers rÃ©fÃ©rencÃ©s dans kustomization.yaml..."
        grep -r "resources:" "${PROJECT_ROOT}/kubernetes/overlays/${environment}" --include="*.yaml" -A 10

        cleanup
        exit 1
    fi

    # Application de la configuration kustomize
    if ! run_with_timeout "kubectl apply -k \"${PROJECT_ROOT}/kubernetes/overlays/${environment}\" --timeout=5m"; then
        log "ERROR" "Ã‰chec du dÃ©ploiement des composants de base via kustomize"

        # Tentative de diagnostic
        log "INFO" "Tentative de diagnostic des problÃ¨mes..."

        # VÃ©rification des erreurs courantes
        log "INFO" "VÃ©rification des erreurs courantes..."

        # VÃ©rification des ressources dÃ©ployÃ©es
        kubectl get all -n "${environment}" 2>/dev/null || true

        # VÃ©rification des Ã©vÃ©nements
        kubectl get events --sort-by='.lastTimestamp' --field-selector type=Warning -n "${environment}" 2>/dev/null || true

        # Tentative de dÃ©ploiement avec validation dÃ©sactivÃ©e
        log "INFO" "Tentative de dÃ©ploiement avec validation dÃ©sactivÃ©e..."
        if ! run_with_timeout "kubectl apply -k \"${PROJECT_ROOT}/kubernetes/overlays/${environment}\" --validate=false --timeout=5m"; then
            log "ERROR" "Ã‰chec du dÃ©ploiement mÃªme avec validation dÃ©sactivÃ©e"
            cleanup
            exit 1
        else
            log "WARNING" "DÃ©ploiement rÃ©ussi avec validation dÃ©sactivÃ©e, mais des problÃ¨mes peuvent subsister"
        fi
    fi

    # VÃ©rification du dÃ©ploiement
    log "INFO" "VÃ©rification du dÃ©ploiement..."

    # VÃ©rification des namespaces
    if ! kubectl get namespace "${environment}" &>/dev/null; then
        log "WARNING" "Le namespace ${environment} n'a pas Ã©tÃ© crÃ©Ã©"
    else
        log "INFO" "Namespace ${environment} crÃ©Ã© avec succÃ¨s"
    fi

    # VÃ©rification des quotas de ressources
    if ! kubectl get resourcequotas -n "${environment}" &>/dev/null; then
        log "WARNING" "Les quotas de ressources n'ont pas Ã©tÃ© crÃ©Ã©s dans le namespace ${environment}"
    else
        log "INFO" "Quotas de ressources crÃ©Ã©s avec succÃ¨s dans le namespace ${environment}"
    fi

    # VÃ©rification des politiques rÃ©seau
    if ! kubectl get networkpolicies -n "${environment}" &>/dev/null; then
        log "WARNING" "Les politiques rÃ©seau n'ont pas Ã©tÃ© crÃ©Ã©es dans le namespace ${environment}"
    else
        log "INFO" "Politiques rÃ©seau crÃ©Ã©es avec succÃ¨s dans le namespace ${environment}"
    fi

    # VÃ©rification et attente des StorageClasses
    log "INFO" "VÃ©rification des StorageClasses..."
    local max_sc_attempts=30
    local sc_attempt=0
    local sc_ready=false

    while [[ "${sc_ready}" == "false" && ${sc_attempt} -lt ${max_sc_attempts} ]]; do
        sc_attempt=$((sc_attempt + 1))
        log "INFO" "Tentative ${sc_attempt}/${max_sc_attempts} de vÃ©rification des StorageClasses..."

        if kubectl get storageclass standard &>/dev/null; then
            log "SUCCESS" "StorageClass 'standard' est prÃªte"
            sc_ready=true
        else
            log "INFO" "StorageClass 'standard' n'est pas encore prÃªte, attente de 10 secondes..."
            sleep 10
        fi
    done

    if [[ "${sc_ready}" == "false" ]]; then
        log "WARNING" "StorageClass 'standard' n'est pas disponible aprÃ¨s ${max_sc_attempts} tentatives"
        log "WARNING" "Les dÃ©ploiements qui dÃ©pendent de cette StorageClass pourraient Ã©chouer"
    fi

    # VÃ©rification et attente de Traefik
    # Note: Traefik peut Ãªtre dÃ©ployÃ© de diffÃ©rentes maniÃ¨res (K3s, Helm, etc.) avec diffÃ©rentes Ã©tiquettes
    # Cette vÃ©rification prend en charge plusieurs mÃ©thodes de dÃ©tection
    log "INFO" "VÃ©rification de Traefik..."
        local max_traefik_attempts=20  # RÃ©duire le nombre de tentatives
        local traefik_attempt=0
        local traefik_ready=false

        while [[ "${traefik_ready}" == "false" && ${traefik_attempt} -lt ${max_traefik_attempts} ]]; do
            traefik_attempt=$((traefik_attempt + 1))
            log "INFO" "Tentative ${traefik_attempt}/${max_traefik_attempts} de vÃ©rification de Traefik..."

            # VÃ©rification multiple pour plus de robustesse
            if kubectl get pods -n kube-system -l app.kubernetes.io/name=traefik -o jsonpath='{.items[*].status.phase}' 2>/dev/null | grep -q "Running"; then
                log "SUCCESS" "Traefik est en cours d'exÃ©cution (label app.kubernetes.io/name=traefik)"
                if kubectl get pods -n kube-system -l app.kubernetes.io/name=traefik -o jsonpath='{.items[*].status.containerStatuses[0].ready}' 2>/dev/null | grep -q "true"; then
                    traefik_ready=true
                    log "SUCCESS" "Traefik est prÃªt"
                else
                    log "INFO" "Traefik dÃ©marre, attente de 5 secondes..."
                    sleep 5
                fi
            elif kubectl get pods -n traefik -l app.kubernetes.io/name=traefik -o jsonpath='{.items[*].status.phase}' 2>/dev/null | grep -q "Running"; then
                log "SUCCESS" "Traefik est en cours d'exÃ©cution (namespace traefik)"
                if kubectl get pods -n traefik -l app.kubernetes.io/name=traefik -o jsonpath='{.items[*].status.containerStatuses[0].ready}' 2>/dev/null | grep -q "true"; then
                    traefik_ready=true
                    log "SUCCESS" "Traefik est prÃªt"
                else
                    log "INFO" "Traefik dÃ©marre, attente de 5 secondes..."
                    sleep 5
                fi
            else
                log "INFO" "Traefik n'est pas encore en cours d'exÃ©cution, attente de 5 secondes..."
                sleep 5
            fi
        done

        if [[ "${traefik_ready}" == "false" ]]; then
            log "WARNING" "Traefik n'est pas prÃªt aprÃ¨s ${max_traefik_attempts} tentatives"
            log "INFO" "Tentative de dÃ©ploiement manuel de Traefik..."

            # Installation manuelle avec configuration corrigÃ©e
            if ! helm repo list | grep -q "traefik"; then
                helm repo add traefik https://traefik.github.io/charts
                helm repo update
            fi

            # DÃ©ploiement dans namespace kube-system (comme K3s par dÃ©faut)
            helm upgrade --install traefik traefik/traefik \
                --namespace kube-system \
                --set deployment.replicas=1 \
                --set service.type=LoadBalancer \
                --set ports.web.port=80 \
                --set ports.web.exposedPort=80 \
                --set ports.websecure.port=443 \
                --set ports.websecure.exposedPort=443 \
                --set ingressClass.enabled=true \
                --set ingressClass.isDefaultClass=true \
                --set providers.kubernetesCRD.enabled=true \
                --set providers.kubernetesIngress.enabled=true \
                --set resources.requests.cpu=100m \
                --set resources.requests.memory=64Mi \
                --set resources.limits.cpu=300m \
                --set resources.limits.memory=256Mi \
                --wait --timeout 5m

        # Attente que Traefik soit prÃªt aprÃ¨s l'installation manuelle
        # Note: Traefik peut Ãªtre dÃ©ployÃ© de diffÃ©rentes maniÃ¨res (K3s, Helm, etc.) avec diffÃ©rentes Ã©tiquettes
        # Cette vÃ©rification prend en charge plusieurs mÃ©thodes de dÃ©tection
        log "INFO" "Attente que Traefik soit prÃªt aprÃ¨s l'installation manuelle..."
        local manual_max_attempts=15
        local manual_attempt=0
        local manual_traefik_ready=false

        while [[ "${manual_traefik_ready}" == "false" && ${manual_attempt} -lt ${manual_max_attempts} ]]; do
            manual_attempt=$((manual_attempt + 1))
            log "INFO" "Tentative ${manual_attempt}/${manual_max_attempts} de vÃ©rification de Traefik aprÃ¨s installation manuelle..."

            # VÃ©rification avec le label app=traefik (mÃ©thode standard)
            if kubectl get pods -n kube-system -l app=traefik -o jsonpath='{.items[*].status.phase}' 2>/dev/null | grep -q "Running"; then
                log "SUCCESS" "Traefik est en cours d'exÃ©cution aprÃ¨s installation manuelle (label app=traefik)"

                # VÃ©rification que Traefik est prÃªt
                if kubectl get pods -n kube-system -l app=traefik -o jsonpath='{.items[*].status.containerStatuses[0].ready}' 2>/dev/null | grep -q "true"; then
                    log "SUCCESS" "Traefik est prÃªt aprÃ¨s installation manuelle"
                    manual_traefik_ready=true
                else
                    log "INFO" "Traefik est en cours d'exÃ©cution mais n'est pas encore prÃªt, attente de 10 secondes..."
                    sleep 10
                fi
            # VÃ©rification alternative avec le nom du pod commenÃ§ant par "traefik-"
            elif kubectl get pods -n kube-system -o name 2>/dev/null | grep -q "pod/traefik-"; then
                log "SUCCESS" "Traefik est en cours d'exÃ©cution aprÃ¨s installation manuelle (pod commenÃ§ant par traefik-)"

                # RÃ©cupÃ©ration du nom du pod Traefik
                local traefik_pod_name=$(kubectl get pods -n kube-system -o name 2>/dev/null | grep "pod/traefik-" | head -n 1 | sed 's|pod/||')

                # VÃ©rification que Traefik est prÃªt
                if kubectl get pod -n kube-system "${traefik_pod_name}" -o jsonpath='{.status.containerStatuses[0].ready}' 2>/dev/null | grep -q "true"; then
                    log "SUCCESS" "Traefik est prÃªt aprÃ¨s installation manuelle"
                    manual_traefik_ready=true
                else
                    log "INFO" "Traefik est en cours d'exÃ©cution mais n'est pas encore prÃªt, attente de 10 secondes..."
                    sleep 10
                fi
            else
                log "INFO" "Traefik n'est pas encore en cours d'exÃ©cution aprÃ¨s installation manuelle, attente de 10 secondes..."
                sleep 10
            fi
        done

        if [[ "${manual_traefik_ready}" == "false" ]]; then
            log "WARNING" "Traefik n'est pas prÃªt mÃªme aprÃ¨s installation manuelle"
            log "WARNING" "Les services qui dÃ©pendent de Traefik pourraient ne pas Ãªtre accessibles"
            log "WARNING" "VÃ©rifiez l'installation de K3s et les logs"
        else
            log "SUCCESS" "Traefik a Ã©tÃ© installÃ© manuellement avec succÃ¨s"
            traefik_ready=true
        fi
    fi

    log "SUCCESS" "DÃ©ploiement de l'infrastructure de base terminÃ© avec succÃ¨s"
}

# Fonction de dÃ©ploiement du monitoring
function deployer_monitoring() {
    log "INFO" "DÃ©ploiement du systÃ¨me de monitoring..."
    INSTALLATION_STEP="deploy_monitoring"

    # Sauvegarde de l'Ã©tat actuel
    echo "${INSTALLATION_STEP}" > "${STATE_FILE}"

    # VÃ©rification de l'accÃ¨s au cluster Kubernetes
    if ! kubectl cluster-info &>/dev/null; then
        log "ERROR" "Impossible d'accÃ©der au cluster Kubernetes"
        log "ERROR" "VÃ©rifiez votre configuration kubectl et le fichier kubeconfig"
        cleanup
        exit 1
    fi

    # CrÃ©ation du namespace pour le monitoring
    log "INFO" "CrÃ©ation du namespace monitoring..."
    LAST_COMMAND="kubectl create namespace monitoring --dry-run=client -o yaml | kubectl apply -f -"

    if ! run_with_timeout "kubectl create namespace monitoring --dry-run=client -o yaml | kubectl apply -f -"; then
        log "ERROR" "Ã‰chec de la crÃ©ation du namespace monitoring"

        # VÃ©rification si le namespace existe dÃ©jÃ 
        if kubectl get namespace monitoring &>/dev/null; then
            log "WARNING" "Le namespace monitoring existe dÃ©jÃ "
        else
            # Tentative de diagnostic
            log "INFO" "Tentative de diagnostic des problÃ¨mes..."
            kubectl get namespaces
            kubectl describe namespace monitoring 2>/dev/null || true

            cleanup
            exit 1
        fi
    fi

    # DÃ©ploiement de Prometheus et Grafana via Helm
    log "INFO" "DÃ©ploiement de Prometheus et Grafana..."

    # VÃ©rification de Helm
    if ! command_exists "helm"; then
        log "ERROR" "Helm n'est pas installÃ© ou n'est pas dans le PATH"
        cleanup
        exit 1
    fi

    # Ajout du dÃ©pÃ´t Helm de Prometheus
    LAST_COMMAND="helm repo add prometheus-community https://prometheus-community.github.io/helm-charts"
    if ! run_with_timeout "helm repo add prometheus-community https://prometheus-community.github.io/helm-charts"; then
        log "ERROR" "Ã‰chec de l'ajout du dÃ©pÃ´t Helm de Prometheus"

        # Tentative de diagnostic
        log "INFO" "Tentative de diagnostic des problÃ¨mes..."
        helm repo list

        cleanup
        exit 1
    fi

    LAST_COMMAND="helm repo update"
    if ! run_with_timeout "helm repo update"; then
        log "ERROR" "Ã‰chec de la mise Ã  jour des dÃ©pÃ´ts Helm"
        cleanup
        exit 1
    fi

    # VÃ©rification des dÃ©pendances avant le dÃ©ploiement de Prometheus
    log "INFO" "VÃ©rification des dÃ©pendances pour Prometheus et Grafana..."

    # VÃ©rifier que cert-manager est installÃ© et fonctionnel
    if ! kubectl get deployment -n cert-manager cert-manager 2>/dev/null | grep -q "cert-manager"; then
        log "WARNING" "cert-manager ne semble pas Ãªtre installÃ© correctement"
        log "INFO" "Tentative de diagnostic..."
        kubectl get pods -n cert-manager
        kubectl get events -n cert-manager
    fi

    # VÃ©rifier que les CRDs nÃ©cessaires sont prÃ©sents
    if ! kubectl get crd 2>/dev/null | grep -q "servicemonitors.monitoring.coreos.com"; then
        log "WARNING" "Les CRDs de Prometheus Operator ne sont pas installÃ©s"
        log "INFO" "Installation des CRDs de Prometheus Operator..."
        run_with_timeout "kubectl apply -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/main/example/prometheus-operator-crd/monitoring.coreos.com_servicemonitors.yaml" 300
    fi

    # CrÃ©ation d'un fichier de valeurs temporaire pour Prometheus
    local values_file=$(mktemp)
    cat > "${values_file}" << EOF
    # Configuration allÃ©gÃ©e et stable pour Prometheus/Grafana
    defaultRules:
      create: true
      rules:
        alertmanager: false
        etcd: false
        configReloaders: false
        general: true  # ActivÃ© pour les rÃ¨gles essentielles
        k8s: false
        kubeApiserverAvailability: false
        kubeApiserverBurnrate: false
        kubeApiserverHistogram: false
        kubeApiserverSlos: false
        kubelet: false
        kubeProxy: false
        kubePrometheusGeneral: false
        kubePrometheusNodeRecording: false
        kubernetesApps: true  # ActivÃ© pour les applications
        kubernetesResources: true  # ActivÃ© pour les ressources
        kubernetesStorage: false
        kubernetesSystem: false
        kubeScheduler: false
        kubeStateMetrics: false
        network: false
        node: true  # ActivÃ© pour la surveillance des nÅ“uds
        nodeExporterAlerting: false
        nodeExporterRecording: false
        prometheus: false
        prometheusOperator: false

    alertmanager:
      enabled: false

    grafana:
      enabled: true
      adminPassword: admin
      service:
        type: NodePort
        nodePort: 30000
      resources:
        requests:
          cpu: 50m  # RÃ©duit de 100m Ã  50m
          memory: 64Mi  # RÃ©duit de 128Mi Ã  64Mi
        limits:
          cpu: 200m
          memory: 256Mi
      persistence:
        enabled: false
      sidecar:
        dashboards:
          enabled: true
        datasources:
          enabled: true
      # Ajout de dashboards prÃ©dÃ©finis
      dashboardProviders:
        dashboardproviders.yaml:
          apiVersion: 1
          providers:
            - name: 'default'
              orgId: 1
              folder: ''
              type: file
              disableDeletion: false
              editable: true
              options:
                path: /var/lib/grafana/dashboards/default
      # Configuration pour utiliser Prometheus comme source de donnÃ©es
      additionalDataSources:
        - name: Prometheus
          type: prometheus
          url: http://prometheus-kube-prometheus-prometheus:9090/
          access: proxy
          isDefault: true

    prometheusOperator:
      enabled: true
      resources:
        requests:
          cpu: 100m
          memory: 128Mi
        limits:
          cpu: 200m
          memory: 256Mi

    prometheus:
      enabled: true
      prometheusSpec:
        retention: 3d  # RÃ©duit de 7d Ã  3d
        retentionSize: "2GB"  # Ajout d'une limitation de taille
        resources:
          requests:
            cpu: 50m  # RÃ©duit de 300m Ã  50m
            memory: 128Mi  # RÃ©duit de 512Mi Ã  128Mi
          limits:
            cpu: 300m  # RÃ©duit de 700m Ã  300m
            memory: 512Mi  # RÃ©duit de 1Gi Ã  512Mi
        scrapeInterval: 2m  # AugmentÃ© de 1m Ã  2m pour rÃ©duire la charge
        evaluationInterval: 2m  # AugmentÃ© de 1m Ã  2m
        storageSpec: {}
        serviceMonitorSelectorNilUsesHelmValues: false
        serviceMonitorSelector: {}
        ruleSelectorNilUsesHelmValues: false
        ruleSelector: {}

    kubeStateMetrics:
      enabled: true
      resources:
        requests:
          cpu: 50m
          memory: 64Mi
        limits:
          cpu: 100m
          memory: 128Mi

    nodeExporter:
      enabled: true
      resources:
        requests:
          cpu: 50m
          memory: 64Mi
        limits:
          cpu: 100m
          memory: 128Mi

    kubeEtcd:
      enabled: false

    kubeControllerManager:
      enabled: false

    kubeScheduler:
      enabled: false

    kubeProxy:
      enabled: false

    kubelet:
      enabled: true
EOF

    # DÃ©ploiement de Prometheus
    LAST_COMMAND="helm upgrade --install prometheus prometheus-community/kube-prometheus-stack --namespace monitoring --values ${values_file}"

    if ! run_with_timeout "helm upgrade --install prometheus prometheus-community/kube-prometheus-stack --namespace monitoring --values ${values_file}" 3600; then
        log "ERROR" "Ã‰chec du dÃ©ploiement de Prometheus et Grafana"

        # Tentative de diagnostic
        log "INFO" "Tentative de diagnostic des problÃ¨mes..."

        # VÃ©rification des pods
        kubectl get pods -n monitoring

        # VÃ©rification des Ã©vÃ©nements
        kubectl get events --sort-by='.lastTimestamp' --field-selector type=Warning -n monitoring

        # VÃ©rification des logs des pods en erreur
        local failed_pods=$(kubectl get pods -n monitoring --field-selector status.phase!=Running,status.phase!=Succeeded -o jsonpath='{.items[*].metadata.name}')
        if [[ -n "${failed_pods}" ]]; then
            for pod in ${failed_pods}; do
                log "INFO" "Logs du pod ${pod}:"
                kubectl logs -n monitoring "${pod}" --tail=50 || true
            done
        fi

        # VÃ©rification des ressources disponibles
        kubectl describe nodes

        # VÃ©rification des ressources du cluster
        log "INFO" "VÃ©rification des ressources du cluster..."
        kubectl top nodes || true
        kubectl top pods --all-namespaces || true

        # VÃ©rification de l'Ã©tat des CRDs
        log "INFO" "VÃ©rification de l'Ã©tat des CRDs..."
        kubectl get crd | grep -E 'cert-manager.io|monitoring.coreos.com'

        # Nettoyage du fichier de valeurs temporaire
        rm -f "${values_file}"

        cleanup
        exit 1
    fi

    # Nettoyage du fichier de valeurs temporaire
    rm -f "${values_file}"

    # VÃ©rification du dÃ©ploiement
    log "INFO" "VÃ©rification du dÃ©ploiement du monitoring..."

    # Attente que les pods soient prÃªts
    log "INFO" "Attente que les pods de monitoring soient prÃªts..."
    local timeout=300  # 5 minutes
    local start_time
    start_time=$(date +%s)
    local all_pods_ready=false

    while [[ "${all_pods_ready}" == "false" ]]; do
        local current_time
        current_time=$(date +%s)
        local elapsed_time
        elapsed_time=$((current_time - start_time))

        if [[ ${elapsed_time} -gt ${timeout} ]]; then
            log "WARNING" "Timeout atteint en attendant que les pods de monitoring soient prÃªts"
            break
        fi

        local not_ready_pods=$(kubectl get pods -n monitoring --field-selector status.phase!=Running,status.phase!=Succeeded -o jsonpath='{.items[*].metadata.name}')
        if [[ -z "${not_ready_pods}" ]]; then
            all_pods_ready=true
            log "SUCCESS" "Tous les pods de monitoring sont prÃªts"
        else
            log "INFO" "En attente que les pods suivants soient prÃªts: ${not_ready_pods}"
            sleep 10
        fi
    done

    # VÃ©rification de l'accÃ¨s Ã  Grafana
    log "INFO" "VÃ©rification de l'accÃ¨s Ã  Grafana..."
    local grafana_service=$(kubectl get service -n monitoring prometheus-grafana -o jsonpath='{.spec.ports[0].nodePort}' 2>/dev/null)

    if [[ -n "${grafana_service}" ]]; then
        log "INFO" "Grafana est accessible Ã  l'adresse: http://${ansible_host}:${grafana_service}"
        log "INFO" "Identifiant: admin"
        log "INFO" "Mot de passe: admin"
    else
        log "WARNING" "Impossible de dÃ©terminer l'adresse d'accÃ¨s Ã  Grafana"
    fi

    log "SUCCESS" "DÃ©ploiement du systÃ¨me de monitoring terminÃ© avec succÃ¨s"
}

# Fonction de vÃ©rification finale
function verifier_installation() {
    log "INFO" "VÃ©rification de l'installation..."
    INSTALLATION_STEP="verify"

    # VÃ©rification de l'accÃ¨s au cluster Kubernetes
    if ! kubectl cluster-info &>/dev/null; then
        log "ERROR" "Impossible d'accÃ©der au cluster Kubernetes"
        log "ERROR" "VÃ©rifiez votre configuration kubectl et le fichier kubeconfig"
        cleanup
        exit 1
    fi

    # VÃ©rification des nÅ“uds
    log "INFO" "VÃ©rification des nÅ“uds Kubernetes..."
    LAST_COMMAND="kubectl get nodes -o wide"

    local nodes_output
    nodes_output=$(kubectl get nodes -o wide 2>&1)
    echo "${nodes_output}"

    # VÃ©rification de l'Ã©tat des nÅ“uds
    if ! echo "${nodes_output}" | grep -q "Ready"; then
        log "WARNING" "Aucun nÅ“ud n'est en Ã©tat 'Ready'"
        log "WARNING" "VÃ©rifiez l'Ã©tat des nÅ“uds et les logs de K3s"
    else
        log "SUCCESS" "Au moins un nÅ“ud est en Ã©tat 'Ready'"
    fi

    # VÃ©rification des namespaces
    log "INFO" "VÃ©rification des namespaces..."
    LAST_COMMAND="kubectl get namespaces"

    local namespaces_output
    namespaces_output=$(kubectl get namespaces 2>&1)
    echo "${namespaces_output}"

    # VÃ©rification des namespaces requis
    local required_namespaces=("kube-system" "lions-infrastructure" "${environment}" "monitoring")
    local missing_namespaces=()

    for ns in "${required_namespaces[@]}"; do
        if ! echo "${namespaces_output}" | grep -q "${ns}"; then
            missing_namespaces+=("${ns}")
        fi
    done

    if [[ ${#missing_namespaces[@]} -gt 0 ]]; then
        log "WARNING" "Namespaces requis manquants: ${missing_namespaces[*]}"
    else
        log "SUCCESS" "Tous les namespaces requis sont prÃ©sents"
    fi

    # VÃ©rification des pods systÃ¨me
    log "INFO" "VÃ©rification des pods systÃ¨me..."
    LAST_COMMAND="kubectl get pods -n kube-system"

    local system_pods_output
    system_pods_output=$(kubectl get pods -n kube-system 2>&1)
    echo "${system_pods_output}"

    # VÃ©rification des pods systÃ¨me essentiels
    local essential_system_pods=("coredns" "metrics-server" "local-path-provisioner")
    local missing_system_pods=()

    for pod in "${essential_system_pods[@]}"; do
        if ! echo "${system_pods_output}" | grep -q "${pod}"; then
            missing_system_pods+=("${pod}")
        fi
    done

    if [[ ${#missing_system_pods[@]} -gt 0 ]]; then
        log "WARNING" "Pods systÃ¨me essentiels manquants: ${missing_system_pods[*]}"
    else
        log "SUCCESS" "Tous les pods systÃ¨me essentiels sont prÃ©sents"
    fi

    # VÃ©rification des pods d'infrastructure
    log "INFO" "VÃ©rification des pods d'infrastructure..."
    LAST_COMMAND="kubectl get pods -n lions-infrastructure"

    local infra_pods_output
    infra_pods_output=$(kubectl get pods -n lions-infrastructure 2>&1)
    echo "${infra_pods_output}"

    # VÃ©rification des pods de monitoring
    log "INFO" "VÃ©rification des pods de monitoring..."
    LAST_COMMAND="kubectl get pods -n monitoring"

    local monitoring_pods_output
    monitoring_pods_output=$(kubectl get pods -n monitoring 2>&1)
    echo "${monitoring_pods_output}"

    # VÃ©rification des pods de monitoring essentiels
    local essential_monitoring_pods=("prometheus" "grafana" "alertmanager")
    local missing_monitoring_pods=()

    for pod in "${essential_monitoring_pods[@]}"; do
        if ! echo "${monitoring_pods_output}" | grep -q "${pod}"; then
            missing_monitoring_pods+=("${pod}")
        fi
    done

    if [[ ${#missing_monitoring_pods[@]} -gt 0 ]]; then
        log "WARNING" "Pods de monitoring essentiels manquants: ${missing_monitoring_pods[*]}"
    else
        log "SUCCESS" "Tous les pods de monitoring essentiels sont prÃ©sents"
    fi

    # VÃ©rification des pods du Kubernetes Dashboard
    log "INFO" "VÃ©rification des pods du Kubernetes Dashboard..."
    LAST_COMMAND="kubectl get pods -n kubernetes-dashboard"

    local dashboard_pods_output
    dashboard_pods_output=$(kubectl get pods -n kubernetes-dashboard 2>&1)
    echo "${dashboard_pods_output}"

    # VÃ©rification des services
    log "INFO" "VÃ©rification des services exposÃ©s..."

    # VÃ©rification de Grafana
    local grafana_service
    grafana_service=$(kubectl get service -n monitoring prometheus-grafana -o jsonpath='{.spec.ports[0].nodePort}' 2>/dev/null)
    if [[ -n "${grafana_service}" ]]; then
        log "INFO" "Grafana est exposÃ© sur le port ${grafana_service}"

        # Tentative de connexion Ã  Grafana
        if command_exists "curl"; then
            local host_to_check="${ansible_host}"
            if [[ "${IS_LOCAL_EXECUTION}" == "true" ]]; then
                host_to_check="localhost"
            fi
            if curl -s -o /dev/null -w "%{http_code}" "http://${host_to_check}:${grafana_service}" | grep -q "200\|302"; then
                log "SUCCESS" "Grafana est accessible Ã  l'adresse: http://${host_to_check}:${grafana_service}"
            else
                log "WARNING" "Grafana n'est pas accessible Ã  l'adresse: http://${host_to_check}:${grafana_service}"
                log "WARNING" "VÃ©rifiez les rÃ¨gles de pare-feu et l'Ã©tat du service"
            fi
        fi
    else
        log "WARNING" "Service Grafana non trouvÃ© ou non exposÃ©"
    fi

    # VÃ©rification du Kubernetes Dashboard
    local dashboard_service=$(kubectl get service -n kubernetes-dashboard kubernetes-dashboard-nodeport -o jsonpath='{.spec.ports[0].nodePort}' 2>/dev/null)
    if [[ -n "${dashboard_service}" ]]; then
        log "INFO" "Kubernetes Dashboard est exposÃ© sur le port ${dashboard_service}"

        # Tentative de connexion au Dashboard
        if command_exists "curl"; then
            local host_to_check="${ansible_host}"
            if [[ "${IS_LOCAL_EXECUTION}" == "true" ]]; then
                host_to_check="localhost"
            fi
            if curl -s -k -o /dev/null -w "%{http_code}" "https://${host_to_check}:${dashboard_service}" | grep -q "200\|302\|401"; then
                log "SUCCESS" "Kubernetes Dashboard est accessible Ã  l'adresse: https://${host_to_check}:${dashboard_service}"
            else
                log "WARNING" "Kubernetes Dashboard n'est pas accessible Ã  l'adresse: https://${host_to_check}:${dashboard_service}"
                log "WARNING" "VÃ©rifiez les rÃ¨gles de pare-feu et l'Ã©tat du service"
            fi
        fi
    else
        log "WARNING" "Service Kubernetes Dashboard non trouvÃ© ou non exposÃ©"
    fi

    # VÃ©rification de Traefik
    # Note: Traefik peut Ãªtre dÃ©ployÃ© de diffÃ©rentes maniÃ¨res (K3s, Helm, etc.) avec diffÃ©rentes Ã©tiquettes
    # Cette vÃ©rification prend en charge plusieurs mÃ©thodes de dÃ©tection
    log "INFO" "VÃ©rification de Traefik..."
    local traefik_pods=""

    # VÃ©rification avec le label app=traefik (mÃ©thode standard)
    traefik_pods=$(kubectl get pods -n kube-system -l app=traefik -o jsonpath='{.items[*].metadata.name}' 2>/dev/null)

    # Si aucun pod n'est trouvÃ© avec le label app=traefik, essayer de trouver des pods commenÃ§ant par "traefik-"
    if [[ -z "${traefik_pods}" ]]; then
        traefik_pods=$(kubectl get pods -n kube-system -o name 2>/dev/null | grep "pod/traefik-" | sed 's|pod/||')
    fi

    if [[ -n "${traefik_pods}" ]]; then
        log "SUCCESS" "Traefik est installÃ© et en cours d'exÃ©cution"

        # VÃ©rification des services Traefik
        local traefik_service=""

        # Essayer d'abord avec le service nommÃ© "traefik"
        traefik_service=$(kubectl get service -n kube-system traefik -o jsonpath='{.spec.ports[?(@.name=="web")].nodePort}' 2>/dev/null)

        # Si aucun service n'est trouvÃ©, essayer de trouver un service contenant "traefik" dans son nom
        if [[ -z "${traefik_service}" ]]; then
            local traefik_service_name=$(kubectl get services -n kube-system -o name 2>/dev/null | grep "service/traefik" | head -n 1 | sed 's|service/||')
            if [[ -n "${traefik_service_name}" ]]; then
                traefik_service=$(kubectl get service -n kube-system "${traefik_service_name}" -o jsonpath='{.spec.ports[0].nodePort}' 2>/dev/null)
            fi
        fi

        if [[ -n "${traefik_service}" ]]; then
            log "INFO" "Traefik est exposÃ© sur le port ${traefik_service}"

            # Tentative de connexion Ã  Traefik
            if command_exists "curl"; then
                local host_to_check="${ansible_host}"
                if [[ "${IS_LOCAL_EXECUTION}" == "true" ]]; then
                    host_to_check="localhost"
                fi
                if curl -s -o /dev/null -w "%{http_code}" "http://${host_to_check}:${traefik_service}" | grep -q "200\|302\|404"; then
                    log "SUCCESS" "Traefik est accessible Ã  l'adresse: http://${host_to_check}:${traefik_service}"
                else
                    log "WARNING" "Traefik n'est pas accessible Ã  l'adresse: http://${host_to_check}:${traefik_service}"
                    log "WARNING" "VÃ©rifiez les rÃ¨gles de pare-feu et l'Ã©tat du service"
                fi
            fi
        else
            log "WARNING" "Service Traefik non trouvÃ© ou non exposÃ©"
        fi
    else
        log "WARNING" "Traefik n'est pas installÃ© ou n'est pas en cours d'exÃ©cution"
        log "WARNING" "VÃ©rifiez l'installation de K3s et les logs"
    fi

    # VÃ©rification des quotas de ressources
    log "INFO" "VÃ©rification des quotas de ressources..."
    LAST_COMMAND="kubectl get resourcequotas --all-namespaces"

    local quotas_output=$(kubectl get resourcequotas --all-namespaces 2>&1)
    echo "${quotas_output}"

    if ! echo "${quotas_output}" | grep -q "compute-resources"; then
        log "WARNING" "Quotas de ressources non configurÃ©s"
        log "WARNING" "VÃ©rifiez la configuration des quotas de ressources"
    else
        log "SUCCESS" "Quotas de ressources configurÃ©s correctement"
    fi

    # VÃ©rification des politiques rÃ©seau
    log "INFO" "VÃ©rification des politiques rÃ©seau..."
    LAST_COMMAND="kubectl get networkpolicies --all-namespaces"

    local netpol_output=$(kubectl get networkpolicies --all-namespaces 2>&1)
    echo "${netpol_output}"

    local essential_netpols=("default-network-policy" "allow-dns" "allow-monitoring")
    local missing_netpols=()

    for netpol in "${essential_netpols[@]}"; do
        if ! echo "${netpol_output}" | grep -q "${netpol}"; then
            missing_netpols+=("${netpol}")
        fi
    done

    if [[ ${#missing_netpols[@]} -gt 0 ]]; then
        log "WARNING" "Politiques rÃ©seau essentielles manquantes: ${missing_netpols[*]}"
    else
        log "SUCCESS" "Toutes les politiques rÃ©seau essentielles sont prÃ©sentes"
    fi

    # VÃ©rification des classes de stockage
    log "INFO" "VÃ©rification des classes de stockage..."
    LAST_COMMAND="kubectl get storageclasses"

    local sc_output=$(kubectl get storageclasses 2>&1)
    echo "${sc_output}"

    if ! echo "${sc_output}" | grep -q "local-path"; then
        log "WARNING" "Classe de stockage local-path non trouvÃ©e"
        log "WARNING" "VÃ©rifiez l'installation du provisioner de stockage local"
    else
        log "SUCCESS" "Classe de stockage local-path trouvÃ©e"
    fi

    # VÃ©rification des CRDs
    log "INFO" "VÃ©rification des dÃ©finitions de ressources personnalisÃ©es (CRDs)..."
    LAST_COMMAND="kubectl get crds"

    local crd_output=$(kubectl get crds 2>&1)

    local essential_crds=("servicemonitors.monitoring.coreos.com" "prometheusrules.monitoring.coreos.com" "ingressroutes.traefik.containo.us")
    local missing_crds=()

    for crd in "${essential_crds[@]}"; do
        if ! echo "${crd_output}" | grep -q "${crd}"; then
            missing_crds+=("${crd}")
        fi
    done

    if [[ ${#missing_crds[@]} -gt 0 ]]; then
        log "WARNING" "CRDs essentielles manquantes: ${missing_crds[*]}"
    else
        log "SUCCESS" "Toutes les CRDs essentielles sont prÃ©sentes"
    fi

    # VÃ©rification des rÃ´les RBAC
    log "INFO" "VÃ©rification des rÃ´les RBAC..."
    LAST_COMMAND="kubectl get clusterroles | grep lions"

    local rbac_output=$(kubectl get clusterroles | grep lions 2>&1)
    echo "${rbac_output}"

    local essential_roles=("lions-admin" "lions-developer" "lions-monitoring")
    local missing_roles=()

    for role in "${essential_roles[@]}"; do
        if ! echo "${rbac_output}" | grep -q "${role}"; then
            missing_roles+=("${role}")
        fi
    done

    if [[ ${#missing_roles[@]} -gt 0 ]]; then
        log "WARNING" "RÃ´les RBAC essentiels manquants: ${missing_roles[*]}"
    else
        log "SUCCESS" "Tous les rÃ´les RBAC essentiels sont prÃ©sents"
    fi

    # VÃ©rification des volumes persistants
    log "INFO" "VÃ©rification des volumes persistants..."
    LAST_COMMAND="kubectl get pv"

    local pv_output=$(kubectl get pv 2>&1)
    echo "${pv_output}"

    # RÃ©sumÃ© de l'installation
    log "INFO" "RÃ©sumÃ© de l'installation:"

    # VÃ©rification des pods non prÃªts
    local not_ready_pods=$(kubectl get pods --all-namespaces --field-selector status.phase!=Running,status.phase!=Succeeded -o jsonpath='{range .items[*]}{.metadata.namespace}/{.metadata.name}{"\n"}{end}' 2>/dev/null)

    if [[ -n "${not_ready_pods}" ]]; then
        log "WARNING" "Pods non prÃªts:"
        echo "${not_ready_pods}"
    else
        log "SUCCESS" "Tous les pods sont prÃªts"
    fi

    # VÃ©rification des pods en Ã©tat d'erreur
    local error_pods=$(kubectl get pods --all-namespaces | grep -v "Running\|Completed\|NAME" 2>/dev/null)

    if [[ -n "${error_pods}" ]]; then
        log "WARNING" "Pods en Ã©tat d'erreur:"
        echo "${error_pods}"

        # RÃ©cupÃ©ration des logs des pods en erreur
        log "INFO" "Logs des pods en Ã©tat d'erreur:"
        echo "${error_pods}" | while read -r line; do
            local ns=$(echo "${line}" | awk '{print $1}')
            local pod=$(echo "${line}" | awk '{print $2}')

            log "INFO" "Logs du pod ${ns}/${pod}:"
            kubectl logs -n "${ns}" "${pod}" --tail=20 2>/dev/null || echo "Impossible de rÃ©cupÃ©rer les logs"
            echo "---"
        done
    else
        log "SUCCESS" "Aucun pod en Ã©tat d'erreur"
    fi

    # VÃ©rification des Ã©vÃ©nements rÃ©cents
    log "INFO" "Ã‰vÃ©nements rÃ©cents (derniÃ¨res 5 minutes):"
    # Using a more compatible approach without --since flag
    kubectl get events --all-namespaces --sort-by='.lastTimestamp' --field-selector type=Warning | head -n 20

    # VÃ©rification de la connectivitÃ© externe
    log "INFO" "VÃ©rification de la connectivitÃ© externe..."

    # VÃ©rification de l'accÃ¨s aux services exposÃ©s
    local host_to_check="${ansible_host}"
    if [[ "${IS_LOCAL_EXECUTION}" == "true" ]]; then
        host_to_check="localhost"
    fi

    local services_to_check=(
        "http://${host_to_check}:30000|Grafana"
        "https://${host_to_check}:30001|Kubernetes Dashboard"
        "http://${host_to_check}:80|Traefik HTTP"
        "https://${host_to_check}:443|Traefik HTTPS"
    )

    for service_info in "${services_to_check[@]}"; do
        local service_url=$(echo "${service_info}" | cut -d'|' -f1)
        local service_name=$(echo "${service_info}" | cut -d'|' -f2)

        if command_exists "curl"; then
            local protocol=$(echo "${service_url}" | cut -d':' -f1)
            local curl_opts="-s -o /dev/null -w %{http_code} --connect-timeout 5"

            if [[ "${protocol}" == "https" ]]; then
                curl_opts="${curl_opts} -k"
            fi

            local status=$(curl ${curl_opts} "${service_url}" 2>/dev/null || echo "000")

            if [[ "${status}" =~ ^(200|301|302|401|403)$ ]]; then
                log "SUCCESS" "${service_name} est accessible Ã  l'adresse ${service_url} (code ${status})"
            else
                log "WARNING" "${service_name} n'est pas accessible Ã  l'adresse ${service_url} (code ${status})"
                log "WARNING" "VÃ©rifiez les rÃ¨gles de pare-feu et l'Ã©tat du service"
            fi
        else
            log "WARNING" "curl n'est pas installÃ©, impossible de vÃ©rifier l'accÃ¨s Ã  ${service_name}"
        fi
    done

    log "SUCCESS" "VÃ©rification de l'installation terminÃ©e avec succÃ¨s"

    # GÃ©nÃ©ration d'un rapport de vÃ©rification
    local report_file
    report_file="${LOG_DIR}/verification-report-$(date +%Y%m%d-%H%M%S).txt"

    {
        echo "=== RAPPORT DE VÃ‰RIFICATION DE L'INFRASTRUCTURE LIONS ==="
        echo "Date: $(date)"
        echo "Environnement: ${environment}"
        echo ""

        echo "=== NÅ’UDS KUBERNETES ==="
        kubectl get nodes -o wide
        echo ""

        echo "=== NAMESPACES ==="
        kubectl get namespaces
        echo ""

        echo "=== PODS PAR NAMESPACE ==="
        kubectl get pods --all-namespaces
        echo ""

        echo "=== SERVICES EXPOSÃ‰S ==="
        kubectl get services --all-namespaces -o wide | grep NodePort
        echo ""

        echo "=== INGRESS ==="
        kubectl get ingress --all-namespaces
        echo ""

        echo "=== Ã‰VÃ‰NEMENTS RÃ‰CENTS ==="
        kubectl get events --all-namespaces --sort-by='.lastTimestamp' | tail -n 20
        echo ""

        echo "=== UTILISATION DES RESSOURCES ==="
        kubectl top nodes 2>/dev/null || echo "Metrics-server non disponible"
        echo ""
        kubectl top pods --all-namespaces 2>/dev/null || echo "Metrics-server non disponible"
        echo ""

        echo "=== Ã‰TAT DE SANTÃ‰ GLOBAL ==="
        if [[ -n "${not_ready_pods}" ]] || [[ -n "${error_pods}" ]]; then
            echo "âš ï¸ Des problÃ¨mes ont Ã©tÃ© dÃ©tectÃ©s, consultez les logs pour plus de dÃ©tails."
        else
            echo "âœ… L'infrastructure semble Ãªtre en bon Ã©tat."
        fi
        echo ""

        echo "=== INSTRUCTIONS D'ACCÃˆS ==="
        echo "Grafana: http://${ansible_host}:30000 (admin/admin)"
        echo "Kubernetes Dashboard: https://${ansible_host}:30001 (token requis)"
        echo ""

        echo "=== FIN DU RAPPORT ==="
    } > "${report_file}"

    log "INFO" "Rapport de vÃ©rification gÃ©nÃ©rÃ©: ${report_file}"

    # Nettoyage du fichier de verrouillage et d'Ã©tat
    # Suppression du fichier d'Ã©tat (toujours local)
    rm -f "${STATE_FILE}"

    # Suppression du fichier de verrouillage
    if [[ -f "${LOCK_FILE}" ]]; then
        # Tentative de suppression sans sudo d'abord
        if ! rm -f "${LOCK_FILE}" 2>/dev/null; then
            log "WARNING" "Impossible de supprimer le fichier de verrouillage sans sudo, tentative avec sudo..."
            # Si Ã§a Ã©choue, essayer avec sudo
            if sudo rm -f "${LOCK_FILE}"; then
                log "SUCCESS" "Fichier de verrouillage supprimÃ© avec succÃ¨s (sudo)"
            else
                log "WARNING" "Impossible de supprimer le fichier de verrouillage, mÃªme avec sudo"
            fi
        fi
    fi
}

# Fonction pour tester la robustesse du script
function test_robustesse() {
    log "INFO" "ExÃ©cution des tests de robustesse..."

    # Sauvegarde de l'Ã©tat actuel (optionnelle)
    backup_state "pre-test-robustesse" "true"

    # Test 1: Simulation d'une erreur de connexion SSH
    log "INFO" "Test 1: Simulation d'une erreur de connexion SSH..."
    local original_host="${ansible_host}"
    ansible_host="invalid.host.example.com"

    # Tentative d'exÃ©cution d'une commande qui nÃ©cessite SSH
    if ! check_vps_resources; then
        log "SUCCESS" "Test 1 rÃ©ussi: L'erreur de connexion SSH a Ã©tÃ© correctement dÃ©tectÃ©e et gÃ©rÃ©e"
    else
        log "ERROR" "Test 1 Ã©chouÃ©: L'erreur de connexion SSH n'a pas Ã©tÃ© correctement dÃ©tectÃ©e"
    fi

    # Restauration de l'hÃ´te original
    ansible_host="${original_host}"

    # Test 2: Simulation d'une erreur de commande kubectl
    log "INFO" "Test 2: Simulation d'une erreur de commande kubectl..."
    local original_kubeconfig="${KUBECONFIG}"
    export KUBECONFIG="/tmp/invalid_kubeconfig_file"

    # Tentative d'exÃ©cution d'une commande kubectl
    if ! kubectl get nodes &>/dev/null; then
        log "SUCCESS" "Test 2 rÃ©ussi: L'erreur de commande kubectl a Ã©tÃ© correctement dÃ©tectÃ©e"
    else
        log "ERROR" "Test 2 Ã©chouÃ©: L'erreur de commande kubectl n'a pas Ã©tÃ© correctement dÃ©tectÃ©e"
    fi

    # Restauration du kubeconfig original
    export KUBECONFIG="${original_kubeconfig}"

    # Test 3: Simulation d'une erreur de timeout
    log "INFO" "Test 3: Simulation d'une erreur de timeout..."
    local original_timeout="${TIMEOUT_SECONDS}"
    TIMEOUT_SECONDS=1

    # Tentative d'exÃ©cution d'une commande avec un timeout trÃ¨s court
    if ! run_with_timeout "sleep 5" 1 "sleep"; then
        log "SUCCESS" "Test 3 rÃ©ussi: L'erreur de timeout a Ã©tÃ© correctement dÃ©tectÃ©e et gÃ©rÃ©e"
    else
        log "ERROR" "Test 3 Ã©chouÃ©: L'erreur de timeout n'a pas Ã©tÃ© correctement dÃ©tectÃ©e"
    fi

    # Restauration du timeout original
    TIMEOUT_SECONDS="${original_timeout}"

    # Test 4: Test du mÃ©canisme de retry pour les erreurs rÃ©seau
    log "INFO" "Test 4: Test du mÃ©canisme de retry pour les erreurs rÃ©seau..."

    # CrÃ©ation d'un script temporaire qui Ã©choue les premiÃ¨res fois puis rÃ©ussit
    local temp_script
    temp_script=$(mktemp)
    cat > "${temp_script}" << 'EOF'
#!/bin/bash
COUNTER_FILE="/tmp/retry_test_counter"

# Initialiser le compteur s'il n'existe pas
if [[ ! -f "${COUNTER_FILE}" ]]; then
    echo "0" > "${COUNTER_FILE}"
fi

# Lire le compteur actuel
COUNTER=$(cat "${COUNTER_FILE}")

# IncrÃ©menter le compteur
COUNTER=$((COUNTER + 1))
echo "${COUNTER}" > "${COUNTER_FILE}"

# Ã‰chouer les 2 premiÃ¨res fois avec une erreur rÃ©seau
if [[ ${COUNTER} -le 2 ]]; then
    echo "Connection timed out"
    exit 1
fi

# RÃ©ussir la 3Ã¨me fois
echo "OpÃ©ration rÃ©ussie"
exit 0
EOF

    chmod +x "${temp_script}"

    # RÃ©initialiser le compteur
    echo "0" > "/tmp/retry_test_counter"

    # ExÃ©cuter la commande avec le mÃ©canisme de retry
    if run_with_timeout "${temp_script}" 10 "network_test"; then
        # VÃ©rifier que le compteur est Ã  3 (2 Ã©checs + 1 succÃ¨s)
        local final_counter
        final_counter=$(cat "/tmp/retry_test_counter")
        if [[ "${final_counter}" -eq 3 ]]; then
            log "SUCCESS" "Test 4 rÃ©ussi: Le mÃ©canisme de retry a fonctionnÃ© correctement (${final_counter} tentatives)"
        else
            log "ERROR" "Test 4 Ã©chouÃ©: Le nombre de tentatives (${final_counter}) ne correspond pas Ã  l'attendu (3)"
        fi
    else
        log "ERROR" "Test 4 Ã©chouÃ©: La commande n'a pas rÃ©ussi malgrÃ© le mÃ©canisme de retry"
    fi

    # Nettoyage
    rm -f "${temp_script}" "/tmp/retry_test_counter"

    # Test 5: Simulation d'une erreur de ressources insuffisantes
    log "INFO" "Test 5: Simulation d'une erreur de ressources insuffisantes..."
    local original_required_space="${REQUIRED_SPACE_MB}"
    REQUIRED_SPACE_MB=999999999

    # Tentative de vÃ©rification des ressources
    if ! check_disk_space; then
        log "SUCCESS" "Test 4 rÃ©ussi: L'erreur de ressources insuffisantes a Ã©tÃ© correctement dÃ©tectÃ©e et gÃ©rÃ©e"
    else
        log "ERROR" "Test 4 Ã©chouÃ©: L'erreur de ressources insuffisantes n'a pas Ã©tÃ© correctement dÃ©tectÃ©e"
    fi

    # Restauration de l'espace requis original
    REQUIRED_SPACE_MB="${original_required_space}"

    # Test 5: Test de la fonction de restauration
    log "INFO" "Test 5: Test de la fonction de restauration..."

    # Tentative de restauration de l'Ã©tat sauvegardÃ©
    if restore_state; then
        log "SUCCESS" "Test 5 rÃ©ussi: La restauration de l'Ã©tat a fonctionnÃ© correctement"
    else
        log "WARNING" "Test 5 Ã©chouÃ©: La restauration de l'Ã©tat n'a pas fonctionnÃ© correctement"
    fi

    log "INFO" "Tests de robustesse terminÃ©s"
    return 0
}

# Parsing des arguments
environment="${LIONS_ENV:-${DEFAULT_ENV}}"
inventory_file="inventories/${environment}/hosts.yml"
skip_init="${LIONS_SKIP_INIT:-false}"
debug_mode="${LIONS_DEBUG_MODE:-false}"
test_mode="${LIONS_TEST_MODE:-false}"

while [[ $# -gt 0 ]]; do
    case "$1" in
        -e|--environment)
            environment="$2"
            inventory_file="inventories/${environment}/hosts.yml"
            shift 2
            ;;
        -i|--inventory)
            inventory_file="$2"
            shift 2
            ;;
        -s|--skip-init)
            skip_init="true"
            shift
            ;;
        -d|--debug)
            debug_mode="true"
            shift
            ;;
        -t|--test)
            test_mode="true"
            shift
            ;;
        -h|--help)
            afficher_aide
            exit 0
            ;;
        *)
            log "ERROR" "Argument inconnu: $1"
            afficher_aide
            exit 1
            ;;
    esac
done

# DÃ©tection du systÃ¨me d'exploitation pour le formatage des chemins
os_name=""
os_name=$(uname -s)
if [[ "${os_name}" == *"MINGW"* || "${os_name}" == *"MSYS"* || "${os_name}" == *"CYGWIN"* || "${os_name}" == *"Windows"* ]]; then
    log "DEBUG" "SystÃ¨me Windows dÃ©tectÃ©, adaptation des chemins..."

    # Convertir les chemins de fichiers pour Windows si nÃ©cessaire
    if [[ "${inventory_file}" == *"/"* && "${inventory_file}" != *"\\"* ]]; then
        # Remplacer les slashes par des backslashes pour Windows
        inventory_file_win=$(echo "${inventory_file}" | tr '/' '\\')
        log "DEBUG" "Chemin d'inventaire adaptÃ© pour Windows: ${inventory_file_win}"

        # VÃ©rifier si le chemin converti existe
        if [[ -f "${inventory_file_win}" ]]; then
            log "DEBUG" "Utilisation du chemin Windows pour l'inventaire"
            inventory_file="${inventory_file_win}"
        else
            log "DEBUG" "Le chemin Windows n'existe pas, conservation du chemin original"
        fi
    fi
fi

# Affichage du titre
echo -e "${COLOR_CYAN}${COLOR_BOLD}"
    echo -e "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
    echo -e "â•‘                                                                   â•‘"
    echo -e "â•‘      â–ˆâ–ˆâ•—     â–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ•—      â•‘"
    echo -e "â•‘      â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•‘      â•‘"
    echo -e "â•‘      â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ•— â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ•— â–ˆâ–ˆâ•‘      â•‘"
    echo -e "â•‘      â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â•šâ•â•â•â•â–ˆâ–ˆâ•‘    â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘      â•‘"
    echo -e "â•‘      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘ â•šâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘    â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ–ˆâ–ˆâ–ˆâ–ˆâ•‘      â•‘"
    echo -e "â•‘      â•šâ•â•â•â•â•â•â•â•šâ•â• â•šâ•â•â•â•â•â• â•šâ•â•  â•šâ•â•â•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•šâ•â•  â•šâ•â•â•â•      â•‘"
    echo -e "â•‘                                                                   â•‘"
    echo -e "â•‘     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—   â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—         â•‘"
    echo -e "â•‘     â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•—  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•—        â•‘"
    echo -e "â•‘     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘        â•‘"
    echo -e "â•‘     â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘        â•‘"
    echo -e "â•‘     â–ˆâ–ˆâ•‘     â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•  â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•   â–ˆâ–ˆâ•‘   â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•        â•‘"
    echo -e "â•‘     â•šâ•â•      â•šâ•â•â•â•â•â•   â•šâ•â•  â•šâ•â• â•šâ•â•â•â•â•â•    â•šâ•â•    â•šâ•â•â•â•â•â•         â•‘"
    echo -e "â•‘                                                                   â•‘"
    echo -e "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${COLOR_RESET}"
    echo -e "${COLOR_YELLOW}${COLOR_BOLD}     Infrastructure de DÃ©ploiement AutomatisÃ© - v2.0.0${COLOR_RESET}"
    echo -e "${COLOR_CYAN}  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${COLOR_RESET}\n"

# Affichage des paramÃ¨tres
log "INFO" "Environnement: ${environment}"
log "INFO" "Fichier d'inventaire: ${inventory_file}"
log "INFO" "Ignorer l'initialisation: ${skip_init}"
log "INFO" "Mode debug: ${debug_mode}"
log "INFO" "Mode test: ${test_mode}"
log "INFO" "Fichier de log: ${LOG_FILE}"

# La vÃ©rification du fichier de verrouillage est dÃ©jÃ  effectuÃ©e dans la fonction verifier_prerequis
# Ne pas crÃ©er de fichier de verrouillage ici pour Ã©viter les conflits

# ExÃ©cution des tests de robustesse si demandÃ©
if [[ "${test_mode}" == "true" ]]; then
    log "INFO" "ExÃ©cution en mode test..."

    # VÃ©rification des prÃ©requis
    log "INFO" "VÃ©rification des prÃ©requis..."
    verifier_prerequis

    # Extraction des informations d'inventaire
    extraire_informations_inventaire

    # ExÃ©cution des tests de robustesse
    test_robustesse

    log "INFO" "Mode test terminÃ©"

    # Suppression du fichier de verrouillage
    if [[ -f "${LOCK_FILE}" ]]; then
        # Tentative de suppression sans sudo d'abord
        if ! rm -f "${LOCK_FILE}" 2>/dev/null; then
            log "WARNING" "Impossible de supprimer le fichier de verrouillage sans sudo, tentative avec sudo..."
            # Si Ã§a Ã©choue, essayer avec sudo
            if sudo rm -f "${LOCK_FILE}"; then
                log "SUCCESS" "Fichier de verrouillage supprimÃ© avec succÃ¨s (sudo)"
            else
                log "WARNING" "Impossible de supprimer le fichier de verrouillage, mÃªme avec sudo"
            fi
        fi
    fi

    exit 0
fi

# ExÃ©cution des Ã©tapes d'installation
if ! verifier_prerequis; then
    log "ERROR" "Ã‰chec de la vÃ©rification des prÃ©requis"
    cleanup
    exit 1
fi

# Extraction des informations d'inventaire
if ! extraire_informations_inventaire; then
    log "ERROR" "Ã‰chec de l'extraction des informations d'inventaire"
    cleanup
    exit 1
fi

# Sauvegarde de l'Ã©tat initial (optionnelle)
backup_state "pre-installation" "true"

# Initialisation du VPS si nÃ©cessaire
if [[ "${skip_init}" == "false" ]]; then
    if ! initialiser_vps; then
        log "ERROR" "Ã‰chec de l'initialisation du VPS"
        log "INFO" "Vous pouvez rÃ©essayer avec l'option --skip-init si le VPS a dÃ©jÃ  Ã©tÃ© initialisÃ©"
        cleanup
        exit 1
    fi
else
    log "INFO" "Initialisation du VPS ignorÃ©e"
fi

# Installation de K3s
if ! installer_k3s; then
    log "ERROR" "Ã‰chec de l'installation de K3s"
    log "INFO" "Tentative de diagnostic et rÃ©paration automatique..."

    # Demander Ã  l'utilisateur s'il souhaite tenter une rÃ©paration automatique
    local repair_response
    read -p "Souhaitez-vous tenter une rÃ©paration automatique de K3s? (o/N): " repair_response

    if [[ "${repair_response}" =~ ^[oO]$ ]]; then
        if check_fix_k3s; then
            log "SUCCESS" "K3s a Ã©tÃ© rÃ©parÃ© avec succÃ¨s"
        else
            log "ERROR" "Impossible de rÃ©parer K3s automatiquement"
            log "INFO" "VÃ©rifiez les logs pour plus d'informations"
            cleanup
            exit 1
        fi
    else
        log "INFO" "RÃ©paration automatique ignorÃ©e"
        log "INFO" "VÃ©rifiez les logs pour plus d'informations"
        cleanup
        exit 1
    fi
fi

# Sauvegarde de l'Ã©tat aprÃ¨s installation de K3s (optionnelle)
backup_state "post-k3s" "true"

# Installation de HashiCorp Vault
if ! installer_vault; then
    log "WARNING" "Ã‰chec de l'installation de HashiCorp Vault"
    log "WARNING" "L'installation peut continuer sans Vault, mais certaines fonctionnalitÃ©s de gestion des secrets ne seront pas disponibles"

    # Demander Ã  l'utilisateur s'il souhaite continuer sans Vault
    if [[ "${LIONS_VAULT_ENABLED:-false}" == "true" ]]; then
        local continue_response
        read -p "Souhaitez-vous continuer l'installation sans Vault? (O/n): " continue_response

        if [[ "${continue_response}" =~ ^[nN]$ ]]; then
            log "INFO" "Installation annulÃ©e par l'utilisateur"
            cleanup
            exit 1
        else
            log "INFO" "Continuation de l'installation sans Vault"
        fi
    fi
fi

# DÃ©ploiement de l'infrastructure de base
if ! deployer_infrastructure_base; then
    log "ERROR" "Ã‰chec du dÃ©ploiement de l'infrastructure de base"
    log "INFO" "VÃ©rifiez les logs pour plus d'informations"
    cleanup
    exit 1
fi

# Sauvegarde de l'Ã©tat aprÃ¨s dÃ©ploiement de l'infrastructure (optionnelle)
backup_state "post-infrastructure" "true"

# DÃ©ploiement du monitoring
if ! deployer_monitoring; then
    log "ERROR" "Ã‰chec du dÃ©ploiement du monitoring"
    log "WARNING" "Le monitoring n'est pas essentiel, l'installation peut continuer"
fi

# Sauvegarde de l'Ã©tat aprÃ¨s dÃ©ploiement du monitoring (optionnelle)
backup_state "post-monitoring" "true"

# DÃ©ploiement des services d'infrastructure (PostgreSQL, PgAdmin, Gitea, Keycloak)
log "INFO" "DÃ©ploiement des services d'infrastructure..."
INSTALLATION_STEP="deploy_services"

# Sauvegarde de l'Ã©tat actuel
echo "${INSTALLATION_STEP}" > "${STATE_FILE}"

# Construction de la commande Ansible
# Utilisation de chemins absolus pour Ã©viter les problÃ¨mes de rÃ©solution de chemin
playbook_path="${ANSIBLE_DIR}/playbooks/deploy-infrastructure-services.yml"

# VÃ©rification que le fichier existe
if [[ ! -f "${playbook_path}" ]]; then
    log "ERROR" "Fichier de playbook non trouvÃ©: ${playbook_path}"
    log "ERROR" "VÃ©rifiez que le chemin est correct et que le fichier existe"
    log "WARNING" "DÃ©ploiement des services d'infrastructure ignorÃ©"
else
    # DÃ©tection du systÃ¨me d'exploitation pour le formatage des chemins
    os_name=""
    os_name=$(uname -s)
    if [[ "${os_name}" == *"MINGW"* || "${os_name}" == *"MSYS"* || "${os_name}" == *"CYGWIN"* || "${os_name}" == *"Windows"* ]]; then
        # Windows: convertir les chemins Unix en chemins Windows
        log "DEBUG" "SystÃ¨me Windows dÃ©tectÃ©, conversion des chemins"

        # VÃ©rifier si le chemin contient dÃ©jÃ  des backslashes
        if [[ "${playbook_path}" != *"\\"* ]]; then
            # Remplacer les slashes par des backslashes pour Windows
            playbook_path=$(echo "${playbook_path}" | tr '/' '\\')
            log "DEBUG" "Chemin de playbook converti: ${playbook_path}"
        fi

        # VÃ©rifier si le chemin existe aprÃ¨s conversion
        if [[ ! -f "${playbook_path}" ]]; then
            log "WARNING" "Le chemin de playbook converti n'existe pas: ${playbook_path}"
            log "DEBUG" "Tentative d'utilisation du chemin original"
            playbook_path="${ANSIBLE_DIR}/playbooks/deploy-infrastructure-services.yml"
        fi
    fi

    ansible_cmd="ansible-playbook -i \"${ANSIBLE_DIR}/${inventory_file}\" \"${playbook_path}\" --extra-vars \"target_env=${environment}\""

    # Ajouter l'option --ask-become-pass seulement si l'exÃ©cution n'est pas locale
    if [[ "${IS_LOCAL_EXECUTION}" != "true" ]]; then
        ansible_cmd="${ansible_cmd} --ask-become-pass"
    fi

    if [[ "${debug_mode}" == "true" ]]; then
        ansible_cmd="${ansible_cmd} -vvv"
    fi

    log "INFO" "ExÃ©cution de la commande: ${ansible_cmd}"
    LAST_COMMAND="${ansible_cmd}"

    # ExÃ©cution directe de la commande avec eval
    if eval "${ansible_cmd}"; then
        log "SUCCESS" "DÃ©ploiement des services d'infrastructure terminÃ© avec succÃ¨s"

        # Attente que les pods soient prÃªts
        log "INFO" "VÃ©rification de l'Ã©tat des pods aprÃ¨s dÃ©ploiement..."

        # Liste des namespaces Ã  vÃ©rifier
        namespaces_to_check=(
            "postgres-${environment}"
            "pgadmin-${environment}"
            "gitea-${environment}"
            "keycloak-${environment}"
            "ollama-${environment}"
        )

        for ns in "${namespaces_to_check[@]}"; do
            if kubectl get namespace "${ns}" &>/dev/null; then
                log "INFO" "Attente que les pods dans le namespace ${ns} soient prÃªts..."
                local timeout=300  # 5 minutes
                local start_time
                start_time=$(date +%s)
                local all_pods_ready=false

                while [[ "${all_pods_ready}" == "false" ]]; do
                    local current_time
                    current_time=$(date +%s)
                    local elapsed_time
                    elapsed_time=$((current_time - start_time))

                    if [[ ${elapsed_time} -gt ${timeout} ]]; then
                        log "WARNING" "Timeout atteint en attendant que les pods dans ${ns} soient prÃªts"
                        break
                    fi

                    local not_ready_pods=$(kubectl get pods -n "${ns}" --field-selector status.phase!=Running,status.phase!=Succeeded -o jsonpath='{.items[*].metadata.name}' 2>/dev/null)
                    if [[ -z "${not_ready_pods}" ]]; then
                        all_pods_ready=true
                        log "SUCCESS" "Tous les pods dans ${ns} sont prÃªts"
                    else
                        log "INFO" "En attente que les pods suivants dans ${ns} soient prÃªts: ${not_ready_pods}"
                        sleep 10
                    fi
                done
            else
                log "INFO" "Namespace ${ns} non trouvÃ©, ignorÃ©"
            fi
        done

        # VÃ©rification des services
        log "INFO" "VÃ©rification des services dÃ©ployÃ©s..."
        for ns in "${namespaces_to_check[@]}"; do
            if kubectl get namespace "${ns}" &>/dev/null; then
                local services=$(kubectl get services -n "${ns}" -o jsonpath='{.items[*].metadata.name}' 2>/dev/null)
                if [[ -n "${services}" ]]; then
                    log "INFO" "Services dans ${ns}: ${services}"
                else
                    log "WARNING" "Aucun service trouvÃ© dans ${ns}"
                fi
            fi
        done

        log "SUCCESS" "VÃ©rification des services terminÃ©e"
    else
        log "WARNING" "Ã‰chec du dÃ©ploiement des services d'infrastructure"
        log "WARNING" "Vous pouvez les dÃ©ployer manuellement plus tard avec la commande:"
        log "WARNING" "ansible-playbook ${ANSIBLE_DIR}/playbooks/deploy-infrastructure-services.yml --extra-vars \"target_env=${environment}\" --ask-become-pass"
    fi
fi

# Sauvegarde de l'Ã©tat aprÃ¨s dÃ©ploiement des services (optionnelle)
backup_state "post-services" "true"

# VÃ©rification finale de l'installation
if ! verifier_installation; then
    log "WARNING" "La vÃ©rification finale de l'installation a Ã©chouÃ©"
    log "WARNING" "Certains composants peuvent ne pas fonctionner correctement"
    log "INFO" "Consultez les logs pour plus d'informations et effectuez les corrections nÃ©cessaires"
fi

# Affichage du rÃ©sumÃ©
echo -e "\n${COLOR_GREEN}${COLOR_BOLD}===========================================================${COLOR_RESET}"
echo -e "${COLOR_GREEN}${COLOR_BOLD}  Installation de l'infrastructure LIONS terminÃ©e avec succÃ¨s !${COLOR_RESET}"
echo -e "${COLOR_GREEN}${COLOR_BOLD}===========================================================${COLOR_RESET}\n"

log "INFO" "Pour accÃ©der Ã  Grafana, utilisez l'URL: http://${ansible_host}:30000"
log "INFO" "Identifiant: admin"
log "INFO" "Mot de passe: admin"

log "INFO" "Pour accÃ©der au Kubernetes Dashboard, utilisez l'URL: https://${ansible_host}:30001"
log "INFO" "Utilisez le token permanent affichÃ© dans les logs d'installation pour vous connecter"
log "INFO" "Vous pouvez Ã©galement rÃ©cupÃ©rer le token permanent avec: kubectl get secret dashboard-admin-token -n kubernetes-dashboard -o jsonpath='{.data.token}' | base64 --decode"
log "INFO" "Ce token est permanent et ne nÃ©cessite pas d'Ãªtre rÃ©gÃ©nÃ©rÃ© Ã  chaque connexion"

log "INFO" "Pour dÃ©ployer des applications, utilisez le script deploy.sh"

# GÃ©nÃ©ration d'un rapport final
report_file="${LOG_DIR}/installation-report-$(date +%Y%m%d-%H%M%S).txt"

{
    echo "=== RAPPORT D'INSTALLATION DE L'INFRASTRUCTURE LIONS ==="
    echo "Date: $(date)"
    echo "Environnement: ${environment}"
    echo ""

    echo "=== RÃ‰SUMÃ‰ DE L'INSTALLATION ==="
    echo "âœ… Initialisation du VPS: RÃ©ussie"
    echo "âœ… Installation de K3s: RÃ©ussie"
    echo "âœ… DÃ©ploiement de l'infrastructure de base: RÃ©ussie"
    echo "âœ… DÃ©ploiement du monitoring: RÃ©ussie"
    echo "âœ… VÃ©rification de l'installation: RÃ©ussie"
    echo ""

    echo "=== INFORMATIONS D'ACCÃˆS ==="
    access_host="${ansible_host}"
    if [[ "${IS_LOCAL_EXECUTION}" == "true" ]]; then
        access_host="localhost"
    fi
    echo "Grafana: http://${access_host}:30000 (admin/admin)"
    echo "Kubernetes Dashboard: https://${access_host}:30001 (token requis)"
    echo ""

    echo "=== PROCHAINES Ã‰TAPES ==="
    echo "1. Changer le mot de passe par dÃ©faut de Grafana"
    echo "2. Configurer les alertes dans Prometheus/Alertmanager"
    echo "3. DÃ©ployer vos applications avec le script deploy.sh"
    echo ""

    echo "=== FIN DU RAPPORT ==="
} > "${report_file}"

log "INFO" "Rapport d'installation gÃ©nÃ©rÃ©: ${report_file}"

# Suppression du fichier de verrouillage
if [[ -f "${LOCK_FILE}" ]]; then
    # Tentative de suppression sans sudo d'abord
    if ! rm -f "${LOCK_FILE}" 2>/dev/null; then
        log "WARNING" "Impossible de supprimer le fichier de verrouillage sans sudo, tentative avec sudo..."
        # Si Ã§a Ã©choue, essayer avec sudo
        if sudo rm -f "${LOCK_FILE}"; then
            log "SUCCESS" "Fichier de verrouillage supprimÃ© avec succÃ¨s (sudo)"
        else
            log "WARNING" "Impossible de supprimer le fichier de verrouillage, mÃªme avec sudo"
        fi
    fi
fi

exit 0
